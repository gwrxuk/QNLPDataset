#!/usr/bin/env python3
"""
æœ€ç»ˆæ— é™åˆ¶ç‰ˆæœ¬å¯è§†åŒ–å™¨ - åªå±•ç¤ºæ— é™åˆ¶é‡å­åˆ†æç»“æœ
"""

import json
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import numpy as np
import pandas as pd
import seaborn as sns
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

def setup_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ"""
    plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'Microsoft YaHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    plt.rcParams['font.size'] = 11
    plt.rcParams['font.weight'] = 'normal'
    print("âœ… é…ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ")

def load_unrestricted_data():
    """åŠ è½½æ— é™åˆ¶ç‰ˆæœ¬æ•°æ®"""
    files = {
        'ai': '../results/fair_comparison_ai_unrestricted_summary.json',
        'journalist': '../results/fair_comparison_journalist_unrestricted_summary.json'
    }
    
    data = {}
    for key, file_path in files.items():
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data[key] = json.load(f)
            print(f"âœ… åŠ è½½æ•°æ®: {key}")
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥ {key}: {e}")
    
    return data

def create_main_comparison_chart(data):
    """åˆ›å»ºä¸»è¦å¯¹æ¯”å›¾è¡¨"""
    
    # è¯­æ³•å åŠ å¼ºåº¦å¯¹æ¯”
    ai_superposition = data['ai']['æ–°èæ¨™é¡Œ']['grammatical_superposition']['mean']
    journalist_superposition = data['journalist']['æ–°èæ¨™é¡Œ']['grammatical_superposition']['mean']
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    
    # 1. è¯­æ³•å åŠ å¼ºåº¦å¯¹æ¯”
    categories = ['AIæ–°é—»', 'è®°è€…æ–°é—»']
    values = [ai_superposition, journalist_superposition]
    colors = ['#ff6b6b', '#4ecdc4']
    
    bars = ax1.bar(categories, values, color=colors, alpha=0.8, width=0.6)
    ax1.set_title('è¯­æ³•å åŠ å¼ºåº¦å¯¹æ¯” - æ–°èæ¨™é¡Œå­—æ®µ', fontsize=14, pad=20)
    ax1.set_ylabel('è¯­æ³•å åŠ å¼ºåº¦', fontsize=12)
    ax1.set_ylim(0, 4.0)
    ax1.grid(True, alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾å’Œç†è®ºæœ€å¤§å€¼çº¿
    for bar, val in zip(bars, values):
        ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.05,
                f'{val:.6f}', ha='center', va='bottom', fontsize=11, fontweight='bold')
    
    # æ·»åŠ ç†è®ºæœ€å¤§å€¼çº¿
    ax1.axhline(y=4.0, color='red', linestyle='--', alpha=0.7, linewidth=2)
    ax1.text(0.5, 4.1, 'ç†è®ºæœ€å¤§å€¼ (4.0)', ha='center', va='bottom', fontsize=10,
            bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))
    
    # æ·»åŠ å·®å¼‚è¯´æ˜
    diff_percent = ((ai_superposition - journalist_superposition) / journalist_superposition) * 100
    ax1.text(0.5, 3.5, f'AIæ¯”è®°è€…é«˜ {diff_percent:.2f}%', 
            ha='center', va='center', fontsize=12, 
            bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7))
    
    # 2. å„å­—æ®µè¯­æ³•å åŠ å¼ºåº¦å¯¹æ¯”
    ai_fields = ['æ–°èæ¨™é¡Œ', 'å½±ç‰‡å°è©±', 'å½±ç‰‡æè¿°']
    ai_values = [data['ai'][field]['grammatical_superposition']['mean'] for field in ai_fields]
    
    journalist_fields = ['æ–°èæ¨™é¡Œ', 'æ–°èå…§å®¹']
    journalist_values = [data['journalist'][field]['grammatical_superposition']['mean'] for field in journalist_fields]
    
    # ç»˜åˆ¶AIæ–°é—»
    x_ai = np.arange(len(ai_fields))
    bars_ai = ax2.bar(x_ai - 0.2, ai_values, width=0.35, label='AIæ–°é—»', 
                     color='#ff6b6b', alpha=0.8)
    
    # ç»˜åˆ¶è®°è€…æ–°é—»
    x_journalist = np.arange(len(journalist_fields))
    bars_journalist = ax2.bar(x_journalist + 0.2, journalist_values, width=0.35, 
                             label='è®°è€…æ–°é—»', color='#4ecdc4', alpha=0.8)
    
    ax2.set_title('å„å­—æ®µè¯­æ³•å åŠ å¼ºåº¦å¯¹æ¯”', fontsize=14, pad=20)
    ax2.set_ylabel('è¯­æ³•å åŠ å¼ºåº¦', fontsize=12)
    ax2.set_ylim(0, 4.0)
    ax2.grid(True, alpha=0.3)
    ax2.legend()
    
    # è®¾ç½®xè½´æ ‡ç­¾
    all_fields = ['æ–°èæ¨™é¡Œ', 'å½±ç‰‡å°è©±/æ–°èå…§å®¹', 'å½±ç‰‡æè¿°']
    ax2.set_xticks(range(len(all_fields)))
    ax2.set_xticklabels(all_fields, rotation=0)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, val in zip(bars_ai, ai_values):
        ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.05,
                f'{val:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')
    
    for bar, val in zip(bars_journalist, journalist_values):
        ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.05,
                f'{val:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')
    
    plt.suptitle('AIç”Ÿæˆæ–°é—» vs è®°è€…æ’°å†™æ–°é—»ï¼šé‡å­è‡ªç„¶è¯­è¨€å¤„ç†åˆ†æ', fontsize=16, y=1.02)
    plt.tight_layout()
    plt.savefig('../visualizations/final_unrestricted_comparison.png', dpi=300, bbox_inches='tight',
                facecolor='white', edgecolor='none')
    plt.close()
    print("âœ… ç”Ÿæˆä¸»è¦å¯¹æ¯”å›¾")

def create_comprehensive_metrics_table(data):
    """åˆ›å»ºç»¼åˆæŒ‡æ ‡å¯¹æ¯”è¡¨"""
    
    # å‡†å¤‡æ•°æ®
    metrics_data = []
    metrics_labels = [
        ('è¯­æ³•å åŠ å¼ºåº¦', 'grammatical_superposition'),
        ('å¤šé‡ç°å®å¼ºåº¦', 'multiple_reality_strength'), 
        ('è¯­ä¹‰æ¨¡ç³Šåº¦', 'semantic_ambiguity'),
        ('æ¡†æ¶ç«äº‰å¼ºåº¦', 'frame_competition'),
        ('è¯­ä¹‰å¹²æ¶‰', 'semantic_interference'),
        ('å†¯çº½æ›¼ç†µ', 'von_neumann_entropy'),
        ('ç±»åˆ«ä¸€è‡´æ€§', 'category_coherence'),
        ('ç»„åˆçº ç¼ å¼ºåº¦', 'compositional_entanglement'),
        ('æ¡†æ¶å†²çªå¼ºåº¦', 'frame_conflict_strength')
    ]
    
    for label, metric in metrics_labels:
        ai_val = data['ai']['æ–°èæ¨™é¡Œ'][metric]['mean']
        journalist_val = data['journalist']['æ–°èæ¨™é¡Œ'][metric]['mean']
        diff = ai_val / journalist_val if journalist_val > 0 else 0
        metrics_data.append([label, ai_val, journalist_val, diff])
    
    # åˆ›å»ºè¡¨æ ¼æ ·å¼çš„å›¾
    fig, ax = plt.subplots(figsize=(16, 10))
    ax.axis('tight')
    ax.axis('off')
    
    # åˆ›å»ºè¡¨æ ¼æ•°æ®
    table_data = []
    table_data.append(['é‡å­æŒ‡æ ‡', 'AIæ–°é—»', 'è®°è€…æ–°é—»', 'å·®å¼‚å€æ•°', 'ä¼˜åŠ¿æ–¹'])
    for row in metrics_data:
        advantage = 'AI' if row[3] > 1 else 'è®°è€…' if row[3] < 1 else 'ç›¸ç­‰'
        table_data.append([row[0], f'{row[1]:.6f}', f'{row[2]:.6f}', f'{row[3]:.3f}Ã—', advantage])
    
    # åˆ›å»ºè¡¨æ ¼
    table = ax.table(cellText=table_data[1:], colLabels=table_data[0], 
                    cellLoc='center', loc='center', 
                    colWidths=[0.25, 0.2, 0.2, 0.15, 0.2])
    
    # è®¾ç½®è¡¨æ ¼æ ·å¼
    table.auto_set_font_size(False)
    table.set_fontsize(11)
    table.scale(1, 2.5)
    
    # è®¾ç½®è¡¨å¤´æ ·å¼
    for i in range(len(table_data[0])):
        table[(0, i)].set_facecolor('#4CAF50')
        table[(0, i)].set_text_props(weight='bold', color='white')
    
    # è®¾ç½®æ•°æ®è¡Œæ ·å¼
    for i in range(1, len(table_data)):
        for j in range(len(table_data[0])):
            if i % 2 == 0:
                table[(i, j)].set_facecolor('#f0f0f0')
            else:
                table[(i, j)].set_facecolor('white')
            
            # çªå‡ºæ˜¾ç¤ºå·®å¼‚å€æ•°åˆ—
            if j == 3:  # å·®å¼‚å€æ•°åˆ—
                diff_val = float(table_data[i][j].replace('Ã—', ''))
                if diff_val > 1.1:
                    table[(i, j)].set_facecolor('#ffeb3b')  # é»„è‰²çªå‡ºæ˜¾ç¤º
                elif diff_val < 0.9:
                    table[(i, j)].set_facecolor('#e3f2fd')  # æµ…è“è‰²
    
    plt.title('AIæ–°é—» vs è®°è€…æ–°é—»ï¼šé‡å­æŒ‡æ ‡å…¨é¢å¯¹æ¯” (æ–°èæ¨™é¡Œå­—æ®µ)', 
             fontsize=16, pad=30, fontweight='bold')
    plt.savefig('../visualizations/final_unrestricted_metrics_table.png', dpi=300, bbox_inches='tight',
                facecolor='white', edgecolor='none')
    plt.close()
    print("âœ… ç”Ÿæˆç»¼åˆæŒ‡æ ‡å¯¹æ¯”è¡¨")

def create_radar_chart(data):
    """åˆ›å»ºé›·è¾¾å›¾å¯¹æ¯”"""
    
    # é€‰æ‹©å…³é”®æŒ‡æ ‡
    key_metrics = [
        'grammatical_superposition', 'multiple_reality_strength', 'semantic_ambiguity',
        'frame_competition', 'semantic_interference', 'von_neumann_entropy'
    ]
    
    key_metrics_cn = [
        'è¯­æ³•å åŠ å¼ºåº¦', 'å¤šé‡ç°å®å¼ºåº¦', 'è¯­ä¹‰æ¨¡ç³Šåº¦',
        'æ¡†æ¶ç«äº‰å¼ºåº¦', 'è¯­ä¹‰å¹²æ¶‰', 'å†¯çº½æ›¼ç†µ'
    ]
    
    # è·å–æ•°æ®
    ai_values = [data['ai']['æ–°èæ¨™é¡Œ'][metric]['mean'] for metric in key_metrics]
    journalist_values = [data['journalist']['æ–°èæ¨™é¡Œ'][metric]['mean'] for metric in key_metrics]
    
    # ä¸ºäº†æ˜¾ç¤ºæ•ˆæœï¼Œå¯¹æ•°æ®è¿›è¡Œå½’ä¸€åŒ–ï¼ˆç›¸å¯¹äºå„è‡ªçš„æœ€å¤§å€¼ï¼‰
    max_values = [max(ai_val, j_val) for ai_val, j_val in zip(ai_values, journalist_values)]
    ai_normalized = [ai_val/max_val for ai_val, max_val in zip(ai_values, max_values)]
    journalist_normalized = [j_val/max_val for j_val, max_val in zip(journalist_values, max_values)]
    
    # è®¾ç½®è§’åº¦
    angles = np.linspace(0, 2 * np.pi, len(key_metrics), endpoint=False).tolist()
    ai_normalized += ai_normalized[:1]  # é—­åˆ
    journalist_normalized += journalist_normalized[:1]
    angles += angles[:1]
    
    # åˆ›å»ºé›·è¾¾å›¾
    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
    
    # ç»˜åˆ¶AIæ–°é—»
    ax.plot(angles, ai_normalized, 'o-', linewidth=3, label='AIæ–°é—»', color='#ff6b6b', alpha=0.8)
    ax.fill(angles, ai_normalized, alpha=0.25, color='#ff6b6b')
    
    # ç»˜åˆ¶è®°è€…æ–°é—»
    ax.plot(angles, journalist_normalized, 'o-', linewidth=3, label='è®°è€…æ–°é—»', color='#4ecdc4', alpha=0.8)
    ax.fill(angles, journalist_normalized, alpha=0.25, color='#4ecdc4')
    
    # è®¾ç½®æ ‡ç­¾
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(key_metrics_cn, fontsize=12)
    ax.set_ylim(0, 1)
    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
    ax.set_yticklabels(['20%', '40%', '60%', '80%', '100%'], fontsize=10)
    ax.grid(True)
    
    # æ·»åŠ å›¾ä¾‹å’Œæ ‡é¢˜
    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0), fontsize=12)
    plt.title('é‡å­ç‰¹å¾é›·è¾¾å›¾å¯¹æ¯” (æ–°èæ¨™é¡Œå­—æ®µ)\nç›¸å¯¹äºå„æŒ‡æ ‡æœ€å¤§å€¼å½’ä¸€åŒ–', 
             fontsize=14, pad=30, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('../visualizations/final_unrestricted_radar.png', dpi=300, bbox_inches='tight',
                facecolor='white', edgecolor='none')
    plt.close()
    print("âœ… ç”Ÿæˆé›·è¾¾å›¾å¯¹æ¯”")

def create_summary_statistics(data):
    """åˆ›å»ºç»Ÿè®¡æ‘˜è¦"""
    
    print("\n" + "="*80)
    print("ğŸ“Š AIæ–°é—» vs è®°è€…æ–°é—»ï¼šé‡å­ç‰¹å¾ç»Ÿè®¡æ‘˜è¦")
    print("="*80)
    
    # æ–°èæ¨™é¡Œå­—æ®µå¯¹æ¯”
    print("\nğŸ“ˆ æ–°èæ¨™é¡Œå­—æ®µæ ¸å¿ƒæŒ‡æ ‡å¯¹æ¯”:")
    print("-"*60)
    print(f"{'æŒ‡æ ‡':<20} {'AIæ–°é—»':<15} {'è®°è€…æ–°é—»':<15} {'å·®å¼‚':<10}")
    print("-"*60)
    
    key_metrics = [
        ('è¯­æ³•å åŠ å¼ºåº¦', 'grammatical_superposition'),
        ('å¤šé‡ç°å®å¼ºåº¦', 'multiple_reality_strength'),
        ('è¯­ä¹‰æ¨¡ç³Šåº¦', 'semantic_ambiguity'),
        ('æ¡†æ¶ç«äº‰å¼ºåº¦', 'frame_competition'),
        ('å†¯çº½æ›¼ç†µ', 'von_neumann_entropy')
    ]
    
    for cn_name, metric in key_metrics:
        ai_val = data['ai']['æ–°èæ¨™é¡Œ'][metric]['mean']
        j_val = data['journalist']['æ–°èæ¨™é¡Œ'][metric]['mean']
        diff = ((ai_val - j_val) / j_val) * 100 if j_val != 0 else 0
        
        print(f"{cn_name:<20} {ai_val:<15.6f} {j_val:<15.6f} {diff:>+7.2f}%")
    
    # å…³é”®å‘ç°
    ai_sup = data['ai']['æ–°èæ¨™é¡Œ']['grammatical_superposition']['mean']
    j_sup = data['journalist']['æ–°èæ¨™é¡Œ']['grammatical_superposition']['mean']
    
    print(f"\nğŸ” å…³é”®å‘ç°:")
    print(f"â€¢ è¯­æ³•å åŠ å¼ºåº¦: AIæ–°é—»è¾¾åˆ°ç†è®ºæœ€å¤§å€¼çš„ {(ai_sup/4.0)*100:.1f}%")
    print(f"â€¢ è¯­æ³•å åŠ å¼ºåº¦: è®°è€…æ–°é—»è¾¾åˆ°ç†è®ºæœ€å¤§å€¼çš„ {(j_sup/4.0)*100:.1f}%")
    print(f"â€¢ AIæ–°é—»åœ¨è¯­æ³•å åŠ å¼ºåº¦ä¸Šæ¯”è®°è€…æ–°é—»é«˜ {((ai_sup-j_sup)/j_sup)*100:.2f}%")
    
    # å„å­—æ®µå¯¹æ¯”
    print(f"\nğŸ“‹ å„å­—æ®µè¯­æ³•å åŠ å¼ºåº¦:")
    print(f"AIæ–°é—»:")
    for field in ['æ–°èæ¨™é¡Œ', 'å½±ç‰‡å°è©±', 'å½±ç‰‡æè¿°']:
        val = data['ai'][field]['grammatical_superposition']['mean']
        print(f"  â€¢ {field}: {val:.6f} ({(val/4.0)*100:.1f}%)")
    
    print(f"è®°è€…æ–°é—»:")
    for field in ['æ–°èæ¨™é¡Œ', 'æ–°èå…§å®¹']:
        val = data['journalist'][field]['grammatical_superposition']['mean']
        print(f"  â€¢ {field}: {val:.6f} ({(val/4.0)*100:.1f}%)")

def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸ¨ å¼€å§‹ç”Ÿæˆæœ€ç»ˆæ— é™åˆ¶ç‰ˆæœ¬å¯è§†åŒ–...")
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    setup_chinese_font()
    
    # åˆ›å»ºå¯è§†åŒ–ç›®å½•
    Path('../visualizations').mkdir(exist_ok=True)
    
    # åŠ è½½æ•°æ®
    data = load_unrestricted_data()
    
    if not data:
        print("âŒ æ— æ³•åŠ è½½æ•°æ®ï¼Œé€€å‡ºç¨‹åº")
        return
    
    print("\nå¼€å§‹ç”Ÿæˆå„ç§å›¾è¡¨...")
    
    # ç”Ÿæˆå„ç§å¯è§†åŒ–
    create_main_comparison_chart(data)
    create_comprehensive_metrics_table(data)
    create_radar_chart(data)
    create_summary_statistics(data)
    
    print(f"\nâœ… æœ€ç»ˆæ— é™åˆ¶ç‰ˆæœ¬å¯è§†åŒ–å·²ç”Ÿæˆå®Œæˆ!")
    print(f"ğŸ“ ä¿å­˜ä½ç½®: ../visualizations/")
    print(f"   â€¢ final_unrestricted_comparison.png - ä¸»è¦å¯¹æ¯”å›¾")
    print(f"   â€¢ final_unrestricted_metrics_table.png - ç»¼åˆæŒ‡æ ‡å¯¹æ¯”è¡¨")
    print(f"   â€¢ final_unrestricted_radar.png - é›·è¾¾å›¾å¯¹æ¯”")

if __name__ == "__main__":
    main()
