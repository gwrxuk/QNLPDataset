#!/usr/bin/env python3
"""
å¿«é€ŸQiskité‡å­åˆ†æå™¨ - ä½¿ç”¨å¯†åº¦çŸ©é™£è¨ˆç®—ç†µç‰ˆæœ¬
ä½¿ç”¨å¯†åº¦çŸ©é™£ (Density Matrix) è¨ˆç®— von Neumann ç†µ
"""

import pandas as pd
import numpy as np
from qiskit import QuantumCircuit
from qiskit.quantum_info import Statevector
import json
import time
import os
from typing import Dict, List, Any, Tuple
from pathlib import Path
import jieba
import jieba.posseg as pseg
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡åˆ†è¯
script_dir = Path(__file__).parent
project_root = script_dir.parent
dict_path = project_root / 'data' / 'dict.txt.big'
jieba.set_dictionary(str(dict_path)) if dict_path.exists() else None

class FastQiskitDensityMatrixAnalyzer:
    """å¿«é€ŸQiskité‡å­åˆ†æå™¨ - ä½¿ç”¨å¯†åº¦çŸ©é™£ç‰ˆæœ¬"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¿«é€Ÿé‡å­åˆ†æå™¨"""
        print("ğŸš€ åˆå§‹åŒ–å¿«é€ŸQiskité‡å­åˆ†æå™¨ï¼ˆå¯†åº¦çŸ©é™£ç‰ˆæœ¬ï¼‰...")
        
        # ç®€åŒ–çš„ç±»åˆ«æ˜ å°„
        self.category_map = {
            'N': 0.25,   # åè¯
            'V': 0.5,    # åŠ¨è¯
            'A': 0.75,   # å½¢å®¹è¯
            'P': 1.0,    # ä»‹è¯
            'D': 0.3,    # å‰¯è¯
            'M': 0.6,    # æ•°è¯
            'Q': 0.8,    # é‡è¯
            'R': 0.4     # ä»£è¯
        }
        
        # ç®€åŒ–çš„æƒ…æ„Ÿè¯å…¸
        self.positive_words = {'æˆåŠŸ', 'è·å¾—', 'ä¼˜ç§€', 'çªç ´', 'åˆ›æ–°', 'å‘å±•', 'æ”¹å–„', 'æå‡', 'è£è·', 'å“è¶Š', 'é¢†å…ˆ', 'è¿›æ­¥'}
        self.negative_words = {'å¤±è´¥', 'é—®é¢˜', 'å›°éš¾', 'å±æœº', 'å†²çª', 'äº‰è®®', 'æ‰¹è¯„', 'è´¨ç–‘', 'æ‹…å¿§', 'ä¸‹é™', 'å‡å°‘', 'æŸå¤±'}
        
        print("âœ… å¿«é€ŸQiskité‡å­åˆ†æå™¨åˆå§‹åŒ–å®Œæˆï¼ˆä½¿ç”¨å¯†åº¦çŸ©é™£è¨ˆç®—ç†µï¼‰")

    def create_simple_quantum_circuit(self, words: List[str], pos_tags: List[str]) -> QuantumCircuit:
        """åˆ›å»ºç®€åŒ–çš„é‡å­ç”µè·¯"""
        
        # é™åˆ¶é‡å­æ¯”ç‰¹æ•°ä»¥æé«˜é€Ÿåº¦
        num_qubits = min(4, max(2, len(set(pos_tags))))
        circuit = QuantumCircuit(num_qubits)
        
        # 1. åŸºç¡€å åŠ 
        for i in range(num_qubits):
            circuit.h(i)
        
        # 2. åŸºäºè¯æ€§çš„ç®€å•æ—‹è½¬
        pos_counts = {}
        for pos in pos_tags:
            pos_counts[pos] = pos_counts.get(pos, 0) + 1
        
        for i, (pos, count) in enumerate(list(pos_counts.items())[:num_qubits]):
            if pos in self.category_map:
                angle = self.category_map[pos] * (count / len(pos_tags)) * np.pi / 4
                circuit.ry(angle, i)
        
        # 3. ç®€å•çº ç¼ ï¼ˆåªåœ¨å‰ä¸¤ä¸ªé‡å­æ¯”ç‰¹é—´ï¼‰
        if num_qubits > 1:
            circuit.cx(0, 1)
        
        return circuit

    def calculate_von_neumann_entropy_from_density_matrix(self, statevector_data: np.ndarray) -> float:
        """ä½¿ç”¨å¯†åº¦çŸ©é™£è¨ˆç®— von Neumann ç†µ"""
        try:
            # æ­¸ä¸€åŒ–ç‹€æ…‹å‘é‡
            norm = np.linalg.norm(statevector_data)
            if norm > 0:
                statevector_data = statevector_data / norm
            else:
                return 0.0
            
            # å‰µå»ºå¯†åº¦çŸ©é™£: Ï = |ÏˆâŸ©âŸ¨Ïˆ|
            density_matrix = np.outer(statevector_data, np.conj(statevector_data))
            
            # è¨ˆç®—å¯†åº¦çŸ©é™£çš„ç‰¹å¾µå€¼
            eigenvals = np.linalg.eigvals(density_matrix)
            eigenvals = eigenvals[eigenvals > 1e-12]  # éæ¿¾å°ç‰¹å¾µå€¼
            
            if len(eigenvals) > 0:
                # von Neumann ç†µ: S(Ï) = -Tr(Ï logâ‚‚ Ï) = -Î£ Î»áµ¢ logâ‚‚ Î»áµ¢
                von_neumann_entropy = float(-np.sum(eigenvals * np.log2(eigenvals + 1e-12)))
                return von_neumann_entropy
            else:
                return 0.0
        except Exception as e:
            print(f"âš ï¸  å¯†åº¦çŸ©é™£è¨ˆç®—éŒ¯èª¤: {e}")
            return 0.0

    def fast_quantum_analysis(self, text: str, field_name: str = "text") -> Dict[str, Any]:
        """å¿«é€Ÿé‡å­åˆ†æ - ä½¿ç”¨å¯†åº¦çŸ©é™£è¨ˆç®—ç†µ"""
        
        if not text or len(text.strip()) == 0:
            return None
        
        # åˆ†è¯å’Œè¯æ€§æ ‡æ³¨
        words = []
        pos_tags = []
        
        for word, flag in pseg.cut(text):
            if len(word.strip()) > 0:
                words.append(word)
                pos_tags.append(flag)
        
        if len(words) == 0:
            return None
        
        try:
            # åˆ›å»ºç®€åŒ–é‡å­ç”µè·¯
            circuit = self.create_simple_quantum_circuit(words, pos_tags)
            
            # æ‰§è¡Œé‡å­ç”µè·¯ - ä½¿ç”¨ Statevector ç›´æ¥è¨ˆç®—
            statevector = Statevector.from_instruction(circuit)
            
            # ç²å–ç‹€æ…‹å‘é‡æ•¸æ“š
            statevector_data = np.array(statevector.data)
            
            # å¿«é€Ÿé‡å­æŒ‡æ ‡è®¡ç®—
            amplitudes = np.abs(statevector_data)
            probabilities = amplitudes**2
            
            # 1. é‡å­ç†µ - ä½¿ç”¨å¯†åº¦çŸ©é™£è¨ˆç®—
            von_neumann_entropy = self.calculate_von_neumann_entropy_from_density_matrix(statevector_data)
            
            # 2. å åŠ å¼ºåº¦
            superposition_strength = float(4 * np.sum(probabilities * (1 - probabilities)))
            
            # 3. é‡å­ç›¸å¹²æ€§ï¼ˆåŸºæ–¼å¯†åº¦çŸ©é™£ï¼‰
            # å‰µå»ºå¯†åº¦çŸ©é™£ç”¨æ–¼ç›¸å¹²æ€§è¨ˆç®—
            norm = np.linalg.norm(statevector_data)
            if norm > 0:
                statevector_normalized = statevector_data / norm
                density_matrix = np.outer(statevector_normalized, np.conj(statevector_normalized))
                # ç›¸å¹²æ€§ï¼šéå°è§’å…ƒç´ çš„ç¸½å’Œ
                diagonal_elements = np.diag(density_matrix)
                off_diagonal = density_matrix - np.diag(diagonal_elements)
                quantum_coherence = float(np.sum(np.abs(off_diagonal)))
            else:
                quantum_coherence = float(1.0 - np.sum(probabilities**2))
            
            # 4. è¯­ä¹‰å¹²æ¶‰ï¼ˆåŸºäºè¯é¢‘æ–¹å·®ï¼‰
            word_counts = {}
            for word in words:
                word_counts[word] = word_counts.get(word, 0) + 1
            semantic_interference = float(np.var(list(word_counts.values())) / len(words))
            
            # 5. æ¡†æ¶ç«äº‰
            if len(probabilities) > 1:
                uniform_prob = 1.0 / len(probabilities)
                kl_div = np.sum(probabilities * np.log2(probabilities / (uniform_prob + 1e-12)))
                max_kl = np.log2(len(probabilities))
                frame_competition = float(1.0 - (kl_div / max_kl)) if max_kl > 0 else 0.0
            else:
                frame_competition = 0.0
            
            # 6. æƒ…æ„Ÿææ€§
            positive_count = sum(1 for word in words if word in self.positive_words)
            negative_count = sum(1 for word in words if word in self.negative_words)
            emotional_intensity = (positive_count + negative_count) / len(words)
            
            # 7. å¤šé‡ç°å®å¼ºåº¦
            reality_strength = (
                superposition_strength * 0.4 +
                semantic_interference * 0.3 +
                frame_competition * 0.2 +
                emotional_intensity * 0.1
            )
            
            # åŸºæœ¬ç»Ÿè®¡
            word_count = len(words)
            unique_words = len(set(words))
            categorical_diversity = len(set(pos_tags))
            
            return {
                'field': field_name,
                'original_text': text[:100] + '...' if len(text) > 100 else text,  # æˆªæ–­é•¿æ–‡æœ¬
                'word_count': word_count,
                'unique_words': unique_words,
                'categorical_diversity': categorical_diversity,
                'quantum_circuit_qubits': circuit.num_qubits,
                'von_neumann_entropy': von_neumann_entropy,
                'superposition_strength': superposition_strength,
                'quantum_coherence': quantum_coherence,
                'semantic_interference': semantic_interference,
                'frame_competition': frame_competition,
                'emotional_intensity': float(emotional_intensity),
                'multiple_reality_strength': float(reality_strength),
                'analysis_version': 'fast_qiskit_density_matrix_v1.0'
            }
            
        except Exception as e:
            print(f"âš ï¸  é‡å­ç”µè·¯å¤±è´¥ï¼Œä½¿ç”¨ç»å…¸è®¡ç®—: {str(e)[:50]}...")
            # å¿«é€Ÿå›é€€åˆ°ç»å…¸è®¡ç®—
            return self.classical_fallback(words, pos_tags, field_name, text)

    def classical_fallback(self, words: List[str], pos_tags: List[str], field_name: str, text: str) -> Dict[str, Any]:
        """ç»å…¸è®¡ç®—å›é€€"""
        
        # è¯é¢‘åˆ†æ
        word_counts = {}
        for word in words:
            word_counts[word] = word_counts.get(word, 0) + 1
        
        probabilities = np.array([count/len(words) for count in word_counts.values()])
        
        # ç®€åŒ–æŒ‡æ ‡ï¼ˆç¶“å…¸è¨ˆç®—ä»ä½¿ç”¨æ¦‚ç‡ç†µï¼‰
        von_neumann_entropy = float(-np.sum(probabilities * np.log2(probabilities + 1e-12)))
        superposition_strength = float(4 * np.sum(probabilities * (1 - probabilities)))
        quantum_coherence = float(len(set(words)) / len(words))
        semantic_interference = float(np.var(list(word_counts.values())) / len(words))
        
        # æ¡†æ¶ç«äº‰
        if len(probabilities) > 1:
            uniform_prob = 1.0 / len(probabilities)
            kl_div = np.sum(probabilities * np.log2(probabilities / (uniform_prob + 1e-12)))
            max_kl = np.log2(len(probabilities))
            frame_competition = float(1.0 - (kl_div / max_kl)) if max_kl > 0 else 0.0
        else:
            frame_competition = 0.0
        
        # æƒ…æ„Ÿåˆ†æ
        positive_count = sum(1 for word in words if word in self.positive_words)
        negative_count = sum(1 for word in words if word in self.negative_words)
        emotional_intensity = (positive_count + negative_count) / len(words)
        
        reality_strength = (
            superposition_strength * 0.4 +
            semantic_interference * 0.3 +
            frame_competition * 0.2 +
            emotional_intensity * 0.1
        )
        
        return {
            'field': field_name,
            'original_text': text[:100] + '...' if len(text) > 100 else text,
            'word_count': len(words),
            'unique_words': len(set(words)),
            'categorical_diversity': len(set(pos_tags)),
            'quantum_circuit_qubits': 0,  # æ ‡è®°ä¸ºç»å…¸è®¡ç®—
            'von_neumann_entropy': von_neumann_entropy,
            'superposition_strength': superposition_strength,
            'quantum_coherence': quantum_coherence,
            'semantic_interference': semantic_interference,
            'frame_competition': frame_competition,
            'emotional_intensity': float(emotional_intensity),
            'multiple_reality_strength': float(reality_strength),
            'analysis_version': 'fast_classical_fallback_density_matrix_v1.0'
        }

    def process_record_batch(self, records: List[Dict], record_type: str) -> List[Dict]:
        """æ‰¹å¤„ç†è®°å½•"""
        results = []
        
        for record in records:
            record_id = record.get('id', 0)
            
            if record_type == 'ai':
                # AIè®°å½•çš„å­—æ®µ
                fields = ['æ–°èæ¨™é¡Œ', 'å½±ç‰‡å°è©±', 'å½±ç‰‡æè¿°']
                for field in fields:
                    if field in record and record[field]:
                        result = self.fast_quantum_analysis(record[field], field)
                        if result:
                            result['record_id'] = record_id
                            result['data_source'] = 'AI_Generated'
                            results.append(result)
            
            elif record_type == 'journalist':
                # è®°è€…è®°å½•çš„å­—æ®µ
                field_mapping = {'title': 'æ–°èæ¨™é¡Œ', 'content': 'æ–°èå…§å®¹'}
                for original_field, mapped_field in field_mapping.items():
                    if original_field in record and record[original_field]:
                        result = self.fast_quantum_analysis(record[original_field], mapped_field)
                        if result:
                            result['record_id'] = record_id
                            result['data_source'] = 'Journalist_Written'
                            results.append(result)
        
        return results

def main():
    """ä¸»å‡½æ•°ï¼šå¿«é€Ÿæ‰§è¡ŒQiskité‡å­åˆ†æï¼ˆå¯†åº¦çŸ©é™£ç‰ˆæœ¬ï¼‰"""
    print("ğŸš€ å¼€å§‹å¿«é€ŸQiskité‡å­ç”µè·¯åˆ†æï¼ˆä½¿ç”¨å¯†åº¦çŸ©é™£è¨ˆç®—ç†µï¼‰...")
    print("=" * 80)
    start_time = time.time()
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    output_dir = project_root / '20251113_densityMatrix'
    output_dir.mkdir(exist_ok=True)
    results_dir = output_dir / 'results'
    results_dir.mkdir(exist_ok=True)
    
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = FastQiskitDensityMatrixAnalyzer()
    
    # 1. åˆ†æAIæ–°é—»æ•°æ®
    print("\nğŸ“Š åˆ†æAIç”Ÿæˆæ–°é—»...")
    ai_data_path = project_root / 'data' / 'dataseet.xlsx'
    
    if ai_data_path.exists():
        ai_df = pd.read_excel(ai_data_path)
        print(f"âœ… åŠ è½½AIæ•°æ®: {len(ai_df)} æ¡è®°å½•")
        
        ai_records = []
        for idx, record in ai_df.iterrows():
            record_dict = record.to_dict()
            record_dict['id'] = idx
            ai_records.append(record_dict)
        
        # æ‰¹å¤„ç†åˆ†æ
        ai_results = analyzer.process_record_batch(ai_records, 'ai')
        
        # ä¿å­˜ç»“æœ
        if ai_results:
            ai_results_df = pd.DataFrame(ai_results)
            ai_results_path = results_dir / 'density_matrix_ai_analysis_results.csv'
            ai_results_df.to_csv(ai_results_path, index=False, encoding='utf-8-sig')
            print(f"âœ… AIåˆ†æç»“æœå·²ä¿å­˜: {ai_results_path}")
            
            # ç”Ÿæˆç»Ÿè®¡æ‘˜è¦
            numeric_cols = ['von_neumann_entropy', 'superposition_strength', 'quantum_coherence', 
                           'semantic_interference', 'frame_competition', 'multiple_reality_strength']
            ai_summary = {}
            for col in numeric_cols:
                if col in ai_results_df.columns:
                    ai_summary[col] = {
                        'mean': float(ai_results_df[col].mean()),
                        'std': float(ai_results_df[col].std()),
                        'min': float(ai_results_df[col].min()),
                        'max': float(ai_results_df[col].max()),
                        'median': float(ai_results_df[col].median())
                    }
            
            ai_summary_path = results_dir / 'density_matrix_ai_analysis_summary.json'
            with open(ai_summary_path, 'w', encoding='utf-8') as f:
                json.dump(ai_summary, f, ensure_ascii=False, indent=2)
            print(f"âœ… AIç»Ÿè®¡æ‘˜è¦å·²ä¿å­˜: {ai_summary_path}")
    else:
        print(f"âŒ æ‰¾ä¸åˆ°AIæ•°æ®æ–‡ä»¶: {ai_data_path}")
        ai_results_df = None
    
    # 2. åˆ†æè®°è€…æ–°é—»æ•°æ®
    print("\nğŸ“Š åˆ†æè®°è€…æ’°å†™æ–°é—»...")
    journalist_data_path = project_root / 'data' / 'cna.csv'
    
    if journalist_data_path.exists():
        journalist_df = pd.read_csv(journalist_data_path)
        print(f"âœ… åŠ è½½è®°è€…æ•°æ®: {len(journalist_df)} æ¡è®°å½•")
        
        journalist_records = []
        for idx, record in journalist_df.iterrows():
            record_dict = record.to_dict()
            record_dict['id'] = idx
            journalist_records.append(record_dict)
        
        # æ‰¹å¤„ç†åˆ†æ
        journalist_results = analyzer.process_record_batch(journalist_records, 'journalist')
        
        # ä¿å­˜ç»“æœ
        if journalist_results:
            journalist_results_df = pd.DataFrame(journalist_results)
            journalist_results_path = results_dir / 'density_matrix_journalist_analysis_results.csv'
            journalist_results_df.to_csv(journalist_results_path, index=False, encoding='utf-8-sig')
            print(f"âœ… è®°è€…åˆ†æç»“æœå·²ä¿å­˜: {journalist_results_path}")
            
            # ç”Ÿæˆç»Ÿè®¡æ‘˜è¦
            numeric_cols = ['von_neumann_entropy', 'superposition_strength', 'quantum_coherence', 
                           'semantic_interference', 'frame_competition', 'multiple_reality_strength']
            journalist_summary = {}
            for col in numeric_cols:
                if col in journalist_results_df.columns:
                    journalist_summary[col] = {
                        'mean': float(journalist_results_df[col].mean()),
                        'std': float(journalist_results_df[col].std()),
                        'min': float(journalist_results_df[col].min()),
                        'max': float(journalist_results_df[col].max()),
                        'median': float(journalist_results_df[col].median())
                    }
            
            journalist_summary_path = results_dir / 'density_matrix_journalist_analysis_summary.json'
            with open(journalist_summary_path, 'w', encoding='utf-8') as f:
                json.dump(journalist_summary, f, ensure_ascii=False, indent=2)
            print(f"âœ… è®°è€…ç»Ÿè®¡æ‘˜è¦å·²ä¿å­˜: {journalist_summary_path}")
    else:
        print(f"âŒ æ‰¾ä¸åˆ°è®°è€…æ•°æ®æ–‡ä»¶: {journalist_data_path}")
        journalist_results_df = None
    
    end_time = time.time()
    total_time = end_time - start_time
    
    print(f"\nğŸ‰ å¯†åº¦çŸ©é™£ç‰ˆæœ¬Qiskité‡å­ç”µè·¯åˆ†æå®Œæˆï¼")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f} ç§’")
    print(f"ğŸ“Š åˆ†ææ¨¡å¼: ä½¿ç”¨å¯†åº¦çŸ©é™£è¨ˆç®— von Neumann ç†µ")
    print(f"ğŸ”¬ ä½¿ç”¨æŠ€æœ¯: å¯†åº¦çŸ©é™£ (Ï = |ÏˆâŸ©âŸ¨Ïˆ|) + ç‰¹å¾µå€¼åˆ†è§£")
    if ai_results_df is not None and journalist_results_df is not None:
        print(f"ğŸ“ˆ æ•°æ®è§„æ¨¡: AIæ–°é—»{len(ai_results_df)}æ¡ + è®°è€…æ–°é—»{len(journalist_results_df)}æ¡")
    print(f"ğŸ“ çµæœä¿å­˜ç›®éŒ„: {output_dir}")

if __name__ == "__main__":
    main()

