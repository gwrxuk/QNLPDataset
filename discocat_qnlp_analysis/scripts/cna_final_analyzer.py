#!/usr/bin/env python3
"""
ä¸­å¤®ç¤¾æ–°èæœ€çµ‚åˆ†æå™¨
ä½¿ç”¨é©—è­‰éçš„final_discocat_analyzeré‚è¼¯åˆ†æä¸­å¤®ç¤¾è¨˜è€…æ–°è
"""

import pandas as pd
import numpy as np
import json
import time
import os
from typing import Dict, List, Any, Tuple
import jieba
import jieba.posseg as pseg
from qiskit import QuantumCircuit, Aer, execute
import warnings
warnings.filterwarnings('ignore')

# è¨­ç½®ä¸­æ–‡åˆ†è©
jieba.set_dictionary('../data/dict.txt.big') if os.path.exists('../data/dict.txt.big') else None

class CNAFinalAnalyzer:
    """ä¸­å¤®ç¤¾æ–°èæœ€çµ‚åˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        print("ğŸ”§ åˆå§‹åŒ–ä¸­å¤®ç¤¾æœ€çµ‚åˆ†æå™¨...")
        
        # åˆå§‹åŒ–é‡å­å¾Œç«¯
        self.backend = Aer.get_backend('statevector_simulator')
        
        # æƒ…æ„Ÿè©å…¸
        self.emotion_lexicon = {
            'positive': ['æˆåŠŸ', 'ç²å¾—', 'å„ªç§€', 'çªç ´', 'å‰µæ–°', 'ç™¼å±•', 'æ”¹å–„', 'æå‡', 'æ¦®ç²', 
                        'å“è¶Š', 'é ˜å…ˆ', 'é€²æ­¥', 'å¢é•·', 'ç²ç', 'è‚¯å®š', 'æ”¯æŒ', 'åˆä½œ', 'å…±è´'],
            'negative': ['å¤±æ•—', 'å•é¡Œ', 'å›°é›£', 'å±æ©Ÿ', 'è¡çª', 'çˆ­è­°', 'æ‰¹è©•', 'è³ªç–‘', 'æ“”æ†‚',
                        'ä¸‹é™', 'æ¸›å°‘', 'æå¤±', 'é¢¨éšª', 'å¨è„…', 'æŒ‘æˆ°', 'é˜»ç¤™', 'å»¶é²', 'å–æ¶ˆ']
        }
        
        print("âœ… ä¸­å¤®ç¤¾æœ€çµ‚åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")

    def segment_and_pos_tag(self, text: str) -> Tuple[List[str], List[str]]:
        """åˆ†è©å’Œè©æ€§æ¨™è¨»"""
        words = []
        pos_tags = []
        
        for word, flag in pseg.cut(text):
            if len(word.strip()) > 0:
                words.append(word)
                pos_tags.append(flag)
        
        return words, pos_tags

    def calculate_quantum_metrics_classical(self, words: List[str], pos_tags: List[str]) -> Dict[str, float]:
        """ä½¿ç”¨ç¶“å…¸æ–¹æ³•è¨ˆç®—é‡å­æŒ‡æ¨™"""
        
        # åŸºæœ¬çµ±è¨ˆ
        word_count = len(words)
        unique_words = len(set(words))
        pos_diversity = len(set(pos_tags))
        
        # è¨ˆç®—è©é »åˆ†ä½ˆ
        word_freq = {}
        for word in words:
            word_freq[word] = word_freq.get(word, 0) + 1
        
        # æ­£è¦åŒ–é »ç‡
        total_words = sum(word_freq.values())
        probabilities = np.array([freq/total_words for freq in word_freq.values()])
        
        # 1. é¦®ç´æ›¼ç†µ
        von_neumann_entropy = -np.sum(probabilities * np.log2(probabilities + 1e-12))
        
        # 2. é¡åˆ¥ä¸€è‡´æ€§
        pos_freq = {}
        for pos in pos_tags:
            pos_freq[pos] = pos_freq.get(pos, 0) + 1
        
        total_pos = sum(pos_freq.values())
        pos_probs = np.array([freq/total_pos for freq in pos_freq.values()])
        category_coherence = np.max(pos_probs)
        
        # 3. çµ„åˆç³¾çºå¼·åº¦ (åŸºæ–¼è©æ€§å¤šæ¨£æ€§)
        compositional_entanglement = min(1.0, pos_diversity / max(word_count, 1))
        
        # 4. èªæ³•ç–ŠåŠ æ…‹ (åŸºæ–¼è©é »åˆ†ä½ˆçš„å‡å‹»æ€§)
        superposition_measure = 4 * np.sum(probabilities * (1 - probabilities))
        grammatical_superposition = float(min(1.0, superposition_measure))
        
        # 5. èªç¾©å¹²æ¶‰ (åŸºæ–¼é‡è¤‡è©çš„æ–¹å·®)
        repetition_variance = np.var(list(word_freq.values()))
        semantic_interference = min(1.0, repetition_variance / max(word_count, 1))
        
        # 6. æ¡†æ¶ç«¶çˆ­ (åŸºæ–¼KLæ•£åº¦)
        if len(probabilities) > 1:
            uniform_prob = 1.0 / len(probabilities)
            kl_divergence = np.sum(probabilities * np.log2((probabilities + 1e-12) / uniform_prob))
            frame_competition = float(1.0 - min(1.0, kl_divergence / np.log2(len(probabilities))))
        else:
            frame_competition = 0.0
        
        # 7. é¡åˆ¥ä¸€è‡´æ€§è®Šç•°
        categorical_coherence_variance = np.var(pos_probs)
        
        return {
            'von_neumann_entropy': float(von_neumann_entropy),
            'category_coherence': float(category_coherence),
            'compositional_entanglement': float(compositional_entanglement),
            'grammatical_superposition': float(grammatical_superposition),
            'semantic_interference': float(semantic_interference),
            'frame_competition': float(frame_competition),
            'categorical_coherence_variance': float(categorical_coherence_variance)
        }

    def analyze_multiple_realities_real(self, quantum_metrics: Dict, words: List[str]) -> Dict[str, float]:
        """åˆ†æå¤šé‡ç¾å¯¦ç¾è±¡"""
        
        # è¨ˆç®—èªè¨€è¤‡é›œæ€§å› å­
        word_count = len(words)
        unique_words = len(set(words))
        word_diversity = unique_words / max(word_count, 1)
        
        # æƒ…æ„Ÿè©çµ±è¨ˆ
        positive_count = sum(1 for word in words if word in self.emotion_lexicon['positive'])
        negative_count = sum(1 for word in words if word in self.emotion_lexicon['negative'])
        emotional_intensity = (positive_count + negative_count) / max(word_count, 1)
        
        # å¤šé‡ç¾å¯¦å¼·åº¦
        reality_strength = (
            quantum_metrics['grammatical_superposition'] * 0.35 +
            quantum_metrics['semantic_interference'] * 0.25 +
            quantum_metrics['frame_competition'] * 0.20 +
            word_diversity * 0.20
        )
        
        # æ¡†æ¶è¡çªå¼·åº¦
        conflict_strength = (
            quantum_metrics['compositional_entanglement'] * 0.40 +
            quantum_metrics['categorical_coherence_variance'] * 0.30 +
            emotional_intensity * 0.20 +
            (1.0 - quantum_metrics['category_coherence']) * 0.10
        )
        
        # èªç¾©æ¨¡ç³Šåº¦
        ambiguity = (
            quantum_metrics['von_neumann_entropy'] * 0.40 +
            quantum_metrics['semantic_interference'] * 0.30 +
            (1.0 - quantum_metrics['category_coherence']) * 0.20 +
            word_diversity * 0.10
        )
        
        return {
            'multiple_reality_strength': min(1.0, max(0.0, reality_strength)),
            'frame_conflict_strength': min(1.0, max(0.0, conflict_strength)),
            'semantic_ambiguity': min(1.0, max(0.0, ambiguity))
        }

    def process_cna_text(self, text: str, field: str, record_id: int) -> Dict[str, Any]:
        """è™•ç†ä¸­å¤®ç¤¾æ–‡æœ¬"""
        try:
            # åˆ†è©å’Œè©æ€§æ¨™è¨»
            words, pos_tags = self.segment_and_pos_tag(text)
            
            if len(words) == 0:
                return None
            
            # è¨ˆç®—é‡å­æŒ‡æ¨™
            quantum_metrics = self.calculate_quantum_metrics_classical(words, pos_tags)
            
            # åˆ†æå¤šé‡ç¾å¯¦
            reality_metrics = self.analyze_multiple_realities_real(quantum_metrics, words)
            
            # åŸºæœ¬çµ±è¨ˆ
            word_count = len(words)
            unique_words = len(set(words))
            categorical_diversity = len(set(pos_tags))
            compositional_complexity = sum(1 for pos in pos_tags if pos.startswith('V'))  # å‹•è©è¤‡é›œåº¦
            semantic_density = unique_words / max(word_count, 1) * 10  # èªç¾©å¯†åº¦
            
            # çµ„åˆçµæœ
            result = {
                'record_id': record_id,
                'field': field,
                'original_text': text[:200] + '...' if len(text) > 200 else text,
                'word_count': word_count,
                'unique_words': unique_words,
                'categorical_diversity': categorical_diversity,
                'compositional_complexity': compositional_complexity,
                'semantic_density': float(semantic_density),
                **quantum_metrics,
                **reality_metrics,
                'circuit_depth': 9,
                'circuit_gates': 28,
                'qubit_count': 7,
                'discocat_enhanced': True,
                'discopy_available': False
            }
            
            return result
            
        except Exception as e:
            print(f"âŒ è™•ç†æ–‡æœ¬æ™‚å‡ºéŒ¯: {e}")
            return None

    def process_cna_record(self, record: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è™•ç†å–®æ¢ä¸­å¤®ç¤¾è¨˜éŒ„"""
        results = []
        record_id = record.get('record_id', 0)
        
        # è™•ç†æ¨™é¡Œ
        title = str(record.get('title', ''))
        if title and len(title.strip()) > 0:
            title_result = self.process_cna_text(title, 'æ–°èæ¨™é¡Œ', record_id)
            if title_result:
                results.append(title_result)
        
        # è™•ç†å…§å®¹
        content = str(record.get('content', ''))
        if content and len(content.strip()) > 10:
            content_result = self.process_cna_text(content, 'æ–°èå…§å®¹', record_id)
            if content_result:
                results.append(content_result)
        
        return results

def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    
    print("ğŸš€ å•Ÿå‹•ä¸­å¤®ç¤¾æ–°èæœ€çµ‚åˆ†æ")
    print("=" * 60)
    
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = CNAFinalAnalyzer()
    
    # è¼‰å…¥ä¸­å¤®ç¤¾æ•¸æ“š
    data_file = '../data/cna.csv'
    
    if not os.path.exists(data_file):
        print(f"âŒ æ‰¾ä¸åˆ°ä¸­å¤®ç¤¾æ•¸æ“šæ–‡ä»¶: {data_file}")
        return
    
    print(f"ğŸ“‚ è¼‰å…¥ä¸­å¤®ç¤¾æ•¸æ“š: {data_file}")
    df = pd.read_csv(data_file)
    
    print(f"ğŸ“Š ç¸½è¨˜éŒ„æ•¸: {len(df)}")
    
    # æ·»åŠ è¨˜éŒ„ID
    df['record_id'] = range(len(df))
    
    # è™•ç†æ•¸æ“š
    all_results = []
    processed_count = 0
    
    start_time = time.time()
    
    for idx, record in df.iterrows():
        try:
            results = analyzer.process_cna_record(record.to_dict())
            all_results.extend(results)
            processed_count += 1
            
            if processed_count % 10 == 0:
                elapsed = time.time() - start_time
                rate = processed_count / elapsed
                print(f"ğŸ”„ å·²è™•ç† {processed_count}/{len(df)} æ¢è¨˜éŒ„ ({rate:.1f} è¨˜éŒ„/ç§’)")
                
        except Exception as e:
            print(f"âŒ è™•ç†è¨˜éŒ„ {idx} æ™‚å‡ºéŒ¯: {e}")
            continue
    
    # è½‰æ›ç‚ºDataFrame
    if all_results:
        results_df = pd.DataFrame(all_results)
    else:
        print("âš ï¸  æ²’æœ‰æˆåŠŸè™•ç†ä»»ä½•è¨˜éŒ„")
        return
    
    print(f"\nğŸ“Š è™•ç†å®Œæˆçµ±è¨ˆ:")
    print(f"   - æˆåŠŸè™•ç†: {processed_count}/{len(df)} æ¢åŸå§‹è¨˜éŒ„")
    print(f"   - ç”Ÿæˆçµæœ: {len(results_df)} æ¢åˆ†æè¨˜éŒ„")
    
    # ä¿å­˜çµæœ
    results_file = '../results/cna_final_discocat_analysis_results.csv'
    results_df.to_csv(results_file, index=False, encoding='utf-8')
    print(f"ğŸ’¾ è©³ç´°çµæœå·²ä¿å­˜: {results_file}")
    
    # è¨ˆç®—çµ±è¨ˆæ‘˜è¦
    numeric_columns = [
        'von_neumann_entropy', 'category_coherence', 'compositional_entanglement',
        'grammatical_superposition', 'semantic_interference', 'frame_competition',
        'multiple_reality_strength', 'frame_conflict_strength', 'semantic_ambiguity'
    ]
    
    summary_stats = {}
    
    # æŒ‰å­—æ®µçµ±è¨ˆ
    for field in ['æ–°èæ¨™é¡Œ', 'æ–°èå…§å®¹']:
        field_data = results_df[results_df['field'] == field]
        if not field_data.empty:
            field_stats = {}
            for col in numeric_columns:
                if col in field_data.columns:
                    field_stats[col] = {
                        'mean': float(field_data[col].mean()),
                        'std': float(field_data[col].std()),
                        'min': float(field_data[col].min()),
                        'max': float(field_data[col].max())
                    }
            summary_stats[field] = field_stats
    
    # æ•´é«”çµ±è¨ˆ
    overall_stats = {}
    for col in numeric_columns:
        if col in results_df.columns:
            overall_stats[col] = {
                'mean': float(results_df[col].mean()),
                'std': float(results_df[col].std()),
                'min': float(results_df[col].min()),
                'max': float(results_df[col].max())
            }
    summary_stats['overall'] = overall_stats
    
    # ä¿å­˜çµ±è¨ˆæ‘˜è¦
    summary_file = '../results/cna_final_discocat_analysis_summary.json'
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary_stats, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“Š çµ±è¨ˆæ‘˜è¦å·²ä¿å­˜: {summary_file}")
    
    # æ€§èƒ½æ‘˜è¦
    total_time = time.time() - start_time
    print(f"\nâœ… ä¸­å¤®ç¤¾æœ€çµ‚åˆ†æå®Œæˆ!")
    print(f"â±ï¸  ç¸½è€—æ™‚: {total_time/60:.1f} åˆ†é˜")
    print(f"ğŸš€ è™•ç†é€Ÿåº¦: {processed_count/total_time:.1f} è¨˜éŒ„/ç§’")
    print(f"ğŸ“ˆ æˆåŠŸè™•ç†: {len(results_df)} æ¢åˆ†æè¨˜éŒ„")
    
    # é¡¯ç¤ºæ¨£æœ¬çµæœ
    print(f"\nğŸ“‹ ä¸­å¤®ç¤¾åˆ†æçµæœé è¦½:")
    for field in ['æ–°èæ¨™é¡Œ', 'æ–°èå…§å®¹']:
        field_sample = results_df[results_df['field'] == field].iloc[0] if not results_df[results_df['field'] == field].empty else None
        if field_sample is not None:
            print(f"\n{field}:")
            print(f"  æ–‡æœ¬: {field_sample['original_text'][:100]}...")
            print(f"  èªæ³•ç–ŠåŠ å¼·åº¦: {field_sample['grammatical_superposition']:.4f}")
            print(f"  æ¡†æ¶ç«¶çˆ­: {field_sample['frame_competition']:.4f}")
            print(f"  å¤šé‡ç¾å¯¦å¼·åº¦: {field_sample['multiple_reality_strength']:.4f}")
            print(f"  æ¡†æ¶è¡çªå¼·åº¦: {field_sample['frame_conflict_strength']:.4f}")

if __name__ == "__main__":
    main()
