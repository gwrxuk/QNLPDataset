#!/usr/bin/env python3
"""
æ— é™åˆ¶é‡å­åˆ†æå™¨ - ç§»é™¤æ‰€æœ‰min(1.0)é™åˆ¶
é‡æ–°è®¡ç®—çœŸå®çš„é‡å­ç‰¹å¾æ•°å€¼
"""

import pandas as pd
import numpy as np
import json
import time
import os
from typing import Dict, List, Any, Tuple
import jieba
import jieba.posseg as pseg
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡åˆ†è¯
jieba.set_dictionary('../data/dict.txt.big') if os.path.exists('../data/dict.txt.big') else None

class UnrestrictedQuantumAnalyzer:
    """æ— é™åˆ¶é‡å­åˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        print("ğŸ”§ åˆå§‹åŒ–æ— é™åˆ¶é‡å­åˆ†æå™¨...")
        
        # æƒ…æ„Ÿè¯å…¸
        self.emotion_lexicon = {
            'positive': ['æˆåŠŸ', 'è·å¾—', 'ä¼˜ç§€', 'çªç ´', 'åˆ›æ–°', 'å‘å±•', 'æ”¹å–„', 'æå‡', 'è£è·', 
                        'å“è¶Š', 'é¢†å…ˆ', 'è¿›æ­¥', 'å¢é•¿', 'è·å¥–', 'è‚¯å®š', 'æ”¯æŒ', 'åˆä½œ', 'å…±èµ¢'],
            'negative': ['å¤±è´¥', 'é—®é¢˜', 'å›°éš¾', 'å±æœº', 'å†²çª', 'äº‰è®®', 'æ‰¹è¯„', 'è´¨ç–‘', 'æ‹…å¿§',
                        'ä¸‹é™', 'å‡å°‘', 'æŸå¤±', 'é£é™©', 'å¨èƒ', 'æŒ‘æˆ˜', 'é˜»ç¢', 'å»¶è¿Ÿ', 'å–æ¶ˆ']
        }
        
        print("âœ… æ— é™åˆ¶é‡å­åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")

    def segment_and_pos_tag(self, text: str) -> Tuple[List[str], List[str]]:
        """åˆ†è¯å’Œè¯æ€§æ ‡æ³¨"""
        words = []
        pos_tags = []
        
        for word, flag in pseg.cut(text):
            if len(word.strip()) > 0:
                words.append(word)
                pos_tags.append(flag)
        
        return words, pos_tags

    def calculate_unrestricted_quantum_metrics(self, words: List[str], pos_tags: List[str]) -> Dict[str, float]:
        """è®¡ç®—æ— é™åˆ¶çš„é‡å­æŒ‡æ ‡"""
        
        # åŸºæœ¬ç»Ÿè®¡
        word_count = len(words)
        unique_words = len(set(words))
        pos_diversity = len(set(pos_tags))
        
        if word_count == 0:
            return self._get_zero_metrics()
        
        # è®¡ç®—è¯é¢‘åˆ†å¸ƒ
        word_freq = {}
        for word in words:
            word_freq[word] = word_freq.get(word, 0) + 1
        
        # æ­£è§„åŒ–é¢‘ç‡
        total_words = sum(word_freq.values())
        probabilities = np.array([freq/total_words for freq in word_freq.values()])
        
        # 1. å†¯çº½æ›¼ç†µï¼ˆæ— é™åˆ¶ï¼‰
        von_neumann_entropy = -np.sum(probabilities * np.log2(probabilities + 1e-12))
        
        # 2. ç±»åˆ«ä¸€è‡´æ€§
        pos_freq = {}
        for pos in pos_tags:
            pos_freq[pos] = pos_freq.get(pos, 0) + 1
        
        total_pos = sum(pos_freq.values())
        pos_probs = np.array([freq/total_pos for freq in pos_freq.values()])
        category_coherence = np.max(pos_probs)
        
        # 3. ç»„åˆçº ç¼ å¼ºåº¦ï¼ˆæ— é™åˆ¶ï¼‰
        compositional_entanglement = pos_diversity / word_count
        
        # 4. è¯­æ³•å åŠ æ€ï¼ˆæ— é™åˆ¶ - è¿™æ˜¯å…³é”®ï¼ï¼‰
        superposition_measure = 4 * np.sum(probabilities * (1 - probabilities))
        grammatical_superposition = float(superposition_measure)  # ç§»é™¤min(1.0)é™åˆ¶ï¼
        
        # 5. è¯­ä¹‰å¹²æ¶‰ï¼ˆæ— é™åˆ¶ï¼‰
        repetition_variance = np.var(list(word_freq.values()))
        semantic_interference = repetition_variance / word_count
        
        # 6. æ¡†æ¶ç«äº‰ï¼ˆæ— é™åˆ¶ï¼‰
        if len(probabilities) > 1:
            uniform_prob = 1.0 / len(probabilities)
            kl_divergence = np.sum(probabilities * np.log2((probabilities + 1e-12) / uniform_prob))
            max_kl = np.log2(len(probabilities))
            frame_competition = float(1.0 - (kl_divergence / max_kl))  # ä¿æŒåŸå§‹è®¡ç®—
        else:
            frame_competition = 0.0
        
        # 7. ç±»åˆ«ä¸€è‡´æ€§å˜å¼‚ï¼ˆæ— é™åˆ¶ï¼‰
        categorical_coherence_variance = np.var(pos_probs)
        
        return {
            'von_neumann_entropy': float(von_neumann_entropy),
            'category_coherence': float(category_coherence),
            'compositional_entanglement': float(compositional_entanglement),
            'grammatical_superposition': float(grammatical_superposition),  # çœŸå®å€¼ï¼
            'semantic_interference': float(semantic_interference),
            'frame_competition': float(frame_competition),
            'categorical_coherence_variance': float(categorical_coherence_variance)
        }

    def _get_zero_metrics(self):
        """è¿”å›é›¶å€¼æŒ‡æ ‡"""
        return {
            'von_neumann_entropy': 0.0,
            'category_coherence': 0.0,
            'compositional_entanglement': 0.0,
            'grammatical_superposition': 0.0,
            'semantic_interference': 0.0,
            'frame_competition': 0.0,
            'categorical_coherence_variance': 0.0
        }

    def analyze_multiple_realities_unrestricted(self, quantum_metrics: Dict, words: List[str]) -> Dict[str, float]:
        """åˆ†æå¤šé‡ç°å®ç°è±¡ï¼ˆæ— é™åˆ¶ï¼‰"""
        
        # è®¡ç®—è¯­è¨€å¤æ‚æ€§å› å­
        word_count = len(words)
        unique_words = len(set(words))
        word_diversity = unique_words / max(word_count, 1)
        
        # æƒ…æ„Ÿè¯ç»Ÿè®¡
        positive_count = sum(1 for word in words if word in self.emotion_lexicon['positive'])
        negative_count = sum(1 for word in words if word in self.emotion_lexicon['negative'])
        emotional_intensity = (positive_count + negative_count) / max(word_count, 1)
        
        # å¤šé‡ç°å®å¼ºåº¦ï¼ˆæ— é™åˆ¶ï¼‰
        reality_strength = (
            quantum_metrics['grammatical_superposition'] * 0.35 +  # ç°åœ¨ä½¿ç”¨çœŸå®å€¼
            quantum_metrics['semantic_interference'] * 0.25 +
            quantum_metrics['frame_competition'] * 0.20 +
            word_diversity * 0.20
        )
        
        # æ¡†æ¶å†²çªå¼ºåº¦ï¼ˆæ— é™åˆ¶ï¼‰
        conflict_strength = (
            quantum_metrics['compositional_entanglement'] * 0.40 +
            quantum_metrics['categorical_coherence_variance'] * 0.30 +
            emotional_intensity * 0.20 +
            (1.0 - quantum_metrics['category_coherence']) * 0.10
        )
        
        # è¯­ä¹‰æ¨¡ç³Šåº¦ï¼ˆæ— é™åˆ¶ï¼‰
        ambiguity = (
            quantum_metrics['von_neumann_entropy'] * 0.40 +
            quantum_metrics['semantic_interference'] * 0.30 +
            (1.0 - quantum_metrics['category_coherence']) * 0.20 +
            word_diversity * 0.10
        )
        
        return {
            'multiple_reality_strength': float(reality_strength),  # ä¸å†é™åˆ¶åœ¨1.0
            'frame_conflict_strength': float(conflict_strength),   # ä¸å†é™åˆ¶åœ¨1.0
            'semantic_ambiguity': float(ambiguity)                 # ä¸å†é™åˆ¶åœ¨1.0
        }

    def process_text_unrestricted(self, text: str, field: str, record_id: int) -> Dict[str, Any]:
        """å¤„ç†æ–‡æœ¬ï¼ˆæ— é™åˆ¶ï¼‰"""
        try:
            # åˆ†è¯å’Œè¯æ€§æ ‡æ³¨
            words, pos_tags = self.segment_and_pos_tag(text)
            
            if len(words) == 0:
                return None
            
            # è®¡ç®—é‡å­æŒ‡æ ‡ï¼ˆæ— é™åˆ¶ï¼‰
            quantum_metrics = self.calculate_unrestricted_quantum_metrics(words, pos_tags)
            
            # åˆ†æå¤šé‡ç°å®ï¼ˆæ— é™åˆ¶ï¼‰
            reality_metrics = self.analyze_multiple_realities_unrestricted(quantum_metrics, words)
            
            # åŸºæœ¬ç»Ÿè®¡
            word_count = len(words)
            unique_words = len(set(words))
            categorical_diversity = len(set(pos_tags))
            compositional_complexity = sum(1 for pos in pos_tags if pos.startswith('V'))  # åŠ¨è¯å¤æ‚åº¦
            semantic_density = unique_words / max(word_count, 1) * 10  # è¯­ä¹‰å¯†åº¦
            
            # ç»„åˆç»“æœ
            result = {
                'record_id': record_id,
                'field': field,
                'original_text': text[:200] + '...' if len(text) > 200 else text,
                'word_count': word_count,
                'unique_words': unique_words,
                'categorical_diversity': categorical_diversity,
                'compositional_complexity': compositional_complexity,
                'semantic_density': float(semantic_density),
                **quantum_metrics,
                **reality_metrics,
                'analysis_version': 'unrestricted_v1.0'
            }
            
            return result
            
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡æœ¬æ—¶å‡ºé”™: {e}")
            return None

    def process_record(self, record: Dict[str, Any], data_type: str) -> List[Dict[str, Any]]:
        """å¤„ç†å•æ¡è®°å½•"""
        results = []
        record_id = record.get('record_id', 0)
        
        if data_type == 'ai':
            # AIæ–°é—»æ•°æ®
            title = str(record.get('æ–°èæ¨™é¡Œ', ''))
            dialogue = str(record.get('å½±ç‰‡å°è©±', ''))
            description = str(record.get('å½±ç‰‡æè¿°', ''))
            
            if title and len(title.strip()) > 0:
                title_result = self.process_text_unrestricted(title, 'æ–°èæ¨™é¡Œ', record_id)
                if title_result:
                    results.append(title_result)
            
            if dialogue and len(dialogue.strip()) > 10:
                dialogue_result = self.process_text_unrestricted(dialogue, 'å½±ç‰‡å°è©±', record_id)
                if dialogue_result:
                    results.append(dialogue_result)
            
            if description and len(description.strip()) > 10:
                description_result = self.process_text_unrestricted(description, 'å½±ç‰‡æè¿°', record_id)
                if description_result:
                    results.append(description_result)
        
        elif data_type == 'journalist':
            # è®°è€…æ–°é—»æ•°æ®
            title = str(record.get('title', ''))
            content = str(record.get('content', ''))
            
            if title and len(title.strip()) > 0:
                title_result = self.process_text_unrestricted(title, 'æ–°èæ¨™é¡Œ', record_id)
                if title_result:
                    results.append(title_result)
            
            if content and len(content.strip()) > 10:
                content_result = self.process_text_unrestricted(content, 'æ–°èå…§å®¹', record_id)
                if content_result:
                    results.append(content_result)
        
        return results

def analyze_ai_news():
    """åˆ†æAIæ–°é—»"""
    print("ğŸ“° å¼€å§‹åˆ†æAIæ–°é—»...")
    
    analyzer = UnrestrictedQuantumAnalyzer()
    
    # åŠ è½½AIæ–°é—»æ•°æ®
    data_file = '../data/dataseet.xlsx'
    if not os.path.exists(data_file):
        print(f"âŒ æ‰¾ä¸åˆ°AIæ–°é—»æ•°æ®æ–‡ä»¶: {data_file}")
        return None
    
    df = pd.read_excel(data_file)
    df = df.dropna(subset=['æ–°èæ¨™é¡Œ', 'å½±ç‰‡å°è©±', 'å½±ç‰‡æè¿°'])
    df['record_id'] = range(len(df))
    
    print(f"ğŸ“Š AIæ–°é—»æ€»è®°å½•æ•°: {len(df)}")
    
    # å¤„ç†æ•°æ®
    all_results = []
    for idx, record in df.iterrows():
        try:
            results = analyzer.process_record(record.to_dict(), 'ai')
            all_results.extend(results)
            
            if (idx + 1) % 100 == 0:
                print(f"ğŸ”„ å·²å¤„ç† {idx + 1}/{len(df)} æ¡AIæ–°é—»è®°å½•")
                
        except Exception as e:
            print(f"âŒ å¤„ç†AIæ–°é—»è®°å½• {idx} æ—¶å‡ºé”™: {e}")
            continue
    
    return pd.DataFrame(all_results) if all_results else pd.DataFrame()

def analyze_journalist_news():
    """åˆ†æè®°è€…æ–°é—»"""
    print("ğŸ‘¨â€ğŸ’¼ å¼€å§‹åˆ†æè®°è€…æ–°é—»...")
    
    analyzer = UnrestrictedQuantumAnalyzer()
    
    # åŠ è½½è®°è€…æ–°é—»æ•°æ®
    data_file = '../data/cna.csv'
    if not os.path.exists(data_file):
        print(f"âŒ æ‰¾ä¸åˆ°è®°è€…æ–°é—»æ•°æ®æ–‡ä»¶: {data_file}")
        return None
    
    df = pd.read_csv(data_file)
    df = df.dropna(subset=['title', 'content'])
    df['record_id'] = range(len(df))
    
    print(f"ğŸ“Š è®°è€…æ–°é—»æ€»è®°å½•æ•°: {len(df)}")
    
    # å¤„ç†æ•°æ®
    all_results = []
    for idx, record in df.iterrows():
        try:
            results = analyzer.process_record(record.to_dict(), 'journalist')
            all_results.extend(results)
            
            if (idx + 1) % 10 == 0:
                print(f"ğŸ”„ å·²å¤„ç† {idx + 1}/{len(df)} æ¡è®°è€…æ–°é—»è®°å½•")
                
        except Exception as e:
            print(f"âŒ å¤„ç†è®°è€…æ–°é—»è®°å½• {idx} æ—¶å‡ºé”™: {e}")
            continue
    
    return pd.DataFrame(all_results) if all_results else pd.DataFrame()

def calculate_summary_stats(df: pd.DataFrame, data_type: str) -> Dict:
    """è®¡ç®—ç»Ÿè®¡æ‘˜è¦"""
    
    numeric_columns = [
        'von_neumann_entropy', 'category_coherence', 'compositional_entanglement',
        'grammatical_superposition', 'semantic_interference', 'frame_competition',
        'multiple_reality_strength', 'frame_conflict_strength', 'semantic_ambiguity'
    ]
    
    summary_stats = {}
    
    # æŒ‰å­—æ®µç»Ÿè®¡
    for field in df['field'].unique():
        field_data = df[df['field'] == field]
        if not field_data.empty:
            field_stats = {}
            for col in numeric_columns:
                if col in field_data.columns:
                    field_stats[col] = {
                        'mean': float(field_data[col].mean()),
                        'std': float(field_data[col].std()),
                        'min': float(field_data[col].min()),
                        'max': float(field_data[col].max())
                    }
            summary_stats[field] = field_stats
    
    # æ•´ä½“ç»Ÿè®¡
    overall_stats = {}
    for col in numeric_columns:
        if col in df.columns:
            overall_stats[col] = {
                'mean': float(df[col].mean()),
                'std': float(df[col].std()),
                'min': float(df[col].min()),
                'max': float(df[col].max())
            }
    summary_stats['overall'] = overall_stats
    
    return summary_stats

def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸš€ å¼€å§‹æ— é™åˆ¶é‡å­åˆ†æ")
    print("=" * 60)
    print("âš ï¸  é‡è¦ï¼šå·²ç§»é™¤æ‰€æœ‰min(1.0)é™åˆ¶ï¼Œå°†è®¡ç®—çœŸå®çš„é‡å­ç‰¹å¾æ•°å€¼")
    print("=" * 60)
    
    start_time = time.time()
    
    # åˆ†æAIæ–°é—»
    print("\nğŸ¤– ç¬¬ä¸€æ­¥ï¼šåˆ†æAIæ–°é—»")
    ai_results = analyze_ai_news()
    if ai_results is not None and not ai_results.empty:
        print(f"âœ… AIæ–°é—»åˆ†æå®Œæˆ: {len(ai_results)} æ¡è®°å½•")
        
        # ä¿å­˜AIæ–°é—»ç»“æœ
        ai_file = '../results/unrestricted_ai_analysis_results.csv'
        ai_results.to_csv(ai_file, index=False, encoding='utf-8')
        print(f"ğŸ’¾ AIæ–°é—»ç»“æœå·²ä¿å­˜: {ai_file}")
        
        # è®¡ç®—AIæ–°é—»ç»Ÿè®¡
        ai_stats = calculate_summary_stats(ai_results, 'ai')
        ai_stats_file = '../results/unrestricted_ai_analysis_summary.json'
        with open(ai_stats_file, 'w', encoding='utf-8') as f:
            json.dump(ai_stats, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“Š AIæ–°é—»ç»Ÿè®¡å·²ä¿å­˜: {ai_stats_file}")
    else:
        print("âŒ AIæ–°é—»åˆ†æå¤±è´¥")
        return
    
    # åˆ†æè®°è€…æ–°é—»
    print("\nğŸ‘¨â€ğŸ’¼ ç¬¬äºŒæ­¥ï¼šåˆ†æè®°è€…æ–°é—»")
    journalist_results = analyze_journalist_news()
    if journalist_results is not None and not journalist_results.empty:
        print(f"âœ… è®°è€…æ–°é—»åˆ†æå®Œæˆ: {len(journalist_results)} æ¡è®°å½•")
        
        # ä¿å­˜è®°è€…æ–°é—»ç»“æœ
        journalist_file = '../results/unrestricted_journalist_analysis_results.csv'
        journalist_results.to_csv(journalist_file, index=False, encoding='utf-8')
        print(f"ğŸ’¾ è®°è€…æ–°é—»ç»“æœå·²ä¿å­˜: {journalist_file}")
        
        # è®¡ç®—è®°è€…æ–°é—»ç»Ÿè®¡
        journalist_stats = calculate_summary_stats(journalist_results, 'journalist')
        journalist_stats_file = '../results/unrestricted_journalist_analysis_summary.json'
        with open(journalist_stats_file, 'w', encoding='utf-8') as f:
            json.dump(journalist_stats, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“Š è®°è€…æ–°é—»ç»Ÿè®¡å·²ä¿å­˜: {journalist_stats_file}")
    else:
        print("âŒ è®°è€…æ–°é—»åˆ†æå¤±è´¥")
        return
    
    # æ˜¾ç¤ºå…³é”®ç»“æœå¯¹æ¯”
    print("\nğŸ” å…³é”®ç»“æœå¯¹æ¯”:")
    print("=" * 50)
    
    # è¯­æ³•å åŠ å¼ºåº¦å¯¹æ¯”ï¼ˆé‡ç‚¹ï¼ï¼‰
    ai_superposition = ai_stats['æ–°èæ¨™é¡Œ']['grammatical_superposition']['mean']
    journalist_superposition = journalist_stats['æ–°èæ¨™é¡Œ']['grammatical_superposition']['mean']
    
    print(f"ğŸ“ˆ è¯­æ³•å åŠ å¼ºåº¦ï¼ˆçœŸå®å€¼ï¼Œæ— é™åˆ¶ï¼‰:")
    print(f"   AIæ–°é—»æ ‡é¢˜:     {ai_superposition:.6f}")
    print(f"   è®°è€…æ–°é—»æ ‡é¢˜:   {journalist_superposition:.6f}")
    print(f"   å·®å¼‚å€æ•°:       {max(ai_superposition, journalist_superposition) / min(ai_superposition, journalist_superposition):.2f}Ã—")
    
    # å…¶ä»–å…³é”®æŒ‡æ ‡
    ai_interference = ai_stats['æ–°èæ¨™é¡Œ']['semantic_interference']['mean']
    journalist_interference = journalist_stats['æ–°èæ¨™é¡Œ']['semantic_interference']['mean']
    
    print(f"\nğŸ“ˆ è¯­ä¹‰å¹²æ¶‰:")
    print(f"   AIæ–°é—»æ ‡é¢˜:     {ai_interference:.6f}")
    print(f"   è®°è€…æ–°é—»æ ‡é¢˜:   {journalist_interference:.6f}")
    print(f"   å·®å¼‚å€æ•°:       {ai_interference / journalist_interference:.2f}Ã—")
    
    ai_reality = ai_stats['æ–°èæ¨™é¡Œ']['multiple_reality_strength']['mean']
    journalist_reality = journalist_stats['æ–°èæ¨™é¡Œ']['multiple_reality_strength']['mean']
    
    print(f"\nğŸ“ˆ å¤šé‡ç°å®å¼ºåº¦ï¼ˆæ— é™åˆ¶ï¼‰:")
    print(f"   AIæ–°é—»æ ‡é¢˜:     {ai_reality:.6f}")
    print(f"   è®°è€…æ–°é—»æ ‡é¢˜:   {journalist_reality:.6f}")
    print(f"   å·®å¼‚å€æ•°:       {ai_reality / journalist_reality:.2f}Ã—")
    
    # æ€§èƒ½ç»Ÿè®¡
    total_time = time.time() - start_time
    total_records = len(ai_results) + len(journalist_results)
    
    print(f"\nâœ… æ— é™åˆ¶é‡å­åˆ†æå®Œæˆ!")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")
    print(f"ğŸš€ å¤„ç†é€Ÿåº¦: {total_records/total_time:.1f} è®°å½•/ç§’")
    print(f"ğŸ“ˆ æ€»å¤„ç†è®°å½•: {total_records} æ¡")
    print(f"ğŸ¯ å…³é”®å‘ç°: è¯­æ³•å åŠ å¼ºåº¦çœŸå®å€¼è¿œé«˜äº1.0ï¼")

if __name__ == "__main__":
    main()
