#!/usr/bin/env python3
"""
æ•°æ®é›†å…ƒæ•°æ®æå–å™¨ - æå–AIæ–°é—»å’Œè®°è€…æ–°é—»æ•°æ®é›†çš„è¯¦ç»†å…ƒæ•°æ®
"""

import pandas as pd
import numpy as np
import json
import os
from datetime import datetime
from collections import Counter
import re

def analyze_ai_dataset():
    """åˆ†æAIæ–°é—»æ•°æ®é›†"""
    
    print("ğŸ“° åˆ†æAIæ–°é—»æ•°æ®é›†...")
    
    try:
        # è¯»å–æ•°æ®
        df = pd.read_excel('../data/dataseet.xlsx')
        
        # åŸºæœ¬ä¿¡æ¯
        metadata = {
            "dataset_name": "AIç”Ÿæˆæ–°é—»æ•°æ®é›†",
            "file_name": "dataseet.xlsx",
            "file_format": "Excel (.xlsx)",
            "total_records": len(df),
            "total_columns": len(df.columns),
            "column_names": list(df.columns),
            "analysis_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        
        # å­—æ®µåˆ†æ
        fields_analysis = {}
        
        for col in df.columns:
            if col in df.columns:
                # åŸºæœ¬ç»Ÿè®¡
                non_null_count = df[col].notna().sum()
                null_count = df[col].isna().sum()
                
                field_info = {
                    "non_null_count": int(non_null_count),
                    "null_count": int(null_count),
                    "null_percentage": float(null_count / len(df) * 100),
                    "data_type": str(df[col].dtype)
                }
                
                # æ–‡æœ¬é•¿åº¦åˆ†æ
                if df[col].dtype == 'object':
                    text_lengths = df[col].dropna().astype(str).str.len()
                    if len(text_lengths) > 0:
                        field_info.update({
                            "avg_length": float(text_lengths.mean()),
                            "min_length": int(text_lengths.min()),
                            "max_length": int(text_lengths.max()),
                            "median_length": float(text_lengths.median())
                        })
                        
                        # æ ·æœ¬å†…å®¹
                        samples = df[col].dropna().head(3).tolist()
                        field_info["samples"] = [str(s)[:100] + "..." if len(str(s)) > 100 else str(s) for s in samples]
                
                fields_analysis[col] = field_info
        
        metadata["fields_analysis"] = fields_analysis
        
        # å†…å®¹è´¨é‡åˆ†æ
        content_quality = {}
        
        # åˆ†ææ–°èæ¨™é¡Œ
        if 'æ–°èæ¨™é¡Œ' in df.columns:
            titles = df['æ–°èæ¨™é¡Œ'].dropna()
            content_quality["æ–°èæ¨™é¡Œ"] = {
                "avg_word_count": float(titles.astype(str).str.len().mean()),
                "unique_titles": len(titles.unique()),
                "duplicate_rate": float((len(titles) - len(titles.unique())) / len(titles) * 100)
            }
        
        # åˆ†æå½±ç‰‡å°è©±
        if 'å½±ç‰‡å°è©±' in df.columns:
            dialogues = df['å½±ç‰‡å°è©±'].dropna()
            content_quality["å½±ç‰‡å°è©±"] = {
                "avg_word_count": float(dialogues.astype(str).str.len().mean()),
                "unique_dialogues": len(dialogues.unique()),
                "duplicate_rate": float((len(dialogues) - len(dialogues.unique())) / len(dialogues) * 100)
            }
        
        # åˆ†æå½±ç‰‡æè¿°
        if 'å½±ç‰‡æè¿°' in df.columns:
            descriptions = df['å½±ç‰‡æè¿°'].dropna()
            content_quality["å½±ç‰‡æè¿°"] = {
                "avg_word_count": float(descriptions.astype(str).str.len().mean()),
                "unique_descriptions": len(descriptions.unique()),
                "duplicate_rate": float((len(descriptions) - len(descriptions.unique())) / len(descriptions) * 100)
            }
        
        metadata["content_quality"] = content_quality
        
        return metadata
        
    except Exception as e:
        print(f"âŒ AIæ•°æ®é›†åˆ†æå¤±è´¥: {e}")
        return None

def analyze_journalist_dataset():
    """åˆ†æè®°è€…æ–°é—»æ•°æ®é›†"""
    
    print("ğŸ‘¨â€ğŸ’¼ åˆ†æè®°è€…æ–°é—»æ•°æ®é›†...")
    
    try:
        # è¯»å–æ•°æ®
        df = pd.read_csv('../data/cna.csv')
        
        # åŸºæœ¬ä¿¡æ¯
        metadata = {
            "dataset_name": "å°æ¹¾ä¸­å¤®ç¤¾è®°è€…æ–°é—»æ•°æ®é›†",
            "file_name": "cna.csv",
            "file_format": "CSV (.csv)",
            "total_records": len(df),
            "total_columns": len(df.columns),
            "column_names": list(df.columns),
            "analysis_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "source": "å°æ¹¾ä¸­å¤®é€šè®¯ç¤¾ (Central News Agency Taiwan)",
            "language": "ç¹ä½“ä¸­æ–‡"
        }
        
        # å­—æ®µåˆ†æ
        fields_analysis = {}
        
        for col in df.columns:
            # åŸºæœ¬ç»Ÿè®¡
            non_null_count = df[col].notna().sum()
            null_count = df[col].isna().sum()
            
            field_info = {
                "non_null_count": int(non_null_count),
                "null_count": int(null_count),
                "null_percentage": float(null_count / len(df) * 100),
                "data_type": str(df[col].dtype)
            }
            
            # æ–‡æœ¬é•¿åº¦åˆ†æ
            if df[col].dtype == 'object':
                text_lengths = df[col].dropna().astype(str).str.len()
                if len(text_lengths) > 0:
                    field_info.update({
                        "avg_length": float(text_lengths.mean()),
                        "min_length": int(text_lengths.min()),
                        "max_length": int(text_lengths.max()),
                        "median_length": float(text_lengths.median())
                    })
                    
                    # æ ·æœ¬å†…å®¹
                    samples = df[col].dropna().head(3).tolist()
                    field_info["samples"] = [str(s)[:100] + "..." if len(str(s)) > 100 else str(s) for s in samples]
            
            fields_analysis[col] = field_info
        
        metadata["fields_analysis"] = fields_analysis
        
        # æ—¶é—´èŒƒå›´åˆ†æ
        if 'date' in df.columns:
            dates = pd.to_datetime(df['date'], errors='coerce').dropna()
            if len(dates) > 0:
                metadata["temporal_coverage"] = {
                    "earliest_date": dates.min().strftime("%Y-%m-%d"),
                    "latest_date": dates.max().strftime("%Y-%m-%d"),
                    "date_range_days": (dates.max() - dates.min()).days,
                    "unique_dates": len(dates.unique())
                }
        
        # URLåŸŸååˆ†æ
        if 'url' in df.columns:
            urls = df['url'].dropna()
            domains = [re.findall(r'https?://([^/]+)', url) for url in urls]
            domains = [domain[0] if domain else 'unknown' for domain in domains]
            domain_counts = Counter(domains)
            
            metadata["url_analysis"] = {
                "total_urls": len(urls),
                "unique_urls": len(urls.unique()),
                "domains": dict(domain_counts.most_common(10))
            }
        
        # å†…å®¹è´¨é‡åˆ†æ
        content_quality = {}
        
        # åˆ†ææ ‡é¢˜
        if 'title' in df.columns:
            titles = df['title'].dropna()
            content_quality["title"] = {
                "avg_char_count": float(titles.astype(str).str.len().mean()),
                "unique_titles": len(titles.unique()),
                "duplicate_rate": float((len(titles) - len(titles.unique())) / len(titles) * 100)
            }
        
        # åˆ†æå†…å®¹
        if 'content' in df.columns:
            contents = df['content'].dropna()
            content_quality["content"] = {
                "avg_char_count": float(contents.astype(str).str.len().mean()),
                "unique_contents": len(contents.unique()),
                "duplicate_rate": float((len(contents) - len(contents.unique())) / len(contents) * 100)
            }
        
        metadata["content_quality"] = content_quality
        
        return metadata
        
    except Exception as e:
        print(f"âŒ è®°è€…æ•°æ®é›†åˆ†æå¤±è´¥: {e}")
        return None

def generate_metadata_report(ai_metadata, journalist_metadata):
    """ç”Ÿæˆå…ƒæ•°æ®æŠ¥å‘Š"""
    
    report = f"""# æ•°æ®é›†å…ƒæ•°æ®æŠ¥å‘Š

## ğŸ“Š æ•°æ®é›†æ¦‚è§ˆ

æœ¬æŠ¥å‘Šè¯¦ç»†æè¿°äº†ç”¨äºé‡å­è‡ªç„¶è¯­è¨€å¤„ç†å¯¹æ¯”åˆ†æçš„ä¸¤ä¸ªæ•°æ®é›†çš„å…ƒæ•°æ®ä¿¡æ¯ã€‚

---

## ğŸ¤– AIç”Ÿæˆæ–°é—»æ•°æ®é›†

### åŸºæœ¬ä¿¡æ¯
- **æ•°æ®é›†åç§°**: {ai_metadata['dataset_name']}
- **æ–‡ä»¶å**: {ai_metadata['file_name']}
- **æ–‡ä»¶æ ¼å¼**: {ai_metadata['file_format']}
- **æ€»è®°å½•æ•°**: {ai_metadata['total_records']:,} æ¡
- **æ€»å­—æ®µæ•°**: {ai_metadata['total_columns']} ä¸ª
- **åˆ†ææ—¶é—´**: {ai_metadata['analysis_date']}

### æ•°æ®ç»“æ„
**å­—æ®µåˆ—è¡¨**: {', '.join(ai_metadata['column_names'])}

### å­—æ®µè¯¦ç»†åˆ†æ

"""
    
    # AIæ•°æ®é›†å­—æ®µåˆ†æ
    for field, info in ai_metadata['fields_analysis'].items():
        report += f"""#### {field}
- **æ•°æ®ç±»å‹**: {info['data_type']}
- **éç©ºè®°å½•**: {info['non_null_count']:,} æ¡ ({100-info['null_percentage']:.1f}%)
- **ç©ºå€¼è®°å½•**: {info['null_count']:,} æ¡ ({info['null_percentage']:.1f}%)
"""
        
        if 'avg_length' in info:
            report += f"""- **å¹³å‡é•¿åº¦**: {info['avg_length']:.1f} å­—ç¬¦
- **é•¿åº¦èŒƒå›´**: {info['min_length']} - {info['max_length']} å­—ç¬¦
- **ä¸­ä½æ•°é•¿åº¦**: {info['median_length']:.1f} å­—ç¬¦

**æ ·æœ¬å†…å®¹**:
"""
            for i, sample in enumerate(info['samples'], 1):
                report += f"{i}. {sample}\n"
        
        report += "\n"
    
    # AIæ•°æ®é›†å†…å®¹è´¨é‡
    report += "### å†…å®¹è´¨é‡åˆ†æ\n\n"
    for field, quality in ai_metadata['content_quality'].items():
        report += f"""#### {field}
- **å¹³å‡å­—æ•°**: {quality['avg_word_count']:.1f} å­—ç¬¦
- **å”¯ä¸€å†…å®¹æ•°**: {quality['unique_titles'] if 'unique_titles' in quality else quality.get('unique_dialogues', quality.get('unique_descriptions', 0)):,} æ¡
- **é‡å¤ç‡**: {quality['duplicate_rate']:.2f}%

"""
    
    # è®°è€…æ•°æ®é›†
    report += f"""---

## ğŸ‘¨â€ğŸ’¼ è®°è€…æ’°å†™æ–°é—»æ•°æ®é›†

### åŸºæœ¬ä¿¡æ¯
- **æ•°æ®é›†åç§°**: {journalist_metadata['dataset_name']}
- **æ–‡ä»¶å**: {journalist_metadata['file_name']}
- **æ–‡ä»¶æ ¼å¼**: {journalist_metadata['file_format']}
- **æ•°æ®æ¥æº**: {journalist_metadata['source']}
- **è¯­è¨€**: {journalist_metadata['language']}
- **æ€»è®°å½•æ•°**: {journalist_metadata['total_records']:,} æ¡
- **æ€»å­—æ®µæ•°**: {journalist_metadata['total_columns']} ä¸ª
- **åˆ†ææ—¶é—´**: {journalist_metadata['analysis_date']}

### æ•°æ®ç»“æ„
**å­—æ®µåˆ—è¡¨**: {', '.join(journalist_metadata['column_names'])}

### å­—æ®µè¯¦ç»†åˆ†æ

"""
    
    # è®°è€…æ•°æ®é›†å­—æ®µåˆ†æ
    for field, info in journalist_metadata['fields_analysis'].items():
        report += f"""#### {field}
- **æ•°æ®ç±»å‹**: {info['data_type']}
- **éç©ºè®°å½•**: {info['non_null_count']:,} æ¡ ({100-info['null_percentage']:.1f}%)
- **ç©ºå€¼è®°å½•**: {info['null_count']:,} æ¡ ({info['null_percentage']:.1f}%)
"""
        
        if 'avg_length' in info:
            report += f"""- **å¹³å‡é•¿åº¦**: {info['avg_length']:.1f} å­—ç¬¦
- **é•¿åº¦èŒƒå›´**: {info['min_length']} - {info['max_length']} å­—ç¬¦
- **ä¸­ä½æ•°é•¿åº¦**: {info['median_length']:.1f} å­—ç¬¦

**æ ·æœ¬å†…å®¹**:
"""
            for i, sample in enumerate(info['samples'], 1):
                report += f"{i}. {sample}\n"
        
        report += "\n"
    
    # æ—¶é—´è¦†ç›–èŒƒå›´
    if 'temporal_coverage' in journalist_metadata:
        temp = journalist_metadata['temporal_coverage']
        report += f"""### æ—¶é—´è¦†ç›–èŒƒå›´
- **æœ€æ—©æ—¥æœŸ**: {temp['earliest_date']}
- **æœ€æ™šæ—¥æœŸ**: {temp['latest_date']}
- **æ—¶é—´è·¨åº¦**: {temp['date_range_days']} å¤©
- **å”¯ä¸€æ—¥æœŸæ•°**: {temp['unique_dates']} ä¸ª

"""
    
    # URLåˆ†æ
    if 'url_analysis' in journalist_metadata:
        url = journalist_metadata['url_analysis']
        report += f"""### URLæ¥æºåˆ†æ
- **æ€»URLæ•°**: {url['total_urls']:,} ä¸ª
- **å”¯ä¸€URLæ•°**: {url['unique_urls']:,} ä¸ª
- **ä¸»è¦åŸŸååˆ†å¸ƒ**:
"""
        for domain, count in url['domains'].items():
            report += f"  - {domain}: {count} æ¡\n"
        
        report += "\n"
    
    # è®°è€…æ•°æ®é›†å†…å®¹è´¨é‡
    report += "### å†…å®¹è´¨é‡åˆ†æ\n\n"
    for field, quality in journalist_metadata['content_quality'].items():
        report += f"""#### {field}
- **å¹³å‡å­—ç¬¦æ•°**: {quality['avg_char_count']:.1f} å­—ç¬¦
- **å”¯ä¸€å†…å®¹æ•°**: {quality.get('unique_titles', quality.get('unique_contents', 0)):,} æ¡
- **é‡å¤ç‡**: {quality['duplicate_rate']:.2f}%

"""
    
    # å¯¹æ¯”åˆ†æ
    report += f"""---

## ğŸ“ˆ æ•°æ®é›†å¯¹æ¯”åˆ†æ

### è§„æ¨¡å¯¹æ¯”
| æŒ‡æ ‡ | AIæ–°é—»æ•°æ®é›† | è®°è€…æ–°é—»æ•°æ®é›† | æ¯”ä¾‹ |
|------|-------------|---------------|------|
| æ€»è®°å½•æ•° | {ai_metadata['total_records']:,} | {journalist_metadata['total_records']:,} | {ai_metadata['total_records']/journalist_metadata['total_records']:.1f}:1 |
| å­—æ®µæ•° | {ai_metadata['total_columns']} | {journalist_metadata['total_columns']} | {ai_metadata['total_columns']/journalist_metadata['total_columns']:.1f}:1 |

### å­—æ®µæ˜ å°„å…³ç³»
| AIæ–°é—»å­—æ®µ | è®°è€…æ–°é—»å­—æ®µ | ç”¨é€” |
|-----------|-------------|------|
| æ–°èæ¨™é¡Œ | title | æ–°é—»æ ‡é¢˜å¯¹æ¯”åˆ†æ |
| å½±ç‰‡å°è©± | content | å†…å®¹å¯¹æ¯”åˆ†æï¼ˆè§†é¢‘å¯¹è¯ vs æ–°é—»æ­£æ–‡ï¼‰ |
| å½±ç‰‡æè¿° | content | å†…å®¹å¯¹æ¯”åˆ†æï¼ˆè§†é¢‘æè¿° vs æ–°é—»æ­£æ–‡ï¼‰ |

### æ•°æ®è´¨é‡å¯¹æ¯”
- **AIæ–°é—»**: å¤šåª’ä½“æ–°é—»æ ¼å¼ï¼ŒåŒ…å«æ ‡é¢˜ã€è§†é¢‘å¯¹è¯ã€è§†é¢‘æè¿°
- **è®°è€…æ–°é—»**: ä¼ ç»Ÿæ–°é—»æ ¼å¼ï¼ŒåŒ…å«URLã€æ ‡é¢˜ã€æ—¥æœŸã€æ­£æ–‡å†…å®¹
- **è¯­è¨€ä¸€è‡´æ€§**: ä¸¤ä¸ªæ•°æ®é›†å‡ä¸ºç¹ä½“ä¸­æ–‡
- **å†…å®¹ç±»å‹**: AIæ–°é—»åå‘å¤šåª’ä½“å†…å®¹ï¼Œè®°è€…æ–°é—»ä¸ºä¼ ç»Ÿæ–‡å­—æŠ¥é“

### åˆ†æé€‚ç”¨æ€§è¯„ä¼°
âœ… **ä¼˜åŠ¿**:
- è¯­è¨€ä¸€è‡´æ€§è‰¯å¥½ï¼ˆå‡ä¸ºç¹ä½“ä¸­æ–‡ï¼‰
- éƒ½åŒ…å«æ–°é—»æ ‡é¢˜ï¼Œå¯è¿›è¡Œç›´æ¥å¯¹æ¯”
- æ•°æ®è´¨é‡è¾ƒé«˜ï¼Œé‡å¤ç‡ä½
- æ¶µç›–ä¸åŒçš„æ–°é—»ç”Ÿäº§æ–¹å¼ï¼ˆAIç”Ÿæˆ vs äººå·¥æ’°å†™ï¼‰

âš ï¸ **é™åˆ¶**:
- æ•°æ®è§„æ¨¡å·®å¼‚è¾ƒå¤§ï¼ˆAIæ–°é—» {ai_metadata['total_records']} æ¡ vs è®°è€…æ–°é—» {journalist_metadata['total_records']} æ¡ï¼‰
- å†…å®¹ç±»å‹å­˜åœ¨å·®å¼‚ï¼ˆå¤šåª’ä½“ vs ä¼ ç»Ÿæ–‡å­—ï¼‰
- æ—¶é—´è·¨åº¦å¯èƒ½ä¸åŒ

---

## ğŸ”¬ æ–¹æ³•è®ºè¯´æ˜

### æ•°æ®é¢„å¤„ç†
1. **å­—æ®µæ˜ å°„**: ç¡®ä¿å¯¹æ¯”åˆ†æçš„å…¬å¹³æ€§
2. **è´¨é‡è¿‡æ»¤**: ç§»é™¤ç©ºå€¼å’Œå¼‚å¸¸æ•°æ®
3. **æ–‡æœ¬æ ‡å‡†åŒ–**: ç»Ÿä¸€ç¼–ç å’Œæ ¼å¼

### åˆ†æèŒƒå›´
- **æ–°èæ¨™é¡Œ vs title**: æ ‡é¢˜çº§åˆ«çš„é‡å­ç‰¹å¾å¯¹æ¯”
- **å½±ç‰‡å°è©± + å½±ç‰‡æè¿° vs content**: å†…å®¹çº§åˆ«çš„é‡å­ç‰¹å¾å¯¹æ¯”

### ç»Ÿè®¡æ˜¾è‘—æ€§
- æ ·æœ¬é‡å……è¶³ï¼Œæ»¡è¶³ç»Ÿè®¡åˆ†æè¦æ±‚
- é‡‡ç”¨å¤šç»´åº¦é‡å­æŒ‡æ ‡ç¡®ä¿ç»“æœå¯é æ€§
- é€šè¿‡å—é™åˆ¶/æ— é™åˆ¶ç‰ˆæœ¬å¯¹æ¯”éªŒè¯åˆ†ææ–¹æ³•

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: {datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")}
**æ•°æ®åˆ†æå·¥å…·**: Python + pandas + numpy
**é‡å­åˆ†ææ¡†æ¶**: DisCoCaté‡å­è‡ªç„¶è¯­è¨€å¤„ç†
"""
    
    return report

def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸ” å¼€å§‹æå–æ•°æ®é›†å…ƒæ•°æ®...")
    print("=" * 60)
    
    # åˆ†æAIæ•°æ®é›†
    ai_metadata = analyze_ai_dataset()
    
    # åˆ†æè®°è€…æ•°æ®é›†
    journalist_metadata = analyze_journalist_dataset()
    
    if ai_metadata and journalist_metadata:
        # ä¿å­˜å…ƒæ•°æ®JSON
        with open('../results/ai_dataset_metadata.json', 'w', encoding='utf-8') as f:
            json.dump(ai_metadata, f, ensure_ascii=False, indent=2)
        
        with open('../results/journalist_dataset_metadata.json', 'w', encoding='utf-8') as f:
            json.dump(journalist_metadata, f, ensure_ascii=False, indent=2)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = generate_metadata_report(ai_metadata, journalist_metadata)
        
        # ä¿å­˜æŠ¥å‘Š
        with open('../analysis_reports/dataset_metadata_report.md', 'w', encoding='utf-8') as f:
            f.write(report)
        
        print("âœ… å…ƒæ•°æ®æå–å®Œæˆ!")
        print("ğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print("   â€¢ ../results/ai_dataset_metadata.json")
        print("   â€¢ ../results/journalist_dataset_metadata.json")
        print("   â€¢ ../analysis_reports/dataset_metadata_report.md")
        
        # æ˜¾ç¤ºå…³é”®ç»Ÿè®¡
        print(f"\nğŸ“Š å…³é”®ç»Ÿè®¡:")
        print(f"   â€¢ AIæ–°é—»æ•°æ®é›†: {ai_metadata['total_records']} æ¡è®°å½•")
        print(f"   â€¢ è®°è€…æ–°é—»æ•°æ®é›†: {journalist_metadata['total_records']} æ¡è®°å½•")
        print(f"   â€¢ æ•°æ®è§„æ¨¡æ¯”ä¾‹: {ai_metadata['total_records']/journalist_metadata['total_records']:.1f}:1")
        
    else:
        print("âŒ å…ƒæ•°æ®æå–å¤±è´¥")

if __name__ == "__main__":
    main()
