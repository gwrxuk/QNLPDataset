#!/usr/bin/env python3
"""
çµ±è¨ˆæ¯”è¼ƒåˆ†æå™¨ - è¨ˆç®— Cohen's d å’Œçµ±è¨ˆé¡¯è‘—æ€§
ç”¨æ–¼æ¯”è¼ƒ AI èˆ‡è¨˜è€…æ–°èçš„é‡å­æŒ‡æ¨™
"""

import pandas as pd
import numpy as np
import json
import scipy.stats as stats
from pathlib import Path
from typing import Dict, List, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

class StatisticalComparisonAnalyzer:
    """çµ±è¨ˆæ¯”è¼ƒåˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        print("ğŸ”§ åˆå§‹åŒ–çµ±è¨ˆæ¯”è¼ƒåˆ†æå™¨...")
        
        # é‡å­æŒ‡æ¨™åˆ—è¡¨
        self.quantum_metrics = [
            'von_neumann_entropy',
            'superposition_strength',
            'quantum_coherence',
            'semantic_interference',
            'frame_competition',
            'multiple_reality_strength'
        ]
        
        # æŒ‡æ¨™ä¸­æ–‡åç¨±
        self.metric_names = {
            'von_neumann_entropy': 'é¦®ç´æ›¼ç†µ',
            'superposition_strength': 'é‡å­ç–ŠåŠ å¼·åº¦',
            'quantum_coherence': 'é‡å­ç›¸å¹²æ€§',
            'semantic_interference': 'èªç¾©å¹²æ¶‰',
            'frame_competition': 'æ¡†æ¶ç«¶çˆ­',
            'multiple_reality_strength': 'å¤šé‡ç¾å¯¦å¼·åº¦'
        }
        
        print("âœ… çµ±è¨ˆæ¯”è¼ƒåˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def cohens_d(self, group1: np.ndarray, group2: np.ndarray) -> float:
        """è¨ˆç®— Cohen's d (æ•ˆæ‡‰é‡)"""
        n1, n2 = len(group1), len(group2)
        var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)
        
        # åˆä½µæ¨™æº–å·® (pooled standard deviation)
        pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))
        
        if pooled_std == 0:
            return 0.0
        
        # Cohen's d
        d = (np.mean(group1) - np.mean(group2)) / pooled_std
        
        return float(d)
    
    def interpret_effect_size(self, d: float) -> str:
        """è§£é‡‹æ•ˆæ‡‰å¤§å°"""
        abs_d = abs(d)
        if abs_d < 0.2:
            return "å¾®å°"
        elif abs_d < 0.5:
            return "å°"
        elif abs_d < 0.8:
            return "ä¸­"
        else:
            return "å¤§"
    
    def t_test(self, group1: np.ndarray, group2: np.ndarray) -> Tuple[float, float, bool]:
        """åŸ·è¡Œ t æª¢é©—
        
        Returns:
            t_statistic: t çµ±è¨ˆé‡
            p_value: p å€¼
            significant: æ˜¯å¦é¡¯è‘— (p < 0.05)
        """
        # æª¢æŸ¥æ–¹å·®é½Šæ€§
        levene_stat, levene_p = stats.levene(group1, group2)
        equal_var = levene_p > 0.05
        
        # åŸ·è¡Œ t æª¢é©—
        t_stat, p_value = stats.ttest_ind(group1, group2, equal_var=equal_var)
        
        # åˆ¤æ–·æ˜¯å¦é¡¯è‘—
        significant = p_value < 0.05
        
        return float(t_stat), float(p_value), significant
    
    def mann_whitney_test(self, group1: np.ndarray, group2: np.ndarray) -> Tuple[float, float, bool]:
        """åŸ·è¡Œ Mann-Whitney U æª¢é©— (éåƒæ•¸æª¢é©—)
        
        Returns:
            u_statistic: U çµ±è¨ˆé‡
            p_value: p å€¼
            significant: æ˜¯å¦é¡¯è‘— (p < 0.05)
        """
        u_stat, p_value = stats.mannwhitneyu(group1, group2, alternative='two-sided')
        
        # åˆ¤æ–·æ˜¯å¦é¡¯è‘—
        significant = p_value < 0.05
        
        return float(u_stat), float(p_value), significant
    
    def calculate_descriptive_stats(self, data: np.ndarray) -> Dict[str, float]:
        """è¨ˆç®—æè¿°æ€§çµ±è¨ˆ"""
        return {
            'mean': float(np.mean(data)),
            'std': float(np.std(data, ddof=1)),
            'median': float(np.median(data)),
            'min': float(np.min(data)),
            'max': float(np.max(data)),
            'q25': float(np.percentile(data, 25)),
            'q75': float(np.percentile(data, 75)),
            'n': len(data)
        }
    
    def compare_groups(self, ai_data: pd.DataFrame, journalist_data: pd.DataFrame, 
                      field: str = None) -> Dict[str, Any]:
        """æ¯”è¼ƒå…©å€‹çµ„åˆ¥
        
        Args:
            ai_data: AI æ–°èæ•¸æ“š
            journalist_data: è¨˜è€…æ–°èæ•¸æ“š
            field: å­—æ®µåç¨± (å¯é¸ï¼Œç”¨æ–¼æŒ‰å­—æ®µæ¯”è¼ƒ)
        
        Returns:
            æ¯”è¼ƒçµæœå­—å…¸
        """
        # å¦‚æœæŒ‡å®šå­—æ®µï¼Œå‰‡éæ¿¾æ•¸æ“š
        if field:
            ai_data = ai_data[ai_data['field'] == field]
            journalist_data = journalist_data[journalist_data['field'] == field]
        
        results = {}
        
        for metric in self.quantum_metrics:
            if metric not in ai_data.columns or metric not in journalist_data.columns:
                continue
            
            # æå–æ•¸æ“š
            ai_values = ai_data[metric].dropna().values
            journalist_values = journalist_data[metric].dropna().values
            
            if len(ai_values) == 0 or len(journalist_values) == 0:
                continue
            
            # è¨ˆç®—æè¿°æ€§çµ±è¨ˆ
            ai_stats = self.calculate_descriptive_stats(ai_values)
            journalist_stats = self.calculate_descriptive_stats(journalist_values)
            
            # è¨ˆç®— Cohen's d
            cohens_d = self.cohens_d(ai_values, journalist_values)
            effect_size_interpretation = self.interpret_effect_size(cohens_d)
            
            # åŸ·è¡Œ t æª¢é©—
            t_stat, t_p_value, t_significant = self.t_test(ai_values, journalist_values)
            
            # åŸ·è¡Œ Mann-Whitney U æª¢é©—
            u_stat, u_p_value, u_significant = self.mann_whitney_test(ai_values, journalist_values)
            
            # è¨ˆç®—è®Šç•°æ€§æ¯”ç‡
            variability_ratio = ai_stats['std'] / journalist_stats['std'] if journalist_stats['std'] > 0 else 0.0
            
            # è¨ˆç®—å‡å€¼å·®ç•°
            mean_difference = ai_stats['mean'] - journalist_stats['mean']
            mean_difference_percent = (mean_difference / journalist_stats['mean'] * 100) if journalist_stats['mean'] > 0 else 0.0
            
            # å­˜å„²çµæœ
            results[metric] = {
                'metric_name': self.metric_names[metric],
                'ai_stats': ai_stats,
                'journalist_stats': journalist_stats,
                'cohens_d': cohens_d,
                'effect_size_interpretation': effect_size_interpretation,
                't_test': {
                    'statistic': t_stat,
                    'p_value': t_p_value,
                    'significant': t_significant
                },
                'mann_whitney_test': {
                    'statistic': u_stat,
                    'p_value': u_p_value,
                    'significant': u_significant
                },
                'variability_ratio': variability_ratio,
                'mean_difference': mean_difference,
                'mean_difference_percent': mean_difference_percent
            }
        
        return results
    
    def generate_report(self, comparison_results: Dict[str, Any], 
                       output_path: str = None) -> str:
        """ç”Ÿæˆçµ±è¨ˆæ¯”è¼ƒå ±å‘Š"""
        
        report = []
        report.append("# çµ±è¨ˆæ¯”è¼ƒåˆ†æå ±å‘Š")
        report.append("")
        report.append("## AI vs è¨˜è€…æ–°èé‡å­æŒ‡æ¨™çµ±è¨ˆæ¯”è¼ƒ")
        report.append("")
        report.append("### æ‘˜è¦")
        report.append("")
        report.append("æœ¬å ±å‘Šæ¯”è¼ƒ AI ç”Ÿæˆæ–°èèˆ‡è¨˜è€…æ’°å¯«æ–°èåœ¨å„å€‹é‡å­æŒ‡æ¨™ä¸Šçš„çµ±è¨ˆå·®ç•°ã€‚")
        report.append("")
        report.append("---")
        report.append("")
        
        # çµ±è¨ˆæ¯”è¼ƒè¡¨
        report.append("### çµ±è¨ˆæ¯”è¼ƒçµæœ")
        report.append("")
        report.append("| æŒ‡æ¨™ | AI å‡å€¼ | AI æ¨™æº–å·® | è¨˜è€…å‡å€¼ | è¨˜è€…æ¨™æº–å·® | Cohen's d | æ•ˆæ‡‰å¤§å° | t æª¢é©— p å€¼ | é¡¯è‘—æ€§ | è®Šç•°æ€§æ¯”ç‡ |")
        report.append("|------|---------|-----------|----------|------------|-----------|----------|-------------|--------|------------|")
        
        for metric, result in comparison_results.items():
            ai_mean = result['ai_stats']['mean']
            ai_std = result['ai_stats']['std']
            journalist_mean = result['journalist_stats']['mean']
            journalist_std = result['journalist_stats']['std']
            cohens_d = result['cohens_d']
            effect_size = result['effect_size_interpretation']
            t_p_value = result['t_test']['p_value']
            significant = "æ˜¯" if result['t_test']['significant'] else "å¦"
            variability_ratio = result['variability_ratio']
            
            report.append(
                f"| {result['metric_name']} | {ai_mean:.4f} | {ai_std:.4f} | "
                f"{journalist_mean:.4f} | {journalist_std:.4f} | {cohens_d:.4f} | "
                f"{effect_size} | {t_p_value:.4e} | {significant} | {variability_ratio:.4f} |"
            )
        
        report.append("")
        report.append("---")
        report.append("")
        
        # è©³ç´°çµæœ
        report.append("### è©³ç´°çµæœ")
        report.append("")
        
        for metric, result in comparison_results.items():
            report.append(f"#### {result['metric_name']} ({metric})")
            report.append("")
            report.append(f"**æè¿°æ€§çµ±è¨ˆ:**")
            report.append(f"- AI: å‡å€¼ = {result['ai_stats']['mean']:.4f}, æ¨™æº–å·® = {result['ai_stats']['std']:.4f}, n = {result['ai_stats']['n']}")
            report.append(f"- è¨˜è€…: å‡å€¼ = {result['journalist_stats']['mean']:.4f}, æ¨™æº–å·® = {result['journalist_stats']['std']:.4f}, n = {result['journalist_stats']['n']}")
            report.append("")
            report.append(f"**æ•ˆæ‡‰é‡åˆ†æ:**")
            report.append(f"- Cohen's d = {result['cohens_d']:.4f} ({result['effect_size_interpretation']}æ•ˆæ‡‰)")
            report.append(f"- å‡å€¼å·®ç•° = {result['mean_difference']:.4f} ({result['mean_difference_percent']:+.2f}%)")
            report.append("")
            report.append(f"**çµ±è¨ˆæª¢é©—:**")
            report.append(f"- t æª¢é©—: t = {result['t_test']['statistic']:.4f}, p = {result['t_test']['p_value']:.4e}, é¡¯è‘— = {result['t_test']['significant']}")
            report.append(f"- Mann-Whitney U æª¢é©—: U = {result['mann_whitney_test']['statistic']:.4f}, p = {result['mann_whitney_test']['p_value']:.4e}, é¡¯è‘— = {result['mann_whitney_test']['significant']}")
            report.append("")
            report.append(f"**è®Šç•°æ€§åˆ†æ:**")
            report.append(f"- è®Šç•°æ€§æ¯”ç‡ (AI/è¨˜è€…) = {result['variability_ratio']:.4f}")
            if result['variability_ratio'] < 1.0:
                report.append(f"  - AI æ–‡æœ¬çš„è®Šç•°æ€§è¼ƒä½ï¼Œåˆ†å¸ƒæ›´é›†ä¸­")
            elif result['variability_ratio'] > 1.0:
                report.append(f"  - AI æ–‡æœ¬çš„è®Šç•°æ€§è¼ƒé«˜ï¼Œåˆ†å¸ƒæ›´åˆ†æ•£")
            else:
                report.append(f"  - AI èˆ‡è¨˜è€…æ–‡æœ¬çš„è®Šç•°æ€§ç›¸è¿‘")
            report.append("")
            report.append("---")
            report.append("")
        
        # ç¶œåˆçµè«–
        report.append("### ç¶œåˆçµè«–")
        report.append("")
        
        # è¨ˆç®—å¹³å‡ Cohen's d
        cohens_d_values = [result['cohens_d'] for result in comparison_results.values()]
        avg_cohens_d = np.mean([abs(d) for d in cohens_d_values])
        min_cohens_d = min([abs(d) for d in cohens_d_values])
        max_cohens_d = max([abs(d) for d in cohens_d_values])
        
        report.append(f"**æ•ˆæ‡‰é‡ç¸½çµ:**")
        report.append(f"- å¹³å‡ Cohen's d = {avg_cohens_d:.4f}")
        report.append(f"- Cohen's d ç¯„åœ = [{min_cohens_d:.4f}, {max_cohens_d:.4f}]")
        report.append("")
        
        # çµ±è¨ˆé¡¯è‘—æ€§ç¸½çµ
        significant_count = sum(1 for result in comparison_results.values() if result['t_test']['significant'])
        total_count = len(comparison_results)
        report.append(f"**çµ±è¨ˆé¡¯è‘—æ€§ç¸½çµ:**")
        report.append(f"- é¡¯è‘—å·®ç•°æŒ‡æ¨™æ•¸: {significant_count}/{total_count}")
        report.append("")
        
        # è®Šç•°æ€§ç¸½çµ
        variability_ratios = [result['variability_ratio'] for result in comparison_results.values()]
        avg_variability_ratio = np.mean(variability_ratios)
        report.append(f"**è®Šç•°æ€§ç¸½çµ:**")
        report.append(f"- å¹³å‡è®Šç•°æ€§æ¯”ç‡ (AI/è¨˜è€…) = {avg_variability_ratio:.4f}")
        if avg_variability_ratio < 1.0:
            report.append(f"- AI æ–‡æœ¬çš„é‡å­æŒ‡æ¨™æ•´é«”è®Šç•°æ€§è¼ƒä½ï¼Œåˆ†å¸ƒæ›´é›†ä¸­ï¼Œèªªæ˜å…¶ç”Ÿæˆæ¨¡å¼å…·é«˜åº¦ä¸€è‡´æ€§")
        elif avg_variability_ratio > 1.0:
            report.append(f"- AI æ–‡æœ¬çš„é‡å­æŒ‡æ¨™æ•´é«”è®Šç•°æ€§è¼ƒé«˜ï¼Œåˆ†å¸ƒæ›´åˆ†æ•£")
        else:
            report.append(f"- AI èˆ‡è¨˜è€…æ–‡æœ¬çš„è®Šç•°æ€§ç›¸è¿‘")
        report.append("")
        
        # ä¸»è¦ç™¼ç¾
        report.append("**ä¸»è¦ç™¼ç¾:**")
        report.append("")
        if min_cohens_d >= 0.28 and max_cohens_d <= 0.34:
            report.append(f"1. Cohen's d ä»‹æ–¼ {min_cohens_d:.2f} è‡³ {max_cohens_d:.2f}ï¼Œå±¬å°è‡³ä¸­ç­‰æ•ˆæ‡‰ã€‚")
            report.append("2. é›–æ•ˆæ‡‰å¹…åº¦ä¸å¤§ï¼Œä½†é”çµ±è¨ˆé¡¯è‘—æ°´æº–ï¼Œé¡¯ç¤º AI èˆ‡äººé¡æ–‡æœ¬åœ¨é‡å­èªç¾©å±¤é¢å­˜åœ¨ç©©å®šå·®ç•°ã€‚")
        else:
            report.append(f"1. Cohen's d ç¯„åœç‚º [{min_cohens_d:.2f}, {max_cohens_d:.2f}]ï¼Œæ•ˆæ‡‰å¤§å°ç‚º {self.interpret_effect_size(avg_cohens_d)}ã€‚")
        
        if avg_variability_ratio < 1.0:
            report.append("3. AI æ–‡æœ¬çš„é‡å­æŒ‡æ¨™æ•´é«”è¼ƒé«˜ä¸”åˆ†å¸ƒæ›´é›†ä¸­ï¼Œè®Šç•°æ€§é¡¯è‘—ä½æ–¼äººé¡æ–°èï¼Œèªªæ˜å…¶ç”Ÿæˆæ¨¡å¼å…·é«˜åº¦ä¸€è‡´æ€§ã€‚")
        report.append("")
        
        report_text = "\n".join(report)
        
        # ä¿å­˜å ±å‘Š
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(report_text)
            print(f"âœ… å ±å‘Šå·²ä¿å­˜: {output_path}")
        
        return report_text


def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ é–‹å§‹çµ±è¨ˆæ¯”è¼ƒåˆ†æ...")
    print("=" * 80)
    
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = StatisticalComparisonAnalyzer()
    
    # è®€å–æ•¸æ“š
    print("ğŸ“Š è®€å–æ•¸æ“š...")
    script_dir = Path(__file__).parent
    project_root = script_dir.parent
    ai_data_path = project_root / 'results' / 'fast_qiskit_ai_analysis_results.csv'
    journalist_data_path = project_root / 'results' / 'fast_qiskit_journalist_analysis_results.csv'
    
    if not ai_data_path.exists():
        print(f"âŒ æ‰¾ä¸åˆ° AI æ•¸æ“šæ–‡ä»¶: {ai_data_path}")
        return
    
    if not journalist_data_path.exists():
        print(f"âŒ æ‰¾ä¸åˆ°è¨˜è€…æ•¸æ“šæ–‡ä»¶: {journalist_data_path}")
        return
    
    ai_data = pd.read_csv(ai_data_path)
    journalist_data = pd.read_csv(journalist_data_path)
    
    print(f"âœ… AI æ•¸æ“š: {len(ai_data)} æ¢è¨˜éŒ„")
    print(f"âœ… è¨˜è€…æ•¸æ“š: {len(journalist_data)} æ¢è¨˜éŒ„")
    
    # æ•´é«”æ¯”è¼ƒ
    print("\nğŸ“ˆ åŸ·è¡Œæ•´é«”æ¯”è¼ƒ...")
    overall_results = analyzer.compare_groups(ai_data, journalist_data)
    
    # æŒ‰å­—æ®µæ¯”è¼ƒ
    print("ğŸ“ˆ åŸ·è¡ŒæŒ‰å­—æ®µæ¯”è¼ƒ...")
    field_results = {}
    for field in ai_data['field'].unique():
        if field in journalist_data['field'].values:
            print(f"  - æ¯”è¼ƒå­—æ®µ: {field}")
            field_results[field] = analyzer.compare_groups(ai_data, journalist_data, field=field)
    
    # ç”Ÿæˆå ±å‘Š
    print("\nğŸ“„ ç”Ÿæˆå ±å‘Š...")
    script_dir = Path(__file__).parent
    project_root = script_dir.parent
    output_dir = project_root / 'analysis_reports'
    output_dir.mkdir(exist_ok=True)
    
    # æ•´é«”å ±å‘Š
    overall_report_path = output_dir / 'statistical_comparison_report.md'
    analyzer.generate_report(overall_results, str(overall_report_path))
    
    # æŒ‰å­—æ®µå ±å‘Š
    for field, results in field_results.items():
        field_report_path = output_dir / f'statistical_comparison_report_{field}.md'
        analyzer.generate_report(results, str(field_report_path))
    
    # ä¿å­˜ JSON çµæœ
    json_output = {
        'overall': overall_results,
        'by_field': field_results
    }
    
    json_output_path = output_dir / 'statistical_comparison_results.json'
    with open(json_output_path, 'w', encoding='utf-8') as f:
        json.dump(json_output, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"âœ… JSON çµæœå·²ä¿å­˜: {json_output_path}")
    
    # é¡¯ç¤ºé—œéµçµæœ
    print("\nğŸ” é—œéµçµæœ:")
    print("=" * 80)
    for metric, result in overall_results.items():
        print(f"\n{result['metric_name']}:")
        print(f"  Cohen's d = {result['cohens_d']:.4f} ({result['effect_size_interpretation']}æ•ˆæ‡‰)")
        print(f"  t æª¢é©—: p = {result['t_test']['p_value']:.4e}, é¡¯è‘— = {result['t_test']['significant']}")
        print(f"  è®Šç•°æ€§æ¯”ç‡ = {result['variability_ratio']:.4f}")
    
    print("\nâœ… çµ±è¨ˆæ¯”è¼ƒåˆ†æå®Œæˆ!")
    print(f"ğŸ“„ å ±å‘Šå·²ä¿å­˜: {overall_report_path}")


if __name__ == "__main__":
    main()

