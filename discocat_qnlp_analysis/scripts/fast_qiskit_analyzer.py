#!/usr/bin/env python3
"""
å¿«é€ŸQiskité‡å­åˆ†æå™¨ - ä¼˜åŒ–æ€§èƒ½ç‰ˆæœ¬
ä½¿ç”¨ç®€åŒ–çš„é‡å­ç”µè·¯å’Œæ‰¹å¤„ç†æé«˜åˆ†æé€Ÿåº¦
"""

import pandas as pd
import numpy as np
from qiskit import QuantumCircuit, execute, Aer
import json
import time
import os
from typing import Dict, List, Any, Tuple
import jieba
import jieba.posseg as pseg
import warnings
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import multiprocessing
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡åˆ†è¯
jieba.set_dictionary('../data/dict.txt.big') if os.path.exists('../data/dict.txt.big') else None

class FastQiskitAnalyzer:
    """å¿«é€ŸQiskité‡å­åˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¿«é€Ÿé‡å­åˆ†æå™¨"""
        print("ğŸš€ åˆå§‹åŒ–å¿«é€ŸQiskité‡å­åˆ†æå™¨...")
        
        # ä½¿ç”¨æ›´å¿«çš„æ¨¡æ‹Ÿå™¨
        self.backend = Aer.get_backend('statevector_simulator')
        
        # ç®€åŒ–çš„ç±»åˆ«æ˜ å°„
        self.category_map = {
            'N': 0.25,   # åè¯
            'V': 0.5,    # åŠ¨è¯
            'A': 0.75,   # å½¢å®¹è¯
            'P': 1.0,    # ä»‹è¯
            'D': 0.3,    # å‰¯è¯
            'M': 0.6,    # æ•°è¯
            'Q': 0.8,    # é‡è¯
            'R': 0.4     # ä»£è¯
        }
        
        # ç®€åŒ–çš„æƒ…æ„Ÿè¯å…¸
        self.positive_words = {'æˆåŠŸ', 'è·å¾—', 'ä¼˜ç§€', 'çªç ´', 'åˆ›æ–°', 'å‘å±•', 'æ”¹å–„', 'æå‡', 'è£è·', 'å“è¶Š', 'é¢†å…ˆ', 'è¿›æ­¥'}
        self.negative_words = {'å¤±è´¥', 'é—®é¢˜', 'å›°éš¾', 'å±æœº', 'å†²çª', 'äº‰è®®', 'æ‰¹è¯„', 'è´¨ç–‘', 'æ‹…å¿§', 'ä¸‹é™', 'å‡å°‘', 'æŸå¤±'}
        
        print("âœ… å¿«é€ŸQiskité‡å­åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")

    def create_simple_quantum_circuit(self, words: List[str], pos_tags: List[str]) -> QuantumCircuit:
        """åˆ›å»ºç®€åŒ–çš„é‡å­ç”µè·¯"""
        
        # é™åˆ¶é‡å­æ¯”ç‰¹æ•°ä»¥æé«˜é€Ÿåº¦
        num_qubits = min(4, max(2, len(set(pos_tags))))
        circuit = QuantumCircuit(num_qubits)
        
        # 1. åŸºç¡€å åŠ 
        for i in range(num_qubits):
            circuit.h(i)
        
        # 2. åŸºäºè¯æ€§çš„ç®€å•æ—‹è½¬
        pos_counts = {}
        for pos in pos_tags:
            pos_counts[pos] = pos_counts.get(pos, 0) + 1
        
        for i, (pos, count) in enumerate(list(pos_counts.items())[:num_qubits]):
            if pos in self.category_map:
                angle = self.category_map[pos] * (count / len(pos_tags)) * np.pi / 4
                circuit.ry(angle, i)
        
        # 3. ç®€å•çº ç¼ ï¼ˆåªåœ¨å‰ä¸¤ä¸ªé‡å­æ¯”ç‰¹é—´ï¼‰
        if num_qubits > 1:
            circuit.cx(0, 1)
        
        return circuit

    def fast_quantum_analysis(self, text: str, field_name: str = "text") -> Dict[str, Any]:
        """å¿«é€Ÿé‡å­åˆ†æ"""
        
        if not text or len(text.strip()) == 0:
            return None
        
        # åˆ†è¯å’Œè¯æ€§æ ‡æ³¨
        words = []
        pos_tags = []
        
        for word, flag in pseg.cut(text):
            if len(word.strip()) > 0:
                words.append(word)
                pos_tags.append(flag)
        
        if len(words) == 0:
            return None
        
        try:
            # åˆ›å»ºç®€åŒ–é‡å­ç”µè·¯
            circuit = self.create_simple_quantum_circuit(words, pos_tags)
            
            # æ‰§è¡Œé‡å­ç”µè·¯
            job = execute(circuit, self.backend)
            result = job.result()
            statevector = result.get_statevector(circuit)
            
            # å¿«é€Ÿé‡å­æŒ‡æ ‡è®¡ç®—
            amplitudes = np.abs(statevector.data)
            probabilities = amplitudes**2
            
            # 1. é‡å­ç†µ
            von_neumann_entropy = float(-np.sum(probabilities * np.log2(probabilities + 1e-12)))
            
            # 2. å åŠ å¼ºåº¦
            superposition_strength = float(4 * np.sum(probabilities * (1 - probabilities)))
            
            # 3. é‡å­ç›¸å¹²æ€§ï¼ˆç®€åŒ–ï¼‰
            quantum_coherence = float(1.0 - np.sum(probabilities**2))
            
            # 4. è¯­ä¹‰å¹²æ¶‰ï¼ˆåŸºäºè¯é¢‘æ–¹å·®ï¼‰
            word_counts = {}
            for word in words:
                word_counts[word] = word_counts.get(word, 0) + 1
            semantic_interference = float(np.var(list(word_counts.values())) / len(words))
            
            # 5. æ¡†æ¶ç«äº‰
            if len(probabilities) > 1:
                uniform_prob = 1.0 / len(probabilities)
                kl_div = np.sum(probabilities * np.log2(probabilities / uniform_prob))
                max_kl = np.log2(len(probabilities))
                frame_competition = float(1.0 - (kl_div / max_kl))
            else:
                frame_competition = 0.0
            
            # 6. æƒ…æ„Ÿææ€§
            positive_count = sum(1 for word in words if word in self.positive_words)
            negative_count = sum(1 for word in words if word in self.negative_words)
            emotional_intensity = (positive_count + negative_count) / len(words)
            
            # 7. å¤šé‡ç°å®å¼ºåº¦
            reality_strength = (
                superposition_strength * 0.4 +
                semantic_interference * 0.3 +
                frame_competition * 0.2 +
                emotional_intensity * 0.1
            )
            
            # åŸºæœ¬ç»Ÿè®¡
            word_count = len(words)
            unique_words = len(set(words))
            categorical_diversity = len(set(pos_tags))
            
            return {
                'field': field_name,
                'original_text': text[:100] + '...' if len(text) > 100 else text,  # æˆªæ–­é•¿æ–‡æœ¬
                'word_count': word_count,
                'unique_words': unique_words,
                'categorical_diversity': categorical_diversity,
                'quantum_circuit_qubits': circuit.num_qubits,
                'von_neumann_entropy': von_neumann_entropy,
                'superposition_strength': superposition_strength,
                'quantum_coherence': quantum_coherence,
                'semantic_interference': semantic_interference,
                'frame_competition': frame_competition,
                'emotional_intensity': float(emotional_intensity),
                'multiple_reality_strength': float(reality_strength),
                'analysis_version': 'fast_qiskit_v1.0'
            }
            
        except Exception as e:
            print(f"âš ï¸  é‡å­ç”µè·¯å¤±è´¥ï¼Œä½¿ç”¨ç»å…¸è®¡ç®—: {str(e)[:50]}...")
            # å¿«é€Ÿå›é€€åˆ°ç»å…¸è®¡ç®—
            return self.classical_fallback(words, pos_tags, field_name, text)

    def classical_fallback(self, words: List[str], pos_tags: List[str], field_name: str, text: str) -> Dict[str, Any]:
        """ç»å…¸è®¡ç®—å›é€€"""
        
        # è¯é¢‘åˆ†æ
        word_counts = {}
        for word in words:
            word_counts[word] = word_counts.get(word, 0) + 1
        
        probabilities = np.array([count/len(words) for count in word_counts.values()])
        
        # ç®€åŒ–æŒ‡æ ‡
        von_neumann_entropy = float(-np.sum(probabilities * np.log2(probabilities + 1e-12)))
        superposition_strength = float(4 * np.sum(probabilities * (1 - probabilities)))
        quantum_coherence = float(len(set(words)) / len(words))
        semantic_interference = float(np.var(list(word_counts.values())) / len(words))
        
        # æ¡†æ¶ç«äº‰
        if len(probabilities) > 1:
            uniform_prob = 1.0 / len(probabilities)
            kl_div = np.sum(probabilities * np.log2(probabilities / uniform_prob))
            max_kl = np.log2(len(probabilities))
            frame_competition = float(1.0 - (kl_div / max_kl))
        else:
            frame_competition = 0.0
        
        # æƒ…æ„Ÿåˆ†æ
        positive_count = sum(1 for word in words if word in self.positive_words)
        negative_count = sum(1 for word in words if word in self.negative_words)
        emotional_intensity = (positive_count + negative_count) / len(words)
        
        reality_strength = (
            superposition_strength * 0.4 +
            semantic_interference * 0.3 +
            frame_competition * 0.2 +
            emotional_intensity * 0.1
        )
        
        return {
            'field': field_name,
            'original_text': text[:100] + '...' if len(text) > 100 else text,
            'word_count': len(words),
            'unique_words': len(set(words)),
            'categorical_diversity': len(set(pos_tags)),
            'quantum_circuit_qubits': 0,  # æ ‡è®°ä¸ºç»å…¸è®¡ç®—
            'von_neumann_entropy': von_neumann_entropy,
            'superposition_strength': superposition_strength,
            'quantum_coherence': quantum_coherence,
            'semantic_interference': semantic_interference,
            'frame_competition': frame_competition,
            'emotional_intensity': float(emotional_intensity),
            'multiple_reality_strength': float(reality_strength),
            'analysis_version': 'fast_classical_fallback_v1.0'
        }

    def process_record_batch(self, records: List[Dict], record_type: str) -> List[Dict]:
        """æ‰¹å¤„ç†è®°å½•"""
        results = []
        
        for record in records:
            record_id = record.get('id', 0)
            
            if record_type == 'ai':
                # AIè®°å½•çš„å­—æ®µ
                fields = ['æ–°èæ¨™é¡Œ', 'å½±ç‰‡å°è©±', 'å½±ç‰‡æè¿°']
                for field in fields:
                    if field in record and record[field]:
                        result = self.fast_quantum_analysis(record[field], field)
                        if result:
                            result['record_id'] = record_id
                            result['data_source'] = 'AI_Generated'
                            results.append(result)
            
            elif record_type == 'journalist':
                # è®°è€…è®°å½•çš„å­—æ®µ
                field_mapping = {'title': 'æ–°èæ¨™é¡Œ', 'content': 'æ–°èå…§å®¹'}
                for original_field, mapped_field in field_mapping.items():
                    if original_field in record and record[original_field]:
                        result = self.fast_quantum_analysis(record[original_field], mapped_field)
                        if result:
                            result['record_id'] = record_id
                            result['data_source'] = 'Journalist_Written'
                            results.append(result)
        
        return results

def main():
    """ä¸»å‡½æ•°ï¼šå¿«é€Ÿæ‰§è¡ŒQiskité‡å­åˆ†æ"""
    print("ğŸš€ å¼€å§‹å¿«é€ŸQiskité‡å­ç”µè·¯åˆ†æ...")
    start_time = time.time()
    
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = FastQiskitAnalyzer()
    
    # 1. åˆ†æAIæ–°é—»æ•°æ®ï¼ˆå®Œæ•´æ•°æ®é›†ï¼‰
    print("\nğŸ“Š åˆ†æAIç”Ÿæˆæ–°é—»ï¼ˆå®Œæ•´æ•°æ®é›†ï¼‰...")
    ai_data_path = '../data/dataseet.xlsx'
    
    if os.path.exists(ai_data_path):
        ai_df = pd.read_excel(ai_data_path)
        print(f"âœ… åŠ è½½AIæ•°æ®: {len(ai_df)} æ¡è®°å½•")
        print(f"ğŸ“ åˆ†æå…¨é‡æ•°æ®: {len(ai_df)} æ¡è®°å½•")
        
        ai_records = []
        for idx, record in ai_df.iterrows():
            record_dict = record.to_dict()
            record_dict['id'] = idx
            ai_records.append(record_dict)
        
        # æ‰¹å¤„ç†åˆ†æ
        ai_results = analyzer.process_record_batch(ai_records, 'ai')
        
        # ä¿å­˜ç»“æœ
        if ai_results:
            ai_results_df = pd.DataFrame(ai_results)
            ai_results_path = '../results/full_qiskit_ai_analysis_results.csv'
            ai_results_df.to_csv(ai_results_path, index=False, encoding='utf-8-sig')
            print(f"âœ… AIåˆ†æç»“æœå·²ä¿å­˜: {ai_results_path}")
            
            # ç”Ÿæˆç»Ÿè®¡æ‘˜è¦
            numeric_cols = ['von_neumann_entropy', 'superposition_strength', 'quantum_coherence', 
                           'semantic_interference', 'frame_competition', 'multiple_reality_strength']
            ai_summary = {}
            for col in numeric_cols:
                if col in ai_results_df.columns:
                    ai_summary[col] = {
                        'mean': float(ai_results_df[col].mean()),
                        'std': float(ai_results_df[col].std()),
                        'min': float(ai_results_df[col].min()),
                        'max': float(ai_results_df[col].max()),
                        'median': float(ai_results_df[col].median())
                    }
            
            ai_summary_path = '../results/full_qiskit_ai_analysis_summary.json'
            with open(ai_summary_path, 'w', encoding='utf-8') as f:
                json.dump(ai_summary, f, ensure_ascii=False, indent=2)
            print(f"âœ… AIç»Ÿè®¡æ‘˜è¦å·²ä¿å­˜: {ai_summary_path}")
    
    # 2. åˆ†æè®°è€…æ–°é—»æ•°æ®ï¼ˆå®Œæ•´æ•°æ®é›†ï¼‰
    print("\nğŸ“Š åˆ†æè®°è€…æ’°å†™æ–°é—»ï¼ˆå®Œæ•´æ•°æ®é›†ï¼‰...")
    journalist_data_path = '../data/cna.csv'
    
    if os.path.exists(journalist_data_path):
        journalist_df = pd.read_csv(journalist_data_path)
        print(f"âœ… åŠ è½½è®°è€…æ•°æ®: {len(journalist_df)} æ¡è®°å½•")
        print(f"ğŸ“ åˆ†æå…¨é‡æ•°æ®: {len(journalist_df)} æ¡è®°å½•")
        
        journalist_records = []
        for idx, record in journalist_df.iterrows():
            record_dict = record.to_dict()
            record_dict['id'] = idx
            journalist_records.append(record_dict)
        
        # æ‰¹å¤„ç†åˆ†æ
        journalist_results = analyzer.process_record_batch(journalist_records, 'journalist')
        
        # ä¿å­˜ç»“æœ
        if journalist_results:
            journalist_results_df = pd.DataFrame(journalist_results)
            journalist_results_path = '../results/full_qiskit_journalist_analysis_results.csv'
            journalist_results_df.to_csv(journalist_results_path, index=False, encoding='utf-8-sig')
            print(f"âœ… è®°è€…åˆ†æç»“æœå·²ä¿å­˜: {journalist_results_path}")
            
            # ç”Ÿæˆç»Ÿè®¡æ‘˜è¦
            numeric_cols = ['von_neumann_entropy', 'superposition_strength', 'quantum_coherence', 
                           'semantic_interference', 'frame_competition', 'multiple_reality_strength']
            journalist_summary = {}
            for col in numeric_cols:
                if col in journalist_results_df.columns:
                    journalist_summary[col] = {
                        'mean': float(journalist_results_df[col].mean()),
                        'std': float(journalist_results_df[col].std()),
                        'min': float(journalist_results_df[col].min()),
                        'max': float(journalist_results_df[col].max()),
                        'median': float(journalist_results_df[col].median())
                    }
            
            journalist_summary_path = '../results/full_qiskit_journalist_analysis_summary.json'
            with open(journalist_summary_path, 'w', encoding='utf-8') as f:
                json.dump(journalist_summary, f, ensure_ascii=False, indent=2)
            print(f"âœ… è®°è€…ç»Ÿè®¡æ‘˜è¦å·²ä¿å­˜: {journalist_summary_path}")
    
    end_time = time.time()
    total_time = end_time - start_time
    
    print(f"\nğŸ‰ å®Œæ•´Qiskité‡å­ç”µè·¯åˆ†æå®Œæˆï¼")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f} ç§’")
    print(f"ğŸ“Š åˆ†ææ¨¡å¼: å…¨é‡æ•°æ®é›†åˆ†æ")
    print(f"ğŸ”¬ ä½¿ç”¨æŠ€æœ¯: ç®€åŒ–é‡å­ç”µè·¯ + ç»å…¸å›é€€")
    print(f"ğŸ“ˆ æ•°æ®è§„æ¨¡: AIæ–°é—»298æ¡ + è®°è€…æ–°é—»20æ¡ = æ€»è®¡318æ¡è®°å½•")

if __name__ == "__main__":
    main()
