#!/usr/bin/env python3
"""
AI vs è¨˜è€…æ–°èåŸå§‹æ•¸å€¼æ¯”è¼ƒåˆ†æ
"""

import pandas as pd
import json
import numpy as np

def load_and_compare_raw_data():
    """è¼‰å…¥ä¸¦æ¯”è¼ƒåŸå§‹æ•¸æ“š"""
    
    print("ğŸ“Š è¼‰å…¥åŸå§‹æ•¸æ“šé€²è¡Œæ¯”è¼ƒåˆ†æ...")
    
    # è¼‰å…¥æ•¸æ“š
    with open('../results/final_discocat_analysis_summary.json', 'r', encoding='utf-8') as f:
        ai_data = json.load(f)
    
    with open('../results/cna_final_discocat_analysis_summary.json', 'r', encoding='utf-8') as f:
        journalist_data = json.load(f)
    
    # é—œéµé‡å­æŒ‡æ¨™
    metrics = [
        'grammatical_superposition',
        'frame_competition', 
        'multiple_reality_strength',
        'frame_conflict_strength',
        'semantic_interference',
        'von_neumann_entropy',
        'category_coherence',
        'compositional_entanglement'
    ]
    
    metric_names_chinese = [
        'èªæ³•ç–ŠåŠ å¼·åº¦',
        'æ¡†æ¶ç«¶çˆ­å¼·åº¦',
        'å¤šé‡ç¾å¯¦å¼·åº¦', 
        'æ¡†æ¶è¡çªå¼·åº¦',
        'èªç¾©å¹²æ¶‰',
        'é¦®ç´æ›¼ç†µ',
        'é¡åˆ¥ä¸€è‡´æ€§',
        'çµ„åˆç³¾çºå¼·åº¦'
    ]
    
    # å‰µå»ºè©³ç´°æ¯”è¼ƒè¡¨
    comparison_data = []
    
    for field_pair in [('æ–°èæ¨™é¡Œ', 'æ–°èæ¨™é¡Œ'), ('å½±ç‰‡å°è©±', 'æ–°èå…§å®¹')]:
        ai_field, journalist_field = field_pair
        
        print(f"\nğŸ“‹ {ai_field} vs {journalist_field} æ¯”è¼ƒ:")
        print("=" * 80)
        
        for i, metric in enumerate(metrics):
            ai_val = ai_data[ai_field][metric]['mean']
            ai_std = ai_data[ai_field][metric]['std']
            journalist_val = journalist_data[journalist_field][metric]['mean']
            journalist_std = journalist_data[journalist_field][metric]['std']
            
            # è¨ˆç®—å·®ç•°å€æ•¸
            if journalist_val != 0:
                ratio = ai_val / journalist_val
            else:
                ratio = float('inf') if ai_val > 0 else 0
            
            comparison_data.append({
                'æ–‡æœ¬é¡å‹': f'{ai_field} vs {journalist_field}',
                'é‡å­æŒ‡æ¨™': metric_names_chinese[i],
                'AIæ–°èå‡å€¼': f'{ai_val:.6f}',
                'AIæ–°èæ¨™æº–å·®': f'{ai_std:.6f}',
                'è¨˜è€…æ–°èå‡å€¼': f'{journalist_val:.6f}',
                'è¨˜è€…æ–°èæ¨™æº–å·®': f'{journalist_std:.6f}',
                'å·®ç•°å€æ•¸(AI/è¨˜è€…)': f'{ratio:.2f}' if ratio != float('inf') else 'âˆ',
                'çµ•å°å·®ç•°': f'{abs(ai_val - journalist_val):.6f}'
            })
            
            print(f"{metric_names_chinese[i]:>8}: AI={ai_val:>10.6f} | è¨˜è€…={journalist_val:>10.6f} | å€æ•¸={ratio:>8.2f}")
    
    # ä¿å­˜è©³ç´°æ¯”è¼ƒè¡¨
    df = pd.DataFrame(comparison_data)
    output_file = '../results/raw_data_detailed_comparison.csv'
    df.to_csv(output_file, index=False, encoding='utf-8')
    print(f"\nğŸ’¾ è©³ç´°åŸå§‹æ•¸æ“šæ¯”è¼ƒè¡¨å·²ä¿å­˜: {output_file}")
    
    return df

def analyze_significant_differences():
    """åˆ†æé¡¯è‘—å·®ç•°"""
    
    print("\nğŸ” é¡¯è‘—å·®ç•°åˆ†æ:")
    print("=" * 50)
    
    # è¼‰å…¥æ•¸æ“š
    with open('../results/final_discocat_analysis_summary.json', 'r', encoding='utf-8') as f:
        ai_data = json.load(f)
    
    with open('../results/cna_final_discocat_analysis_summary.json', 'r', encoding='utf-8') as f:
        journalist_data = json.load(f)
    
    # åˆ†ææ¨™é¡Œæ•¸æ“šçš„é—œéµå·®ç•°
    title_differences = []
    
    # 1. èªç¾©å¹²æ¶‰ - æœ€å¤§å·®ç•°
    ai_interference = ai_data['æ–°èæ¨™é¡Œ']['semantic_interference']['mean']
    journalist_interference = journalist_data['æ–°èæ¨™é¡Œ']['semantic_interference']['mean']
    interference_ratio = ai_interference / journalist_interference if journalist_interference > 0 else float('inf')
    
    title_differences.append({
        'æŒ‡æ¨™': 'èªç¾©å¹²æ¶‰',
        'AIæ–°è': ai_interference,
        'è¨˜è€…æ–°è': journalist_interference,
        'å·®ç•°å€æ•¸': interference_ratio,
        'è§£é‡‹': 'AIæ–°èèªç¾©ç›¸äº’ä½œç”¨æ›´è¤‡é›œ'
    })
    
    # 2. é¦®ç´æ›¼ç†µ - è³‡è¨Šå¯†åº¦
    ai_entropy = ai_data['æ–°èæ¨™é¡Œ']['von_neumann_entropy']['mean']
    journalist_entropy = journalist_data['æ–°èæ¨™é¡Œ']['von_neumann_entropy']['mean']
    entropy_ratio = journalist_entropy / ai_entropy
    
    title_differences.append({
        'æŒ‡æ¨™': 'é¦®ç´æ›¼ç†µ',
        'AIæ–°è': ai_entropy,
        'è¨˜è€…æ–°è': journalist_entropy,
        'å·®ç•°å€æ•¸': entropy_ratio,
        'è§£é‡‹': 'è¨˜è€…æ–°èè³‡è¨Šå¯†åº¦æ›´é«˜'
    })
    
    # 3. çµ„åˆç³¾çºå¼·åº¦
    ai_entanglement = ai_data['æ–°èæ¨™é¡Œ']['compositional_entanglement']['mean']
    journalist_entanglement = journalist_data['æ–°èæ¨™é¡Œ']['compositional_entanglement']['mean']
    entanglement_ratio = journalist_entanglement / ai_entanglement
    
    title_differences.append({
        'æŒ‡æ¨™': 'çµ„åˆç³¾çºå¼·åº¦',
        'AIæ–°è': ai_entanglement,
        'è¨˜è€…æ–°è': journalist_entanglement,
        'å·®ç•°å€æ•¸': entanglement_ratio,
        'è§£é‡‹': 'è¨˜è€…æ–°èèªæ³•æˆåˆ†é—œè¯æ›´å¼·'
    })
    
    # 4. é¡åˆ¥ä¸€è‡´æ€§
    ai_coherence = ai_data['æ–°èæ¨™é¡Œ']['category_coherence']['mean']
    journalist_coherence = journalist_data['æ–°èæ¨™é¡Œ']['category_coherence']['mean']
    coherence_ratio = journalist_coherence / ai_coherence
    
    title_differences.append({
        'æŒ‡æ¨™': 'é¡åˆ¥ä¸€è‡´æ€§',
        'AIæ–°è': ai_coherence,
        'è¨˜è€…æ–°è': journalist_coherence,
        'å·®ç•°å€æ•¸': coherence_ratio,
        'è§£é‡‹': 'è¨˜è€…æ–°èè©æ€§ä½¿ç”¨æ›´ä¸€è‡´'
    })
    
    # é¡¯ç¤ºåˆ†æçµæœ
    for diff in title_differences:
        print(f"\nğŸ“ˆ {diff['æŒ‡æ¨™']}:")
        print(f"   AIæ–°è:     {diff['AIæ–°è']:.6f}")
        print(f"   è¨˜è€…æ–°è:   {diff['è¨˜è€…æ–°è']:.6f}")
        print(f"   å·®ç•°å€æ•¸:   {diff['å·®ç•°å€æ•¸']:.2f}Ã—")
        print(f"   è§£é‡‹:       {diff['è§£é‡‹']}")
    
    return title_differences

def create_ranking_analysis():
    """å‰µå»ºæ’ååˆ†æ"""
    
    print("\nğŸ† é‡å­ç‰¹å¾µå¼·åº¦æ’ååˆ†æ:")
    print("=" * 50)
    
    # è¼‰å…¥æ•¸æ“š
    with open('../results/final_discocat_analysis_summary.json', 'r', encoding='utf-8') as f:
        ai_data = json.load(f)
    
    with open('../results/cna_final_discocat_analysis_summary.json', 'r', encoding='utf-8') as f:
        journalist_data = json.load(f)
    
    # æ¨™é¡Œæ•¸æ“šæ’å
    metrics_for_ranking = [
        ('èªæ³•ç–ŠåŠ å¼·åº¦', 'grammatical_superposition'),
        ('æ¡†æ¶ç«¶çˆ­å¼·åº¦', 'frame_competition'),
        ('å¤šé‡ç¾å¯¦å¼·åº¦', 'multiple_reality_strength'),
        ('æ¡†æ¶è¡çªå¼·åº¦', 'frame_conflict_strength'),
        ('èªç¾©å¹²æ¶‰', 'semantic_interference'),
        ('é¦®ç´æ›¼ç†µ', 'von_neumann_entropy'),
        ('é¡åˆ¥ä¸€è‡´æ€§', 'category_coherence'),
        ('çµ„åˆç³¾çºå¼·åº¦', 'compositional_entanglement')
    ]
    
    ai_ranking = []
    journalist_ranking = []
    
    for name, metric in metrics_for_ranking:
        ai_val = ai_data['æ–°èæ¨™é¡Œ'][metric]['mean']
        journalist_val = journalist_data['æ–°èæ¨™é¡Œ'][metric]['mean']
        
        ai_ranking.append((name, ai_val))
        journalist_ranking.append((name, journalist_val))
    
    # æŒ‰æ•¸å€¼æ’åº
    ai_ranking.sort(key=lambda x: x[1], reverse=True)
    journalist_ranking.sort(key=lambda x: x[1], reverse=True)
    
    print("\nğŸ¤– AIæ–°èé‡å­ç‰¹å¾µå¼·åº¦æ’å:")
    for i, (name, value) in enumerate(ai_ranking, 1):
        print(f"   {i:2d}. {name:<12}: {value:.6f}")
    
    print("\nğŸ‘¨â€ğŸ’¼ è¨˜è€…æ–°èé‡å­ç‰¹å¾µå¼·åº¦æ’å:")
    for i, (name, value) in enumerate(journalist_ranking, 1):
        print(f"   {i:2d}. {name:<12}: {value:.6f}")

def main():
    """ä¸»å‡½æ•¸"""
    
    print("ğŸš€ é–‹å§‹åŸå§‹æ•¸å€¼æ¯”è¼ƒåˆ†æ")
    print("=" * 60)
    
    # è©³ç´°æ•¸æ“šæ¯”è¼ƒ
    comparison_df = load_and_compare_raw_data()
    
    # é¡¯è‘—å·®ç•°åˆ†æ
    significant_diffs = analyze_significant_differences()
    
    # æ’ååˆ†æ
    create_ranking_analysis()
    
    print(f"\nâœ… åŸå§‹æ•¸å€¼æ¯”è¼ƒåˆ†æå®Œæˆ!")
    print(f"ğŸ“„ è©³ç´°æ¯”è¼ƒè¡¨: ../results/raw_data_detailed_comparison.csv")
    print(f"ğŸ” é—œéµç™¼ç¾:")
    print(f"   â€¢ èªç¾©å¹²æ¶‰: AIæ–°èæ˜¯è¨˜è€…æ–°èçš„ {significant_diffs[0]['å·®ç•°å€æ•¸']:.0f} å€")
    print(f"   â€¢ é¦®ç´æ›¼ç†µ: è¨˜è€…æ–°èæ˜¯AIæ–°èçš„ {significant_diffs[1]['å·®ç•°å€æ•¸']:.2f} å€")
    print(f"   â€¢ çµ„åˆç³¾çº: è¨˜è€…æ–°èæ˜¯AIæ–°èçš„ {significant_diffs[2]['å·®ç•°å€æ•¸']:.2f} å€")
    print(f"   â€¢ é¡åˆ¥ä¸€è‡´æ€§: è¨˜è€…æ–°èæ˜¯AIæ–°èçš„ {significant_diffs[3]['å·®ç•°å€æ•¸']:.2f} å€")

if __name__ == "__main__":
    main()
