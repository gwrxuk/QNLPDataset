#!/usr/bin/env python3
"""
Qiskité‡å­ç”µè·¯åˆ†æå™¨ - ä½¿ç”¨çœŸå®é‡å­ç”µè·¯è¿›è¡ŒQNLPåˆ†æ
åŸºäºDisCoCatç†è®ºçš„çœŸå®é‡å­è‡ªç„¶è¯­è¨€å¤„ç†å®ç°
"""

import pandas as pd
import numpy as np
from qiskit import QuantumCircuit, execute, Aer, ClassicalRegister
from qiskit.quantum_info import entropy, Statevector, DensityMatrix, partial_trace
from qiskit.circuit.library import RYGate, CXGate, HGate, RZGate
import json
import time
import os
from typing import Dict, List, Any, Tuple
import jieba
import jieba.posseg as pseg
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡åˆ†è¯
jieba.set_dictionary('../data/dict.txt.big') if os.path.exists('../data/dict.txt.big') else None

class QiskitQuantumAnalyzer:
    """åŸºäºQiskité‡å­ç”µè·¯çš„QNLPåˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–é‡å­åˆ†æå™¨"""
        print("ğŸ”§ åˆå§‹åŒ–Qiskité‡å­åˆ†æå™¨...")
        
        # é‡å­åç«¯
        self.backend = Aer.get_backend('statevector_simulator')
        self.density_backend = Aer.get_backend('qasm_simulator')
        
        # å¢å¼ºçš„ç±»åˆ«æ˜ å°„ä¸é‡å­å±æ€§
        self.category_map = {
            'N': {'qubit': 0, 'angle': np.pi/8, 'weight': 1.0, 'phase': 0.0},      # åè¯
            'V': {'qubit': 1, 'angle': np.pi/4, 'weight': 1.2, 'phase': np.pi/6}, # åŠ¨è¯
            'A': {'qubit': 2, 'angle': np.pi/6, 'weight': 0.8, 'phase': np.pi/4}, # å½¢å®¹è¯
            'P': {'qubit': 3, 'angle': np.pi/3, 'weight': 0.9, 'phase': np.pi/3}, # ä»‹è¯
            'D': {'qubit': 4, 'angle': np.pi/5, 'weight': 0.7, 'phase': np.pi/8}, # å‰¯è¯
            'M': {'qubit': 5, 'angle': np.pi/7, 'weight': 0.6, 'phase': np.pi/5}, # æ•°è¯
            'Q': {'qubit': 6, 'angle': np.pi/9, 'weight': 0.5, 'phase': np.pi/7}, # é‡è¯
            'R': {'qubit': 7, 'angle': np.pi/10, 'weight': 0.4, 'phase': np.pi/9} # ä»£è¯
        }
        
        # æƒ…æ„Ÿè¯å…¸
        self.emotion_lexicon = {
            'positive': ['æˆåŠŸ', 'è·å¾—', 'ä¼˜ç§€', 'çªç ´', 'åˆ›æ–°', 'å‘å±•', 'æ”¹å–„', 'æå‡', 'è£è·', 
                        'å“è¶Š', 'é¢†å…ˆ', 'è¿›æ­¥', 'å¢é•¿', 'è·å¥–', 'è‚¯å®š', 'æ”¯æŒ', 'åˆä½œ', 'å…±èµ¢',
                        'ç¹è£', 'å…´æ—º', 'è¾‰ç…Œ', 'èƒœåˆ©', 'å–œæ‚¦', 'æ»¡æ„', 'èµæ‰¬', 'è¡¨å½°'],
            'negative': ['å¤±è´¥', 'é—®é¢˜', 'å›°éš¾', 'å±æœº', 'å†²çª', 'äº‰è®®', 'æ‰¹è¯„', 'è´¨ç–‘', 'æ‹…å¿§',
                        'ä¸‹é™', 'å‡å°‘', 'æŸå¤±', 'é£é™©', 'å¨èƒ', 'æŒ‘æˆ˜', 'é˜»ç¢', 'å»¶è¿Ÿ', 'å–æ¶ˆ',
                        'è¡°é€€', 'æ¶åŒ–', 'æ··ä¹±', 'ç¾éš¾', 'æ‚²ä¼¤', 'æ„¤æ€’', 'æŠ—è®®', 'è°´è´£']
        }
        
        print("âœ… Qiskité‡å­åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")

    def segment_and_pos_tag(self, text: str) -> Tuple[List[str], List[str]]:
        """åˆ†è¯å’Œè¯æ€§æ ‡æ³¨"""
        words = []
        pos_tags = []
        
        for word, flag in pseg.cut(text):
            if len(word.strip()) > 0:
                words.append(word)
                pos_tags.append(flag)
        
        return words, pos_tags

    def create_quantum_circuit(self, words: List[str], pos_tags: List[str], 
                             semantic_density: float = 0.0) -> QuantumCircuit:
        """åˆ›å»ºé‡å­ç”µè·¯åŸºäºè¯­è¨€åˆ†æ"""
        
        if not words or not pos_tags:
            # æœ€å°ç”µè·¯ç”¨äºç©ºè¾“å…¥
            circuit = QuantumCircuit(3)
            circuit.h(0)
            return circuit
        
        # åŸºäºç±»åˆ«å¤šæ ·æ€§ç¡®å®šé‡å­æ¯”ç‰¹æ•°
        unique_categories = list(set(pos_tags))
        num_qubits = min(8, max(3, len(unique_categories) + 2))
        
        circuit = QuantumCircuit(num_qubits)
        
        # 1. åˆå§‹åŒ–ï¼šåˆ›å»ºå åŠ æ€
        for i in range(num_qubits):
            circuit.h(i)
        
        # 2. åº”ç”¨ç±»åˆ«ç‰¹å®šçš„æ—‹è½¬é—¨
        from collections import Counter
        category_counts = Counter(pos_tags)
        
        for i, (cat, count) in enumerate(category_counts.items()):
            if cat in self.category_map and i < num_qubits - 1:
                cat_info = self.category_map[cat]
                # åŸºäºé¢‘ç‡å’Œç±»å‹çš„æ—‹è½¬è§’åº¦
                angle = cat_info['angle'] * (count / len(pos_tags)) * cat_info['weight']
                circuit.ry(angle, i)
                # æ·»åŠ ç›¸ä½é—¨
                circuit.rz(cat_info['phase'], i)
        
        # 3. åŸºäºè¯æ±‡å…³ç³»åˆ›å»ºçº ç¼ 
        word_freq = Counter(words)
        repeated_words = [word for word, count in word_freq.items() if count > 1]
        
        # ä¸ºé‡å¤è¯æ±‡åˆ›å»ºçº ç¼ 
        if len(repeated_words) > 0 and num_qubits > 2:
            for i in range(min(len(repeated_words), num_qubits - 1)):
                target = (i + 1) % num_qubits
                circuit.cx(i, target)
        
        # 4. è¯­ä¹‰å¯†åº¦è°ƒåˆ¶
        if semantic_density > 0 and num_qubits > 1:
            density_angle = semantic_density * np.pi / 4
            for i in range(num_qubits - 1):
                circuit.ry(density_angle, i)
        
        # 5. æƒ…æ„Ÿææ€§çº ç¼ 
        positive_count = sum(1 for word in words if word in self.emotion_lexicon['positive'])
        negative_count = sum(1 for word in words if word in self.emotion_lexicon['negative'])
        
        if positive_count > 0 and negative_count > 0 and num_qubits > 2:
            # æƒ…æ„Ÿå†²çªæ—¶åˆ›å»ºç‰¹æ®Šçº ç¼ 
            circuit.cx(0, num_qubits - 1)
            circuit.ry(np.pi * (positive_count - negative_count) / len(words), num_qubits - 1)
        
        return circuit

    def measure_quantum_properties(self, circuit: QuantumCircuit, 
                                 words: List[str], pos_tags: List[str]) -> Dict[str, float]:
        """æµ‹é‡é‡å­ç”µè·¯çš„å±æ€§"""
        
        try:
            # æ·»åŠ æµ‹é‡é—¨åˆ°ç”µè·¯çš„å‰¯æœ¬
            measured_circuit = circuit.copy()
            measured_circuit.add_register(ClassicalRegister(circuit.num_qubits))
            measured_circuit.measure_all()
            
            # æ‰§è¡Œé‡å­ç”µè·¯è·å–çŠ¶æ€å‘é‡
            job = execute(circuit, self.backend)
            result = job.result()
            statevector = result.get_statevector(circuit)
            
            # ç¡®ä¿çŠ¶æ€å‘é‡æ˜¯æœ‰æ•ˆçš„
            if statevector is None or len(statevector.data) == 0:
                raise ValueError("Invalid statevector obtained")
            
            # å½’ä¸€åŒ–çŠ¶æ€å‘é‡
            statevector_data = np.array(statevector.data)
            norm = np.linalg.norm(statevector_data)
            if norm > 0:
                statevector_data = statevector_data / norm
            else:
                raise ValueError("Zero norm statevector")
            
            # åˆ›å»ºå¯†åº¦çŸ©é˜µ
            density_matrix = np.outer(statevector_data, np.conj(statevector_data))
            
            # 1. å†¯çº½æ›¼ç†µ (é‡å­ä¿¡æ¯ç†µ)
            eigenvals = np.linalg.eigvals(density_matrix)
            eigenvals = eigenvals[eigenvals > 1e-12]  # è¿‡æ»¤å°ç‰¹å¾å€¼
            if len(eigenvals) > 0:
                von_neumann_entropy = float(-np.sum(eigenvals * np.log2(eigenvals + 1e-12)))
            else:
                von_neumann_entropy = 0.0
            
            # 2. é‡å­çº ç¼ åº¦ (åŸºäºçº¿æ€§ç†µ)
            num_qubits = circuit.num_qubits
            if num_qubits > 1:
                # ä½¿ç”¨çº¿æ€§ç†µä½œä¸ºçº ç¼ åº¦é‡
                linear_entropy = 1.0 - np.trace(density_matrix @ density_matrix)
                entanglement_entropy = float(linear_entropy.real)
            else:
                entanglement_entropy = 0.0
            
            # 3. é‡å­å åŠ å¼ºåº¦ (åŸºäºçŠ¶æ€å‘é‡çš„å¹…åº¦åˆ†å¸ƒ)
            amplitudes = np.abs(statevector_data)
            probabilities = amplitudes**2
            superposition_strength = float(4 * np.sum(probabilities * (1 - probabilities)))
            
            # 4. é‡å­ç›¸å¹²æ€§ (åŸºäºéå¯¹è§’å…ƒç´ )
            diagonal_elements = np.diag(density_matrix)
            off_diagonal = density_matrix - np.diag(diagonal_elements)
            coherence = float(np.sum(np.abs(off_diagonal)))
            
            # 5. è¯­ä¹‰å¹²æ¶‰å¼ºåº¦ (åŸºäºç›¸ä½ä¿¡æ¯)
            phases = np.angle(statevector_data)
            phase_variance = float(np.var(phases))
            semantic_interference = phase_variance / (2 * np.pi)
            
            # 6. æ¡†æ¶ç«äº‰å¼ºåº¦ (åŸºäºæ¦‚ç‡åˆ†å¸ƒçš„KLæ•£åº¦)
            probabilities_filtered = probabilities[probabilities > 1e-12]
            competition_entropy = float(min(1.0, von_neumann_entropy * 0.5))
            if len(probabilities_filtered) > 1:
                uniform_prob = 1.0 / len(probabilities_filtered)
                kl_divergence = np.sum(probabilities_filtered * np.log2((probabilities_filtered + 1e-12) / uniform_prob))
                max_kl = np.log2(len(probabilities_filtered))
                frame_competition_kl = float(1.0 - min(1.0, kl_divergence / max_kl))
            else:
                frame_competition_kl = 0.0
            
            # 7. ç±»åˆ«ä¸€è‡´æ€§ (åŸºäºè¯æ€§æ ‡ç­¾åˆ†å¸ƒ)
            from collections import Counter
            pos_freq = Counter(pos_tags)
            pos_probs = np.array([count/len(pos_tags) for count in pos_freq.values()])
            if len(pos_probs) > 1:
                pos_entropy = -np.sum(pos_probs * np.log2(pos_probs + 1e-12))
                max_entropy = np.log2(len(pos_probs))
                category_coherence = float(1.0 - pos_entropy / max_entropy)
            else:
                category_coherence = 1.0
            
            # 8. ç»„åˆçº ç¼ åº¦ (è¯æ€§å¤šæ ·æ€§)
            pos_diversity = len(set(pos_tags))
            compositional_entanglement = float(pos_diversity / len(words))
            
            # 9. ç±»åˆ«ä¸€è‡´æ€§å˜å¼‚
            categorical_coherence_variance = float(np.var(pos_probs))
            
            return {
                'von_neumann_entropy': von_neumann_entropy,
                'quantum_entanglement': entanglement_entropy,
                'superposition_strength': superposition_strength,
                'quantum_coherence': coherence,
                'semantic_interference': semantic_interference,
                'frame_competition': competition_entropy,
                'frame_competition_kl': frame_competition_kl,
                'category_coherence': category_coherence,
                'compositional_entanglement': compositional_entanglement,
                'categorical_coherence_variance': categorical_coherence_variance
            }
            
        except Exception as e:
            print(f"âŒ é‡å­ç”µè·¯æ‰§è¡Œé”™è¯¯: {e}")
            # ä½¿ç”¨åŸºäºç»å…¸æ¦‚ç‡çš„å›é€€è®¡ç®—
            return self._fallback_quantum_calculation(words, pos_tags)

    def _fallback_quantum_calculation(self, words: List[str], pos_tags: List[str]) -> Dict[str, float]:
        """å›é€€åˆ°ç»å…¸æ¦‚ç‡è®¡ç®—ï¼ˆå½“é‡å­ç”µè·¯å¤±è´¥æ—¶ï¼‰"""
        from collections import Counter
        
        # åŸºäºè¯é¢‘çš„æ¦‚ç‡åˆ†å¸ƒ
        word_freq = Counter(words)
        total_words = sum(word_freq.values())
        probabilities = np.array([freq/total_words for freq in word_freq.values()])
        
        # 1. ç»å…¸ç†µï¼ˆæ¨¡æ‹Ÿå†¯çº½æ›¼ç†µï¼‰
        von_neumann_entropy = float(-np.sum(probabilities * np.log2(probabilities + 1e-12)))
        
        # 2. æ¨¡æ‹Ÿé‡å­çº ç¼ ï¼ˆåŸºäºè¯æ±‡é‡å¤ï¼‰
        repeated_words = sum(1 for count in word_freq.values() if count > 1)
        quantum_entanglement = float(repeated_words / len(word_freq))
        
        # 3. å åŠ å¼ºåº¦ï¼ˆåŸºäºæ¦‚ç‡åˆ†å¸ƒï¼‰
        superposition_strength = float(4 * np.sum(probabilities * (1 - probabilities)))
        
        # 4. ç›¸å¹²æ€§ï¼ˆåŸºäºè¯æ±‡å¤šæ ·æ€§ï¼‰
        unique_ratio = len(set(words)) / len(words)
        quantum_coherence = float(unique_ratio)
        
        # 5. è¯­ä¹‰å¹²æ¶‰ï¼ˆåŸºäºé‡å¤æ¨¡å¼ï¼‰
        repetition_variance = np.var(list(word_freq.values()))
        semantic_interference = float(repetition_variance / len(words))
        
        # 6. æ¡†æ¶ç«äº‰
        competition_entropy = float(min(1.0, von_neumann_entropy * 0.5))
        if len(probabilities) > 1:
            uniform_prob = 1.0 / len(probabilities)
            kl_divergence = np.sum(probabilities * np.log2((probabilities + 1e-12) / uniform_prob))
            max_kl = np.log2(len(probabilities))
            frame_competition_kl = float(1.0 - min(1.0, kl_divergence / max_kl))
        else:
            frame_competition_kl = 0.0
        
        # 7. ç±»åˆ«ä¸€è‡´æ€§
        pos_freq = Counter(pos_tags)
        pos_probs = np.array([count/len(pos_tags) for count in pos_freq.values()])
        if len(pos_probs) > 1:
            pos_entropy = -np.sum(pos_probs * np.log2(pos_probs + 1e-12))
            max_entropy = np.log2(len(pos_probs))
            category_coherence = float(1.0 - pos_entropy / max_entropy)
        else:
            category_coherence = 1.0
        
        # 8. ç»„åˆçº ç¼ åº¦
        pos_diversity = len(set(pos_tags))
        compositional_entanglement = float(pos_diversity / len(words))
        
        # 9. ç±»åˆ«ä¸€è‡´æ€§å˜å¼‚
        categorical_coherence_variance = float(np.var(pos_probs))
        
        return {
            'von_neumann_entropy': von_neumann_entropy,
            'quantum_entanglement': quantum_entanglement,
            'superposition_strength': superposition_strength,
            'quantum_coherence': quantum_coherence,
            'semantic_interference': semantic_interference,
            'frame_competition': competition_entropy,
            'frame_competition_kl': frame_competition_kl,
            'category_coherence': category_coherence,
            'compositional_entanglement': compositional_entanglement,
            'categorical_coherence_variance': categorical_coherence_variance
        }

    def analyze_multiple_realities(self, quantum_metrics: Dict, words: List[str]) -> Dict[str, float]:
        """åˆ†æå¤šé‡ç°å®ç°è±¡"""
        
        # è®¡ç®—è¯­è¨€å¤æ‚æ€§å› å­
        word_count = len(words)
        unique_words = len(set(words))
        word_diversity = unique_words / max(word_count, 1)
        
        # æƒ…æ„Ÿå¼ºåº¦
        positive_count = sum(1 for word in words if word in self.emotion_lexicon['positive'])
        negative_count = sum(1 for word in words if word in self.emotion_lexicon['negative'])
        emotional_intensity = (positive_count + negative_count) / max(word_count, 1)
        
        # å¤šé‡ç°å®å¼ºåº¦ (åŸºäºé‡å­å åŠ å’Œçº ç¼ )
        reality_strength = (
            quantum_metrics['superposition_strength'] * 0.30 +
            quantum_metrics['quantum_entanglement'] * 0.25 +
            quantum_metrics['semantic_interference'] * 0.20 +
            quantum_metrics['frame_competition'] * 0.15 +
            word_diversity * 0.10
        )
        
        # æ¡†æ¶å†²çªå¼ºåº¦ (åŸºäºé‡å­ç›¸å¹²æ€§å’Œçº ç¼ )
        conflict_strength = (
            quantum_metrics['compositional_entanglement'] * 0.35 +
            quantum_metrics['quantum_coherence'] * 0.25 +
            quantum_metrics['categorical_coherence_variance'] * 0.20 +
            emotional_intensity * 0.15 +
            (1.0 - quantum_metrics['category_coherence']) * 0.05
        )
        
        # è¯­ä¹‰æ¨¡ç³Šåº¦ (åŸºäºç†µå’Œå¹²æ¶‰)
        ambiguity = (
            quantum_metrics['von_neumann_entropy'] * 0.40 +
            quantum_metrics['semantic_interference'] * 0.30 +
            (1.0 - quantum_metrics['category_coherence']) * 0.20 +
            word_diversity * 0.10
        )
        
        return {
            'multiple_reality_strength': float(reality_strength),
            'frame_conflict_strength': float(conflict_strength),
            'semantic_ambiguity': float(ambiguity)
        }

    def analyze_text_quantum(self, text: str, field_name: str = "text") -> Dict[str, Any]:
        """ä½¿ç”¨é‡å­ç”µè·¯åˆ†æå•ä¸ªæ–‡æœ¬"""
        
        if not text or len(text.strip()) == 0:
            return None
        
        # åˆ†è¯å’Œè¯æ€§æ ‡æ³¨
        words, pos_tags = self.segment_and_pos_tag(text)
        
        if len(words) == 0:
            return None
        
        # è®¡ç®—è¯­ä¹‰å¯†åº¦
        semantic_density = len(set(words)) / len(words) * 10.0
        
        # åˆ›å»ºé‡å­ç”µè·¯
        circuit = self.create_quantum_circuit(words, pos_tags, semantic_density)
        
        # æµ‹é‡é‡å­å±æ€§
        quantum_metrics = self.measure_quantum_properties(circuit, words, pos_tags)
        
        # åˆ†æå¤šé‡ç°å®
        reality_metrics = self.analyze_multiple_realities(quantum_metrics, words)
        
        # åŸºæœ¬ç»Ÿè®¡
        word_count = len(words)
        unique_words = len(set(words))
        categorical_diversity = len(set(pos_tags))
        compositional_complexity = categorical_diversity / max(word_count, 1) if word_count > 0 else 0
        
        return {
            'field': field_name,
            'original_text': text,
            'word_count': word_count,
            'unique_words': unique_words,
            'categorical_diversity': categorical_diversity,
            'compositional_complexity': float(compositional_complexity),
            'semantic_density': float(semantic_density),
            'quantum_circuit_qubits': circuit.num_qubits,
            **quantum_metrics,
            **reality_metrics,
            'analysis_version': 'qiskit_quantum_v1.0'
        }

    def process_ai_record(self, record: Dict) -> List[Dict]:
        """å¤„ç†AIæ–°é—»è®°å½•"""
        results = []
        record_id = record.get('id', 0)
        
        # åˆ†æä¸‰ä¸ªå­—æ®µ
        fields = ['æ–°èæ¨™é¡Œ', 'å½±ç‰‡å°è©±', 'å½±ç‰‡æè¿°']
        
        for field in fields:
            if field in record and record[field]:
                result = self.analyze_text_quantum(record[field], field)
                if result:
                    result['record_id'] = record_id
                    result['data_source'] = 'AI_Generated'
                    results.append(result)
        
        return results

    def process_journalist_record(self, record: Dict) -> List[Dict]:
        """å¤„ç†è®°è€…æ–°é—»è®°å½•"""
        results = []
        record_id = record.get('id', 0)
        
        # åˆ†æä¸¤ä¸ªå­—æ®µï¼Œä½†æ˜ å°„ä¸ºç»Ÿä¸€çš„å­—æ®µå
        field_mapping = {
            'title': 'æ–°èæ¨™é¡Œ',
            'content': 'æ–°èå…§å®¹'
        }
        
        for original_field, mapped_field in field_mapping.items():
            if original_field in record and record[original_field]:
                result = self.analyze_text_quantum(record[original_field], mapped_field)
                if result:
                    result['record_id'] = record_id
                    result['data_source'] = 'Journalist_Written'
                    results.append(result)
        
        return results

def main():
    """ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´çš„Qiskité‡å­åˆ†æ"""
    print("ğŸš€ å¼€å§‹Qiskité‡å­ç”µè·¯åˆ†æ...")
    
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = QiskitQuantumAnalyzer()
    
    # 1. åˆ†æAIæ–°é—»æ•°æ®
    print("\nğŸ“Š åˆ†æAIç”Ÿæˆæ–°é—»...")
    ai_data_path = '../data/dataseet.xlsx'
    
    if os.path.exists(ai_data_path):
        ai_df = pd.read_excel(ai_data_path)
        print(f"âœ… åŠ è½½AIæ•°æ®: {len(ai_df)} æ¡è®°å½•")
        
        ai_results = []
        for idx, record in ai_df.iterrows():
            record_dict = record.to_dict()
            record_dict['id'] = idx
            results = analyzer.process_ai_record(record_dict)
            ai_results.extend(results)
        
        # ä¿å­˜AIåˆ†æç»“æœ
        ai_results_df = pd.DataFrame(ai_results)
        ai_results_path = '../results/qiskit_ai_analysis_results.csv'
        ai_results_df.to_csv(ai_results_path, index=False, encoding='utf-8-sig')
        print(f"âœ… AIåˆ†æç»“æœå·²ä¿å­˜: {ai_results_path}")
        
        # ç”ŸæˆAIç»Ÿè®¡æ‘˜è¦
        ai_summary = {}
        for col in ['von_neumann_entropy', 'quantum_entanglement', 'superposition_strength', 
                   'quantum_coherence', 'semantic_interference', 'frame_competition',
                   'multiple_reality_strength', 'frame_conflict_strength', 'semantic_ambiguity']:
            ai_summary[col] = {
                'mean': float(ai_results_df[col].mean()),
                'std': float(ai_results_df[col].std()),
                'min': float(ai_results_df[col].min()),
                'max': float(ai_results_df[col].max()),
                'median': float(ai_results_df[col].median())
            }
        
        ai_summary_path = '../results/qiskit_ai_analysis_summary.json'
        with open(ai_summary_path, 'w', encoding='utf-8') as f:
            json.dump(ai_summary, f, ensure_ascii=False, indent=2)
        print(f"âœ… AIç»Ÿè®¡æ‘˜è¦å·²ä¿å­˜: {ai_summary_path}")
    
    # 2. åˆ†æè®°è€…æ–°é—»æ•°æ®
    print("\nğŸ“Š åˆ†æè®°è€…æ’°å†™æ–°é—»...")
    journalist_data_path = '../data/cna.csv'
    
    if os.path.exists(journalist_data_path):
        journalist_df = pd.read_csv(journalist_data_path)
        print(f"âœ… åŠ è½½è®°è€…æ•°æ®: {len(journalist_df)} æ¡è®°å½•")
        
        journalist_results = []
        for idx, record in journalist_df.iterrows():
            record_dict = record.to_dict()
            record_dict['id'] = idx
            results = analyzer.process_journalist_record(record_dict)
            journalist_results.extend(results)
        
        # ä¿å­˜è®°è€…åˆ†æç»“æœ
        journalist_results_df = pd.DataFrame(journalist_results)
        journalist_results_path = '../results/qiskit_journalist_analysis_results.csv'
        journalist_results_df.to_csv(journalist_results_path, index=False, encoding='utf-8-sig')
        print(f"âœ… è®°è€…åˆ†æç»“æœå·²ä¿å­˜: {journalist_results_path}")
        
        # ç”Ÿæˆè®°è€…ç»Ÿè®¡æ‘˜è¦
        journalist_summary = {}
        for col in ['von_neumann_entropy', 'quantum_entanglement', 'superposition_strength', 
                   'quantum_coherence', 'semantic_interference', 'frame_competition',
                   'multiple_reality_strength', 'frame_conflict_strength', 'semantic_ambiguity']:
            journalist_summary[col] = {
                'mean': float(journalist_results_df[col].mean()),
                'std': float(journalist_results_df[col].std()),
                'min': float(journalist_results_df[col].min()),
                'max': float(journalist_results_df[col].max()),
                'median': float(journalist_results_df[col].median())
            }
        
        journalist_summary_path = '../results/qiskit_journalist_analysis_summary.json'
        with open(journalist_summary_path, 'w', encoding='utf-8') as f:
            json.dump(journalist_summary, f, ensure_ascii=False, indent=2)
        print(f"âœ… è®°è€…ç»Ÿè®¡æ‘˜è¦å·²ä¿å­˜: {journalist_summary_path}")
    
    print("\nğŸ‰ Qiskité‡å­ç”µè·¯åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main()
