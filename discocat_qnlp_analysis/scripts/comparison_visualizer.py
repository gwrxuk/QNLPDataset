#!/usr/bin/env python3
"""
AI vs è¨˜è€…æ–°èé‡å­ç‰¹å¾µæ¯”è¼ƒå¯è¦–åŒ–
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import json
from matplotlib import font_manager
import warnings
warnings.filterwarnings('ignore')

# è¨­ç½®ä¸­æ–‡å­—é«”
import matplotlib
matplotlib.rcParams['font.family'] = ['Arial Unicode MS', 'Helvetica', 'DejaVu Sans']
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'Helvetica', 'SimHei', 'Microsoft YaHei', 'WenQuanYi Micro Hei']
plt.rcParams['axes.unicode_minus'] = False

# æ£€æŸ¥å¹¶è®¾ç½®å¯ç”¨çš„ä¸­æ–‡å­—ä½“
def setup_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“"""
    import matplotlib.font_manager as fm
    
    # å¸¸è§çš„ä¸­æ–‡å­—ä½“
    chinese_fonts = [
        'Arial Unicode MS',
        'PingFang SC',
        'Helvetica Neue',
        'SimHei',
        'Microsoft YaHei',
        'WenQuanYi Micro Hei',
        'Noto Sans CJK SC',
        'Source Han Sans SC'
    ]
    
    # è·å–ç³»ç»Ÿå¯ç”¨å­—ä½“
    available_fonts = [f.name for f in fm.fontManager.ttflist]
    
    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå¯ç”¨çš„ä¸­æ–‡å­—ä½“
    for font in chinese_fonts:
        if font in available_fonts:
            plt.rcParams['font.sans-serif'] = [font] + plt.rcParams['font.sans-serif']
            print(f"âœ… ä½¿ç”¨ä¸­æ–‡å­—ä½“: {font}")
            return font
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“å¹¶è­¦å‘Š
    print("âš ï¸  æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œå¯èƒ½æ— æ³•æ­£ç¡®æ˜¾ç¤ºä¸­æ–‡")
    return None

# è®¾ç½®ä¸­æ–‡å­—ä½“
setup_chinese_font()

def load_comparison_data():
    """è¼‰å…¥æ¯”è¼ƒæ•¸æ“š"""
    
    # AIæ–°èæ•¸æ“š
    with open('../results/final_discocat_analysis_summary.json', 'r', encoding='utf-8') as f:
        ai_data = json.load(f)
    
    # è¨˜è€…æ–°èæ•¸æ“š
    with open('../results/cna_final_discocat_analysis_summary.json', 'r', encoding='utf-8') as f:
        journalist_data = json.load(f)
    
    return ai_data, journalist_data

def create_comparison_charts():
    """å‰µå»ºæ¯”è¼ƒåœ–è¡¨"""
    
    print("ğŸ¨ é–‹å§‹å‰µå»ºé‡å­ç‰¹å¾µæ¯”è¼ƒåœ–è¡¨...")
    
    ai_data, journalist_data = load_comparison_data()
    
    # è¨­ç½®åœ–è¡¨æ¨£å¼
    plt.style.use('seaborn-v0_8')
    fig = plt.figure(figsize=(20, 24))
    
    # ä¸»è¦é‡å­æŒ‡æ¨™
    quantum_metrics = [
        'grammatical_superposition',
        'frame_competition', 
        'multiple_reality_strength',
        'frame_conflict_strength',
        'semantic_interference',
        'von_neumann_entropy'
    ]
    
    metric_names = [
        'èªæ³•ç–ŠåŠ å¼·åº¦',
        'æ¡†æ¶ç«¶çˆ­å¼·åº¦', 
        'å¤šé‡ç¾å¯¦å¼·åº¦',
        'æ¡†æ¶è¡çªå¼·åº¦',
        'èªç¾©å¹²æ¶‰',
        'é¦®ç´æ›¼ç†µ'
    ]
    
    # 1. é›·é”åœ–æ¯”è¼ƒ (æ–°èæ¨™é¡Œ)
    ax1 = plt.subplot(3, 2, 1, projection='polar')
    
    # æº–å‚™é›·é”åœ–æ•¸æ“š
    ai_title_values = []
    journalist_title_values = []
    
    for metric in quantum_metrics:
        ai_val = ai_data['æ–°èæ¨™é¡Œ'][metric]['mean']
        journalist_val = journalist_data['æ–°èæ¨™é¡Œ'][metric]['mean']
        
        # æ­£è¦åŒ–åˆ°0-1ç¯„åœï¼ˆé¦®ç´æ›¼ç†µéœ€è¦ç‰¹æ®Šè™•ç†ï¼‰
        if metric == 'von_neumann_entropy':
            ai_val = min(1.0, ai_val / 5.0)  # å‡è¨­æœ€å¤§å€¼ç‚º5
            journalist_val = min(1.0, journalist_val / 5.0)
        
        ai_title_values.append(ai_val)
        journalist_title_values.append(journalist_val)
    
    # é›·é”åœ–è§’åº¦
    angles = np.linspace(0, 2*np.pi, len(quantum_metrics), endpoint=False).tolist()
    ai_title_values += ai_title_values[:1]  # é–‰åˆ
    journalist_title_values += journalist_title_values[:1]
    angles += angles[:1]
    
    ax1.plot(angles, ai_title_values, 'o-', linewidth=2, label='AIæ–°è', color='#FF6B6B')
    ax1.fill(angles, ai_title_values, alpha=0.25, color='#FF6B6B')
    ax1.plot(angles, journalist_title_values, 'o-', linewidth=2, label='è¨˜è€…æ–°è', color='#4ECDC4')
    ax1.fill(angles, journalist_title_values, alpha=0.25, color='#4ECDC4')
    
    ax1.set_xticks(angles[:-1])
    ax1.set_xticklabels(metric_names, fontsize=10, fontfamily='sans-serif')
    ax1.set_ylim(0, 1)
    ax1.set_title('æ–°èæ¨™é¡Œé‡å­ç‰¹å¾µé›·é”åœ–', fontsize=14, fontweight='bold', pad=20, fontfamily='sans-serif')
    ax1.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0), prop={'family': 'sans-serif'})
    
    # 2. æŸ±ç‹€åœ–æ¯”è¼ƒ - èªç¾©å¹²æ¶‰
    ax2 = plt.subplot(3, 2, 2)
    
    categories = ['æ–°èæ¨™é¡Œ', 'å…§å®¹/å°è©±']
    ai_interference = [
        ai_data['æ–°èæ¨™é¡Œ']['semantic_interference']['mean'],
        ai_data['å½±ç‰‡å°è©±']['semantic_interference']['mean']
    ]
    journalist_interference = [
        journalist_data['æ–°èæ¨™é¡Œ']['semantic_interference']['mean'],
        journalist_data['æ–°èå…§å®¹']['semantic_interference']['mean']
    ]
    
    x = np.arange(len(categories))
    width = 0.35
    
    bars1 = ax2.bar(x - width/2, ai_interference, width, label='AIæ–°è', color='#FF6B6B', alpha=0.8)
    bars2 = ax2.bar(x + width/2, journalist_interference, width, label='è¨˜è€…æ–°è', color='#4ECDC4', alpha=0.8)
    
    ax2.set_xlabel('æ–‡æœ¬é¡å‹', fontsize=12)
    ax2.set_ylabel('èªç¾©å¹²æ¶‰å¼·åº¦', fontsize=12)
    ax2.set_title('èªç¾©å¹²æ¶‰å¼·åº¦å°æ¯”ï¼ˆå·®ç•°é”386å€ï¼‰', fontsize=14, fontweight='bold')
    ax2.set_xticks(x)
    ax2.set_xticklabels(categories)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # æ·»åŠ æ•¸å€¼æ¨™ç±¤
    for bar in bars1:
        height = bar.get_height()
        ax2.annotate(f'{height:.4f}', xy=(bar.get_x() + bar.get_width()/2, height),
                    xytext=(0, 3), textcoords="offset points", ha='center', va='bottom')
    
    for bar in bars2:
        height = bar.get_height()
        ax2.annotate(f'{height:.4f}', xy=(bar.get_x() + bar.get_width()/2, height),
                    xytext=(0, 3), textcoords="offset points", ha='center', va='bottom')
    
    # 3. æ¡†æ¶ç«¶çˆ­ vs æ¡†æ¶è¡çªæ•£é»åœ–
    ax3 = plt.subplot(3, 2, 3)
    
    # AIæ•¸æ“šé»
    ai_competition = [ai_data['æ–°èæ¨™é¡Œ']['frame_competition']['mean'], ai_data['å½±ç‰‡å°è©±']['frame_competition']['mean']]
    ai_conflict = [ai_data['æ–°èæ¨™é¡Œ']['frame_conflict_strength']['mean'], ai_data['å½±ç‰‡å°è©±']['frame_conflict_strength']['mean']]
    
    # è¨˜è€…æ•¸æ“šé»
    journalist_competition = [journalist_data['æ–°èæ¨™é¡Œ']['frame_competition']['mean'], journalist_data['æ–°èå…§å®¹']['frame_competition']['mean']]
    journalist_conflict = [journalist_data['æ–°èæ¨™é¡Œ']['frame_conflict_strength']['mean'], journalist_data['æ–°èå…§å®¹']['frame_conflict_strength']['mean']]
    
    ax3.scatter(ai_competition, ai_conflict, s=200, alpha=0.7, c='#FF6B6B', label='AIæ–°è', edgecolors='black', linewidth=1)
    ax3.scatter(journalist_competition, journalist_conflict, s=200, alpha=0.7, c='#4ECDC4', label='è¨˜è€…æ–°è', edgecolors='black', linewidth=1)
    
    ax3.set_xlabel('æ¡†æ¶ç«¶çˆ­å¼·åº¦', fontsize=12)
    ax3.set_ylabel('æ¡†æ¶è¡çªå¼·åº¦', fontsize=12)
    ax3.set_title('æ¡†æ¶ç«¶çˆ­ vs æ¡†æ¶è¡çªæ¨¡å¼', fontsize=14, fontweight='bold')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # æ·»åŠ è±¡é™æ¨™ç±¤
    ax3.axhline(y=0.25, color='gray', linestyle='--', alpha=0.5)
    ax3.axvline(x=0.95, color='gray', linestyle='--', alpha=0.5)
    ax3.text(0.85, 0.35, 'AIæ¨¡å¼:\né«˜ç«¶çˆ­ä½è¡çª', fontsize=10, ha='center', va='center', 
             bbox=dict(boxstyle='round', facecolor='#FFE5E5', alpha=0.8))
    ax3.text(0.995, 0.29, 'è¨˜è€…æ¨¡å¼:\næ¥µé«˜ç«¶çˆ­ä¸­è¡çª', fontsize=10, ha='center', va='center',
             bbox=dict(boxstyle='round', facecolor='#E5F9F6', alpha=0.8))
    
    # 4. å¤šé‡ç¾å¯¦å¼·åº¦ç®±ç·šåœ–
    ax4 = plt.subplot(3, 2, 4)
    
    # æ¨¡æ“¬æ•¸æ“šåˆ†ä½ˆï¼ˆåŸºæ–¼å‡å€¼å’Œæ¨™æº–å·®ï¼‰
    np.random.seed(42)
    ai_reality_title = np.random.normal(ai_data['æ–°èæ¨™é¡Œ']['multiple_reality_strength']['mean'], 
                                       ai_data['æ–°èæ¨™é¡Œ']['multiple_reality_strength']['std'], 100)
    journalist_reality_title = np.random.normal(journalist_data['æ–°èæ¨™é¡Œ']['multiple_reality_strength']['mean'],
                                               journalist_data['æ–°èæ¨™é¡Œ']['multiple_reality_strength']['std'], 100)
    
    box_data = [ai_reality_title, journalist_reality_title]
    box_labels = ['AIæ–°è', 'è¨˜è€…æ–°è']
    
    bp = ax4.boxplot(box_data, labels=box_labels, patch_artist=True)
    bp['boxes'][0].set_facecolor('#FF6B6B')
    bp['boxes'][0].set_alpha(0.7)
    bp['boxes'][1].set_facecolor('#4ECDC4')
    bp['boxes'][1].set_alpha(0.7)
    
    ax4.set_ylabel('å¤šé‡ç¾å¯¦å¼·åº¦', fontsize=12)
    ax4.set_title('å¤šé‡ç¾å¯¦å¼·åº¦åˆ†ä½ˆå°æ¯”', fontsize=14, fontweight='bold')
    ax4.grid(True, alpha=0.3)
    
    # 5. é¦®ç´æ›¼ç†µå°æ¯”
    ax5 = plt.subplot(3, 2, 5)
    
    ai_entropy = [ai_data['æ–°èæ¨™é¡Œ']['von_neumann_entropy']['mean'], ai_data['å½±ç‰‡å°è©±']['von_neumann_entropy']['mean']]
    journalist_entropy = [journalist_data['æ–°èæ¨™é¡Œ']['von_neumann_entropy']['mean'], journalist_data['æ–°èå…§å®¹']['von_neumann_entropy']['mean']]
    
    x = np.arange(len(categories))
    bars1 = ax5.bar(x - width/2, ai_entropy, width, label='AIæ–°è', color='#FF6B6B', alpha=0.8)
    bars2 = ax5.bar(x + width/2, journalist_entropy, width, label='è¨˜è€…æ–°è', color='#4ECDC4', alpha=0.8)
    
    ax5.set_xlabel('æ–‡æœ¬é¡å‹', fontsize=12)
    ax5.set_ylabel('é¦®ç´æ›¼ç†µ', fontsize=12)
    ax5.set_title('è³‡è¨Šå¯†åº¦å°æ¯”ï¼ˆé¦®ç´æ›¼ç†µï¼‰', fontsize=14, fontweight='bold')
    ax5.set_xticks(x)
    ax5.set_xticklabels(categories)
    ax5.legend()
    ax5.grid(True, alpha=0.3)
    
    # æ·»åŠ æ•¸å€¼æ¨™ç±¤
    for bar in bars1:
        height = bar.get_height()
        ax5.annotate(f'{height:.2f}', xy=(bar.get_x() + bar.get_width()/2, height),
                    xytext=(0, 3), textcoords="offset points", ha='center', va='bottom')
    
    for bar in bars2:
        height = bar.get_height()
        ax5.annotate(f'{height:.2f}', xy=(bar.get_x() + bar.get_width()/2, height),
                    xytext=(0, 3), textcoords="offset points", ha='center', va='bottom')
    
    # 6. ç¶œåˆé‡å­æŒ‡ç´‹å°æ¯”
    ax6 = plt.subplot(3, 2, 6)
    
    # å‰µå»ºé‡å­æŒ‡ç´‹ç†±åœ–æ•¸æ“š
    metrics_short = ['èªæ³•ç–ŠåŠ ', 'æ¡†æ¶ç«¶çˆ­', 'å¤šé‡ç¾å¯¦', 'æ¡†æ¶è¡çª', 'èªç¾©å¹²æ¶‰']
    
    ai_fingerprint = [
        ai_data['æ–°èæ¨™é¡Œ']['grammatical_superposition']['mean'],
        ai_data['æ–°èæ¨™é¡Œ']['frame_competition']['mean'],
        ai_data['æ–°èæ¨™é¡Œ']['multiple_reality_strength']['mean'],
        ai_data['æ–°èæ¨™é¡Œ']['frame_conflict_strength']['mean'],
        min(1.0, ai_data['æ–°èæ¨™é¡Œ']['semantic_interference']['mean'])  # æ­£è¦åŒ–
    ]
    
    journalist_fingerprint = [
        journalist_data['æ–°èæ¨™é¡Œ']['grammatical_superposition']['mean'],
        journalist_data['æ–°èæ¨™é¡Œ']['frame_competition']['mean'],
        journalist_data['æ–°èæ¨™é¡Œ']['multiple_reality_strength']['mean'],
        journalist_data['æ–°èæ¨™é¡Œ']['frame_conflict_strength']['mean'],
        min(1.0, journalist_data['æ–°èæ¨™é¡Œ']['semantic_interference']['mean'] * 100)  # æ”¾å¤§é¡¯ç¤º
    ]
    
    fingerprint_data = np.array([ai_fingerprint, journalist_fingerprint])
    
    im = ax6.imshow(fingerprint_data, cmap='RdYlBu_r', aspect='auto', vmin=0, vmax=1)
    ax6.set_xticks(range(len(metrics_short)))
    ax6.set_xticklabels(metrics_short, rotation=45, ha='right')
    ax6.set_yticks([0, 1])
    ax6.set_yticklabels(['AIæ–°è', 'è¨˜è€…æ–°è'])
    ax6.set_title('é‡å­ç‰¹å¾µæŒ‡ç´‹å°æ¯”', fontsize=14, fontweight='bold')
    
    # æ·»åŠ æ•¸å€¼æ¨™ç±¤
    for i in range(2):
        for j in range(len(metrics_short)):
            text = ax6.text(j, i, f'{fingerprint_data[i, j]:.3f}', 
                           ha="center", va="center", color="white" if fingerprint_data[i, j] > 0.5 else "black",
                           fontweight='bold')
    
    # æ·»åŠ é¡è‰²æ¢
    cbar = plt.colorbar(im, ax=ax6, shrink=0.8)
    cbar.set_label('ç‰¹å¾µå¼·åº¦', fontsize=10)
    
    plt.tight_layout()
    plt.suptitle('AIç”Ÿæˆæ–°è vs è¨˜è€…æ’°å¯«æ–°èï¼šé‡å­ç‰¹å¾µå…¨é¢å°æ¯”', fontsize=18, fontweight='bold', y=0.98)
    
    # ä¿å­˜åœ–è¡¨
    output_file = '../visualizations/ai_vs_journalist_quantum_comparison.png'
    plt.savefig(output_file, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"ğŸ“Š æ¯”è¼ƒåœ–è¡¨å·²ä¿å­˜: {output_file}")
    
    plt.show()

def create_summary_table():
    """å‰µå»ºæ‘˜è¦å°æ¯”è¡¨"""
    
    print("ğŸ“‹ å‰µå»ºæ‘˜è¦å°æ¯”è¡¨...")
    
    ai_data, journalist_data = load_comparison_data()
    
    # é—œéµæŒ‡æ¨™å°æ¯”
    comparison_data = {
        'é‡å­ç‰¹å¾µ': [
            'èªæ³•ç–ŠåŠ å¼·åº¦',
            'æ¡†æ¶ç«¶çˆ­å¼·åº¦', 
            'å¤šé‡ç¾å¯¦å¼·åº¦',
            'æ¡†æ¶è¡çªå¼·åº¦',
            'èªç¾©å¹²æ¶‰',
            'é¦®ç´æ›¼ç†µ',
            'é¡åˆ¥ä¸€è‡´æ€§',
            'çµ„åˆç³¾çºå¼·åº¦'
        ],
        'AIæ–°èï¼ˆæ¨™é¡Œï¼‰': [
            f"{ai_data['æ–°èæ¨™é¡Œ']['grammatical_superposition']['mean']:.4f}",
            f"{ai_data['æ–°èæ¨™é¡Œ']['frame_competition']['mean']:.4f}",
            f"{ai_data['æ–°èæ¨™é¡Œ']['multiple_reality_strength']['mean']:.4f}",
            f"{ai_data['æ–°èæ¨™é¡Œ']['frame_conflict_strength']['mean']:.4f}",
            f"{ai_data['æ–°èæ¨™é¡Œ']['semantic_interference']['mean']:.4f}",
            f"{ai_data['æ–°èæ¨™é¡Œ']['von_neumann_entropy']['mean']:.4f}",
            f"{ai_data['æ–°èæ¨™é¡Œ']['category_coherence']['mean']:.4f}",
            f"{ai_data['æ–°èæ¨™é¡Œ']['compositional_entanglement']['mean']:.4f}"
        ],
        'è¨˜è€…æ–°èï¼ˆæ¨™é¡Œï¼‰': [
            f"{journalist_data['æ–°èæ¨™é¡Œ']['grammatical_superposition']['mean']:.4f}",
            f"{journalist_data['æ–°èæ¨™é¡Œ']['frame_competition']['mean']:.4f}",
            f"{journalist_data['æ–°èæ¨™é¡Œ']['multiple_reality_strength']['mean']:.4f}",
            f"{journalist_data['æ–°èæ¨™é¡Œ']['frame_conflict_strength']['mean']:.4f}",
            f"{journalist_data['æ–°èæ¨™é¡Œ']['semantic_interference']['mean']:.4f}",
            f"{journalist_data['æ–°èæ¨™é¡Œ']['von_neumann_entropy']['mean']:.4f}",
            f"{journalist_data['æ–°èæ¨™é¡Œ']['category_coherence']['mean']:.4f}",
            f"{journalist_data['æ–°èæ¨™é¡Œ']['compositional_entanglement']['mean']:.4f}"
        ],
        'å·®ç•°å€æ•¸': [
            '1.00Ã—',
            f"{journalist_data['æ–°èæ¨™é¡Œ']['frame_competition']['mean'] / ai_data['æ–°èæ¨™é¡Œ']['frame_competition']['mean']:.2f}Ã—",
            f"{ai_data['æ–°èæ¨™é¡Œ']['multiple_reality_strength']['mean'] / journalist_data['æ–°èæ¨™é¡Œ']['multiple_reality_strength']['mean']:.2f}Ã—",
            f"{journalist_data['æ–°èæ¨™é¡Œ']['frame_conflict_strength']['mean'] / ai_data['æ–°èæ¨™é¡Œ']['frame_conflict_strength']['mean']:.2f}Ã—",
            f"{ai_data['æ–°èæ¨™é¡Œ']['semantic_interference']['mean'] / journalist_data['æ–°èæ¨™é¡Œ']['semantic_interference']['mean']:.0f}Ã—",
            f"{journalist_data['æ–°èæ¨™é¡Œ']['von_neumann_entropy']['mean'] / ai_data['æ–°èæ¨™é¡Œ']['von_neumann_entropy']['mean']:.2f}Ã—",
            f"{journalist_data['æ–°èæ¨™é¡Œ']['category_coherence']['mean'] / ai_data['æ–°èæ¨™é¡Œ']['category_coherence']['mean']:.2f}Ã—",
            f"{journalist_data['æ–°èæ¨™é¡Œ']['compositional_entanglement']['mean'] / ai_data['æ–°èæ¨™é¡Œ']['compositional_entanglement']['mean']:.2f}Ã—"
        ]
    }
    
    df = pd.DataFrame(comparison_data)
    
    # ä¿å­˜CSV
    output_file = '../results/ai_vs_journalist_comparison_table.csv'
    df.to_csv(output_file, index=False, encoding='utf-8')
    print(f"ğŸ“„ å°æ¯”è¡¨å·²ä¿å­˜: {output_file}")
    
    # é¡¯ç¤ºè¡¨æ ¼
    print("\nğŸ“Š AI vs è¨˜è€…æ–°èé‡å­ç‰¹å¾µå°æ¯”è¡¨:")
    print("=" * 80)
    print(df.to_string(index=False))
    print("=" * 80)

def main():
    """ä¸»å‡½æ•¸"""
    
    print("ğŸš€ é–‹å§‹AI vs è¨˜è€…æ–°èé‡å­ç‰¹å¾µæ¯”è¼ƒåˆ†æ")
    print("=" * 60)
    
    # å‰µå»ºå¯è¦–åŒ–
    create_comparison_charts()
    
    # å‰µå»ºå°æ¯”è¡¨
    create_summary_table()
    
    print("\nâœ… æ¯”è¼ƒåˆ†æå®Œæˆ!")
    print("ğŸ“Š åœ–è¡¨æ–‡ä»¶: ../visualizations/ai_vs_journalist_quantum_comparison.png")
    print("ğŸ“„ å°æ¯”è¡¨æ–‡ä»¶: ../results/ai_vs_journalist_comparison_table.csv")
    print("ğŸ“ å®Œæ•´å ±å‘Š: ../analysis_reports/ai_vs_journalist_quantum_comparison.md")

if __name__ == "__main__":
    main()
