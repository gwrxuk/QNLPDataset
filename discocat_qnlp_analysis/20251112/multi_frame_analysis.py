#!/usr/bin/env python3
"""
Comprehensive frame analysis for CNA and AI datasets (2025-11-12 batch).

This script expands the earlier reform-focused framing model by incorporating a
broader catalogue of lexical frames (accountability, justice, victim support,
economic impact, public sentiment, safety, policy, communication strategy,
corporate governance, labour conditions, plus reform itself).  For each text
segment we compute frame intensities, normalised probabilities, competition
scores, and dominant-frame diagnostics.

Outputs (written to the 20251112 directory):
  - cna_multi_frame_analysis.csv
  - ai_multi_frame_analysis.csv
  - multi_frame_summary.json
"""

from __future__ import annotations

import json
import math
from collections import Counter
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, List, Tuple

import jieba
import numpy as np
import pandas as pd


# ---------------------------------------------------------------------------
# Frame lexicons
# ---------------------------------------------------------------------------

FRAME_LEXICONS: Dict[str, Tuple[str, ...]] = {
    "reform": (
        "æ”¹é©", "é©æ–°", "è®Šé©", "æ”¹å–„", "æå‡", "å„ªåŒ–", "æ”¹é€²", "èª¿æ•´", "æ•´é “", "é‡å•Ÿ", "æ›´æ–°",
    ),
    "accountability": (
        "å•è²¬", "ç©¶è²¬", "è²¬ä»»", "ç›£ç£", "æª¢è¨", "æ‡²è™•", "æ‡²æˆ’", "è¿½ç©¶", "é“æ­‰", "è² è²¬",
        "è™•åˆ†", "ç´€å¾‹", "è£ç½°", "ä¿®æ­£",
    ),
    "justice": (
        "å¸æ³•", "æ³•é™¢", "æ³•åº­", "æª¢æ–¹", "æª¢å¯Ÿå®˜", "èµ·è¨´", "åˆ¤æ±º", "è¨´è¨Ÿ", "é•æ³•", "é•è¦",
        "æ³•è¦", "åˆ‘è²¬", "åˆ‘æœŸ", "æ³•å¾‹", "è£å®š", "ç¾ˆæŠ¼", "åµè¾¦",
    ),
    "victim_support": (
        "å—å®³è€…", "è¢«å®³äºº", "å—å®³", "å—å®³äººå®¶å±¬", "æ±‚åŠ©", "æ”¯æ´", "é—œæ‡·", "é™ªä¼´", "ä¿è­·",
        "æ•‘åŠ©", "è¼”å°", "å®‰ç½®", "æ…°å•", "å”åŠ©", "ä¼¸å¼µ", "æ´åŠ©",
    ),
    "public_sentiment": (
        "æ°‘çœ¾", "è¼¿è«–", "ç¤¾æœƒ", "è²æ´", "æŠ—è­°", "éŠè¡Œ", "è«‹é¡˜", "é€£ç½²", "ç¾¤çœ¾", "ç¶²å‹",
        "æ‰¹è©•", "è²æµª", "é—œæ³¨", "åå½ˆ", "æ”¯æŒ", "å‘¼ç±²",
    ),
    "economic": (
        "ç¶“æ¿Ÿ", "æˆæœ¬", "æŠ•è³‡", "ç‡Ÿæ”¶", "åˆ©æ½¤", "å¸‚å ´", "è‚¡åƒ¹", "è²¡å‹™", "å•†æ©Ÿ", "ç”¢æ¥­",
        "æ”¶ç›Š", "æ”¯å‡º", "è³‡é‡‘", "å°±æ¥­", "è²¡æ”¿", "é ç®—", "ä¼°å€¼", "ä½µè³¼",
    ),
    "safety": (
        "å®‰å…¨", "é˜²è­·", "ä¿è­·", "é¢¨éšª", "å±æ©Ÿ", "å±å®³", "ä¿å®‰", "é é˜²", "å®ˆå‰‡", "ç›£æ§",
        "æª¢æ¸¬", "ä¿éšœ", "é€šå ±", "è­¦æˆ’", "é˜²ç¯„", "ç®¡æ§", "ç·Šæ€¥",
    ),
    "communication": (
        "è²æ˜", "å…¬å‘Š", "èªªæ˜", "æ¾„æ¸…", "è¨˜è€…æœƒ", "å›æ‡‰", "è¡¨ç¤º", "å…¬é–‹", "ç™¼è¨€", "ç™¼å¸ƒ",
        "å ±å‘Š", "å‘ŠçŸ¥", "èªªæ³•", "ç°¡å ±", "æ­éœ²",
    ),
    "policy": (
        "æ”¿ç­–", "æ³•æ¡ˆ", "åˆ¶åº¦", "è¦ç¯„", "æªæ–½", "æŒ‡å¼•", "æ–¹æ¡ˆ", "è¦å®š", "ç®¡ç†", "æ¨™æº–",
        "è‰æ¡ˆ", "è¦åŠƒ", "è¨ˆç•«", "æ–¹æ¡ˆ", "ç”³è«‹", "æµç¨‹",
    ),
    "corporate_governance": (
        "è‘£äº‹é•·", "ç¸½ç¶“ç†", "é«˜å±¤", "ç®¡ç†å±¤", "ä¼æ¥­æ–‡åŒ–", "å…¬å¸", "å“ç‰Œ", "ç¸½éƒ¨", "ä¸»ç®¡",
        "è‘£äº‹æœƒ", "ç¶“ç‡Ÿ", "ç‡Ÿé‹", "äººè³‡", "äººäº‹", "æ”¿ç­–æœƒ", "å…§éƒ¨", "éƒ¨é–€", "åœ˜éšŠ",
    ),
    "labour": (
        "å“¡å·¥", "åŒäº‹", "è·å ´", "å‹å·¥", "å·¥ä½œ", "äººåŠ›", "åŸ¹è¨“", "ç¦åˆ©", "è·å“¡", "è·å‹™",
        "é›‡ä¸»", "å—åƒ±è€…", "è·å·¥", "ç­è¡¨", "è¼ªç­", "è·æ¶¯",
    ),
}

FRAME_ORDER: Tuple[str, ...] = tuple(FRAME_LEXICONS.keys())


# ---------------------------------------------------------------------------
# Helper dataclasses & functions
# ---------------------------------------------------------------------------

@dataclass
class FrameMetrics:
    frame_counts: Dict[str, int]
    frame_probs: Dict[str, float]
    competition_entropy: float
    competition_kl: float
    normalised_entropy: float
    von_neumann_entropy: float
    active_frames: int
    dominant_frame: str
    dominant_probability: float


def tokenize(text: str) -> List[str]:
    if not isinstance(text, str):
        return []
    return [token.strip() for token in jieba.lcut(text) if token.strip()]


def score_frames(tokens: Iterable[str]) -> Dict[str, int]:
    counts = {frame: 0 for frame in FRAME_ORDER}
    for token in tokens:
        for frame, lexicon in FRAME_LEXICONS.items():
            if token in lexicon:
                counts[frame] += 1
    return counts


def compute_metrics(frame_counts: Dict[str, int]) -> FrameMetrics:
    total_hits = sum(frame_counts.values())
    if total_hits == 0:
        frame_probs = {frame: 0.0 for frame in FRAME_ORDER}
        return FrameMetrics(
            frame_counts=frame_counts,
            frame_probs=frame_probs,
            competition_entropy=0.0,
            competition_kl=0.0,
            normalised_entropy=0.0,
            von_neumann_entropy=0.0,
            active_frames=0,
            dominant_frame="none",
            dominant_probability=0.0,
        )

    frame_probs = {frame: count / total_hits for frame, count in frame_counts.items()}
    positive_probs = [prob for prob in frame_probs.values() if prob > 0]
    active_frames = len(positive_probs)

    if active_frames > 1:
        uniform_prob = 1.0 / active_frames
        kl_divergence = sum(prob * math.log2(prob / uniform_prob) for prob in positive_probs)
        max_kl = math.log2(active_frames)
        competition_kl = 1.0 - min(1.0, kl_divergence / max_kl)
    else:
        competition_kl = 0.0

    if active_frames > 1:
        von_neumann_entropy = -sum(prob * math.log2(prob) for prob in positive_probs)
        normalised_entropy = von_neumann_entropy / math.log2(active_frames)
    else:
        von_neumann_entropy = 0.0
        normalised_entropy = 0.0

    # Density matrix is diagonal with probabilities on the diagonal
    density_matrix = np.diag([frame_probs[frame] for frame in FRAME_ORDER])
    eigenvalues = np.diag(density_matrix)
    positive_eigs = [val for val in eigenvalues if val > 1e-12]
    if positive_eigs:
        entropy_bits = -sum(val * math.log2(val) for val in positive_eigs)
        competition_entropy = min(1.0, entropy_bits * 0.5)
    else:
        entropy_bits = 0.0
        competition_entropy = 0.0

    dominant_frame, dominant_probability = max(frame_probs.items(), key=lambda item: item[1])

    return FrameMetrics(
        frame_counts=frame_counts,
        frame_probs=frame_probs,
        competition_entropy=competition_entropy,
        competition_kl=competition_kl,
        normalised_entropy=normalised_entropy,
        von_neumann_entropy=von_neumann_entropy,
        active_frames=active_frames,
        dominant_frame=dominant_frame,
        dominant_probability=dominant_probability,
    )


def analyse_text_record(record_id: int, field: str, text: str, source: str) -> Dict[str, object]:
    tokens = tokenize(text)
    frame_counts = score_frames(tokens)
    metrics = compute_metrics(frame_counts)

    result: Dict[str, object] = {
        "source": source,
        "record_index": record_id,
        "field": field,
        "token_count": len(tokens),
        "frame_competition": metrics.competition_entropy,
        "frame_competition_kl": metrics.competition_kl,
        "frame_entropy": metrics.normalised_entropy,
        "von_neumann_entropy": metrics.von_neumann_entropy,
        "active_frames": metrics.active_frames,
        "dominant_frame": metrics.dominant_frame,
        "dominant_probability": metrics.dominant_probability,
        "original_text": text,
    }

    for frame in FRAME_ORDER:
        result[f"count_{frame}"] = metrics.frame_counts[frame]
        result[f"prob_{frame}"] = metrics.frame_probs[frame]

    return result


def analyse_dataframe(df: pd.DataFrame, text_columns: Iterable[str], source: str) -> pd.DataFrame:
    records: List[Dict[str, object]] = []
    for idx, row in df.iterrows():
        for column in text_columns:
            if column not in row or pd.isna(row[column]):
                continue
            text = str(row[column]).strip()
            if not text:
                continue
            record = analyse_text_record(idx, column, text, source)
            records.append(record)
    return pd.DataFrame(records)


def summarise_results(df: pd.DataFrame) -> Dict[str, Dict[str, float]]:
    summary: Dict[str, Dict[str, float]] = {}
    metric_columns = [
        "frame_competition",
        "frame_competition_kl",
        "frame_entropy",
        "von_neumann_entropy",
        "active_frames",
        "dominant_probability",
    ]
    for metric in metric_columns:
        if metric in df.columns:
            series = df[metric]
            summary[metric] = {
                "mean": float(series.mean()),
                "std": float(series.std(ddof=0)),
                "min": float(series.min()),
                "max": float(series.max()),
                "median": float(series.median()),
            }
    for frame in FRAME_ORDER:
        prob_col = f"prob_{frame}"
        if prob_col in df.columns:
            series = df[prob_col]
            summary[f"{frame}_probability"] = {
                "mean": float(series.mean()),
                "std": float(series.std(ddof=0)),
                "min": float(series.min()),
                "max": float(series.max()),
            }
    return summary


# ---------------------------------------------------------------------------
# Main execution
# ---------------------------------------------------------------------------

def main() -> None:
    base_dir = Path(__file__).resolve().parent
    cna_path = base_dir / "cna.csv"
    ai_path = base_dir / "dataseet.xlsx"

    if not cna_path.exists():
        raise FileNotFoundError(f"CNA dataset not found at {cna_path}")
    if not ai_path.exists():
        raise FileNotFoundError(f"AI dataset workbook not found at {ai_path}")

    print("ğŸ“¥ Loading datasets...")
    cna_df = pd.read_csv(cna_path)
    ai_df = pd.read_excel(ai_path)

    print("ğŸ” Analysing CNA corpus with expanded frame catalogue...")
    cna_results = analyse_dataframe(
        cna_df,
        text_columns=("title", "content"),
        source="CNA",
    )
    cna_output_path = base_dir / "cna_multi_frame_analysis.csv"
    cna_results.to_csv(cna_output_path, index=False, encoding="utf-8-sig")
    print(f"âœ… CNA frame analysis saved to {cna_output_path}")

    print("ğŸ” Analysing AI corpus with expanded frame catalogue...")
    ai_results = analyse_dataframe(
        ai_df,
        text_columns=("æ–°èæ¨™é¡Œ", "å½±ç‰‡å°è©±", "å½±ç‰‡æè¿°"),
        source="AI_Generated",
    )
    ai_output_path = base_dir / "ai_multi_frame_analysis.csv"
    ai_results.to_csv(ai_output_path, index=False, encoding="utf-8-sig")
    print(f"âœ… AI frame analysis saved to {ai_output_path}")

    summary = {
        "CNA": summarise_results(cna_results),
        "AI_Generated": summarise_results(ai_results),
        "frame_catalogue": FRAME_ORDER,
    }

    summary_path = base_dir / "multi_frame_summary.json"
    with summary_path.open("w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“„ Summary statistics written to {summary_path}")


if __name__ == "__main__":
    jieba.initialize()
    main()

