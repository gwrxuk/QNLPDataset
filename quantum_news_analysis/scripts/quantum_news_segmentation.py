#!/usr/bin/env python3
"""
é‡å­æ–°èåˆ†æ - ä¸­æ–‡æ–·è©æ¨¡çµ„
Quantum News Analysis - Chinese Word Segmentation Module

æœ¬æ¨¡çµ„ä½¿ç”¨jiebaé€²è¡ŒçœŸå¯¦çš„ä¸­æ–‡æ–·è©è™•ç†ï¼Œç‚ºå¾ŒçºŒçš„é‡å­è‡ªç„¶èªè¨€è™•ç†åˆ†æåšæº–å‚™ã€‚
This module uses jieba for real Chinese word segmentation, preparing data for quantum NLP analysis.
"""

import pandas as pd
import numpy as np
import jieba
import jieba.posseg as pseg
import re
from collections import Counter
from typing import List, Dict, Tuple
import json
import time

class QuantumNewsSegmenter:
    """é‡å­æ–°èæ–·è©å™¨ - å°ˆé–€è™•ç†æ–°èæ–‡æœ¬çš„ä¸­æ–‡æ–·è©"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ–·è©å™¨"""
        self.setup_jieba()
        self.processed_count = 0
        self.total_words = 0
        self.vocabulary = set()
        
    def setup_jieba(self):
        """è¨­ç½®jiebaæ–·è©å™¨çš„æ–°èé ˜åŸŸè©å…¸"""
        # æ·»åŠ æ–°èå¸¸ç”¨è©å½™
        news_words = [
            'äººå·¥æ™ºæ…§', 'AI', 'æ©Ÿå™¨å­¸ç¿’', 'æ·±åº¦å­¸ç¿’', 'å¤§æ•¸æ“š',
            'å€å¡Šéˆ', 'è™›æ“¬å¯¦å¢ƒ', 'æ“´å¢å¯¦å¢ƒ', 'ç‰©è¯ç¶²', '5G',
            'æ–°èæ¨™é¡Œ', 'å½±ç‰‡å°è©±', 'å½±ç‰‡æè¿°', 'åª’é«”å ±å°',
            'è­°é¡Œè¨­å®š', 'æ¡†æ¶å»ºæ§‹', 'æ•˜äº‹åˆ†æ', 'èªæ„æ¡†æ¶'
        ]
        
        for word in news_words:
            jieba.add_word(word)
        
        print("âœ… jiebaæ–°èè©å…¸å·²è¼‰å…¥")
    
    def clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬ï¼Œä¿ç•™ä¸­æ–‡ã€æ•¸å­—å’Œé‡è¦æ¨™é»"""
        if pd.isna(text):
            return ""
        
        text = str(text).strip()
        
        # ä¿ç•™ä¸­æ–‡ã€æ•¸å­—ã€é‡è¦æ¨™é»ç¬¦è™Ÿ
        text = re.sub(r'[^\u4e00-\u9fff0-9ï¼ï¼Ÿã€‚ï¼Œã€ï¼›ï¼šã€Œã€ã€ã€ï¼ˆï¼‰\[\]ã€Šã€‹\-]', ' ', text)
        
        # ç§»é™¤å¤šé¤˜ç©ºç™½
        text = re.sub(r'\s+', '', text)
        
        return text
    
    def segment_text(self, text: str) -> Dict:
        """
        å°å–®ä¸€æ–‡æœ¬é€²è¡Œæ–·è©åˆ†æ
        
        Args:
            text: å¾…åˆ†æçš„æ–‡æœ¬
            
        Returns:
            Dict: åŒ…å«æ–·è©çµæœçš„å­—å…¸
        """
        if not text or not text.strip():
            return self._empty_result()
        
        # æ¸…ç†æ–‡æœ¬
        cleaned_text = self.clean_text(text)
        if not cleaned_text:
            return self._empty_result()
        
        # ä½¿ç”¨jiebaé€²è¡Œæ–·è©å’Œè©æ€§æ¨™è¨»
        words = []
        pos_tags = []
        
        for word, flag in pseg.cut(cleaned_text):
            if len(word.strip()) > 0:
                words.append(word)
                pos_tags.append(f"{word}/{flag}")
        
        # è¨ˆç®—çµ±è¨ˆä¿¡æ¯
        word_count = len(words)
        unique_words = len(set(words))
        word_freq = Counter(words)
        
        # æ›´æ–°å…¨å±€çµ±è¨ˆ
        self.total_words += word_count
        self.vocabulary.update(words)
        
        return {
            'original_text': text,
            'cleaned_text': cleaned_text,
            'words': words,
            'segmented_text': ' / '.join(words),
            'pos_tags': ' | '.join(pos_tags),
            'word_count': word_count,
            'unique_word_count': unique_words,
            'words_list': ', '.join(words),
            'word_frequencies': dict(word_freq),
            'lexical_diversity': unique_words / word_count if word_count > 0 else 0
        }
    
    def _empty_result(self) -> Dict:
        """è¿”å›ç©ºçµæœ"""
        return {
            'original_text': '',
            'cleaned_text': '',
            'words': [],
            'segmented_text': '',
            'pos_tags': '',
            'word_count': 0,
            'unique_word_count': 0,
            'words_list': '',
            'word_frequencies': {},
            'lexical_diversity': 0
        }
    
    def analyze_field(self, df: pd.DataFrame, field_name: str) -> List[Dict]:
        """
        åˆ†æç‰¹å®šæ¬„ä½çš„æ‰€æœ‰æ–‡æœ¬
        
        Args:
            df: æ•¸æ“šæ¡†
            field_name: æ¬„ä½åç¨±
            
        Returns:
            List[Dict]: åˆ†æçµæœåˆ—è¡¨
        """
        print(f"\nğŸ“Š åˆ†æ {field_name} æ¬„ä½")
        print("=" * 50)
        
        if field_name not in df.columns:
            print(f"âŒ æ¬„ä½ {field_name} ä¸å­˜åœ¨")
            return []
        
        field_data = df[field_name].dropna()
        print(f"ğŸ“ æœ‰æ•ˆè¨˜éŒ„æ•¸: {len(field_data)}")
        
        results = []
        
        for idx, (record_idx, text) in enumerate(field_data.items()):
            if idx % 50 == 0:
                print(f"è™•ç†é€²åº¦: {idx + 1}/{len(field_data)}")
            
            # é€²è¡Œæ–·è©åˆ†æ
            seg_result = self.segment_text(str(text))
            
            # æ·»åŠ è¨˜éŒ„ä¿¡æ¯
            result = {
                'record_id': record_idx,
                'field': field_name,
                **seg_result
            }
            
            results.append(result)
            self.processed_count += 1
        
        print(f"âœ… {field_name} åˆ†æå®Œæˆ: {len(results)} ç­†è¨˜éŒ„")
        return results
    
    def generate_field_statistics(self, results: List[Dict], field_name: str) -> Dict:
        """ç”Ÿæˆæ¬„ä½çµ±è¨ˆä¿¡æ¯"""
        if not results:
            return {}
        
        word_counts = [r['word_count'] for r in results]
        unique_word_counts = [r['unique_word_count'] for r in results]
        lexical_diversities = [r['lexical_diversity'] for r in results]
        
        # æ”¶é›†æ‰€æœ‰è©å½™
        all_words = []
        for result in results:
            all_words.extend(result['words'])
        
        word_freq = Counter(all_words)
        
        stats = {
            'field_name': field_name,
            'total_records': len(results),
            'avg_word_count': np.mean(word_counts),
            'std_word_count': np.std(word_counts),
            'min_word_count': np.min(word_counts),
            'max_word_count': np.max(word_counts),
            'avg_unique_words': np.mean(unique_word_counts),
            'avg_lexical_diversity': np.mean(lexical_diversities),
            'total_vocabulary_size': len(set(all_words)),
            'total_word_tokens': len(all_words),
            'top_10_words': word_freq.most_common(10)
        }
        
        return stats
    
    def get_global_statistics(self) -> Dict:
        """ç²å–å…¨å±€çµ±è¨ˆä¿¡æ¯"""
        return {
            'total_processed_records': self.processed_count,
            'total_word_tokens': self.total_words,
            'vocabulary_size': len(self.vocabulary),
            'avg_words_per_record': self.total_words / self.processed_count if self.processed_count > 0 else 0
        }

def main():
    """ä¸»å‡½æ•¸ - åŸ·è¡Œå®Œæ•´çš„æ–°èæ–‡æœ¬æ–·è©åˆ†æ"""
    print("ğŸš€ é‡å­æ–°èåˆ†æ - ä¸­æ–‡æ–·è©è™•ç†")
    print("=" * 60)
    
    start_time = time.time()
    
    # åˆå§‹åŒ–æ–·è©å™¨
    segmenter = QuantumNewsSegmenter()
    
    try:
        # è®€å–æ•¸æ“š
        print("ğŸ“Š è®€å–æ–°èæ•¸æ“šé›†...")
        df = pd.read_excel('../data/dataseet.xlsx')
        print(f"æ•¸æ“šé›†å½¢ç‹€: {df.shape}")
        print(f"å¯ç”¨æ¬„ä½: {list(df.columns)}")
        
        # ç›®æ¨™æ¬„ä½
        target_fields = ['æ–°èæ¨™é¡Œ', 'å½±ç‰‡å°è©±', 'å½±ç‰‡æè¿°']
        available_fields = [field for field in target_fields if field in df.columns]
        
        if not available_fields:
            print("âŒ æœªæ‰¾åˆ°ç›®æ¨™æ¬„ä½")
            return
        
        print(f"âœ… å°‡åˆ†ææ¬„ä½: {available_fields}")
        
        # åˆ†æå„æ¬„ä½
        all_results = []
        field_statistics = {}
        
        for field in available_fields:
            field_results = segmenter.analyze_field(df, field)
            all_results.extend(field_results)
            
            # ç”Ÿæˆæ¬„ä½çµ±è¨ˆ
            field_stats = segmenter.generate_field_statistics(field_results, field)
            field_statistics[field] = field_stats
            
            # ä¿å­˜æ¬„ä½çµæœ
            if field_results:
                field_df = pd.DataFrame(field_results)
                filename = f"../results/{field}_segmentation_results.csv"
                field_df.to_csv(filename, index=False, encoding='utf-8-sig')
                print(f"ğŸ’¾ {field} çµæœå·²ä¿å­˜: {filename}")
        
        # ä¿å­˜å®Œæ•´çµæœ
        if all_results:
            # ä¸»è¦çµæœ
            results_df = pd.DataFrame(all_results)
            results_df.to_csv('../results/complete_segmentation_results.csv', 
                            index=False, encoding='utf-8-sig')
            print(f"\nğŸ’¾ å®Œæ•´æ–·è©çµæœå·²ä¿å­˜: ../results/complete_segmentation_results.csv")
            
            # çµ±è¨ˆæ‘˜è¦
            global_stats = segmenter.get_global_statistics()
            
            analysis_summary = {
                'analysis_timestamp': pd.Timestamp.now().isoformat(),
                'processing_time_seconds': time.time() - start_time,
                'global_statistics': global_stats,
                'field_statistics': field_statistics,
                'fields_analyzed': available_fields,
                'total_records_processed': len(all_results)
            }
            
            with open('../results/segmentation_analysis_summary.json', 'w', encoding='utf-8') as f:
                json.dump(analysis_summary, f, ensure_ascii=False, indent=2, default=str)
            
            # é¡¯ç¤ºæ‘˜è¦
            print(f"\nğŸ“ˆ åˆ†ææ‘˜è¦:")
            print(f"  è™•ç†è¨˜éŒ„æ•¸: {global_stats['total_processed_records']}")
            print(f"  ç¸½è©å½™tokens: {global_stats['total_word_tokens']:,}")
            print(f"  è©å½™è¡¨å¤§å°: {global_stats['vocabulary_size']:,}")
            print(f"  å¹³å‡è©æ•¸/è¨˜éŒ„: {global_stats['avg_words_per_record']:.1f}")
            print(f"  è™•ç†æ™‚é–“: {time.time() - start_time:.1f} ç§’")
            
            # å„æ¬„ä½çµ±è¨ˆ
            for field, stats in field_statistics.items():
                print(f"\n  {field} çµ±è¨ˆ:")
                print(f"    è¨˜éŒ„æ•¸: {stats['total_records']}")
                print(f"    å¹³å‡è©æ•¸: {stats['avg_word_count']:.1f} Â± {stats['std_word_count']:.1f}")
                print(f"    è©å½™å¤šæ¨£æ€§: {stats['avg_lexical_diversity']:.3f}")
                print(f"    é«˜é »è©å½™: {[word for word, count in stats['top_10_words'][:5]]}")
            
            print(f"\nâœ… ä¸­æ–‡æ–·è©åˆ†æå®Œæˆï¼")
            print(f"ğŸ“ çµæœä¿å­˜åœ¨: ../results/")
            
        else:
            print("âŒ æ²’æœ‰æˆåŠŸè™•ç†ä»»ä½•è¨˜éŒ„")
    
    except Exception as e:
        print(f"âŒ è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
