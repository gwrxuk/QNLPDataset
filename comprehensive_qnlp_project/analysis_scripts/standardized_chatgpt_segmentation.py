#!/usr/bin/env python3
"""
æ¨™æº–åŒ–ChatGPTä¸­æ–‡æ–·è©åˆ†æ - èˆ‡jiebaæ ¼å¼å®Œå…¨ä¸€è‡´
Standardized ChatGPT Chinese Word Segmentation - Consistent with jieba format
"""

from openai import OpenAI
import pandas as pd
import numpy as np
import os
import time
import re
from typing import List, Dict, Optional
from collections import Counter

class StandardizedChatGPTSegmenter:
    """æ¨™æº–åŒ–ChatGPTæ–·è©å™¨ - èˆ‡jiebaæ ¼å¼ä¸€è‡´"""
    
    def __init__(self, api_key: Optional[str] = None, model: str = "gpt-3.5-turbo"):
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        self.model = model
        self.client = None
        self.request_count = 0
        self.total_tokens = 0
        self.error_count = 0
        
        if self.api_key:
            self.client = OpenAI(api_key=self.api_key)
    
    def clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬ - èˆ‡jiebaä¿æŒä¸€è‡´"""
        if pd.isna(text):
            return ""
        
        text = str(text).strip()
        # ç§»é™¤è‹±æ–‡éƒ¨åˆ†ä½†ä¿ç•™æ¨™é»
        text = re.sub(r'[A-Za-z][A-Za-z\s]*[A-Za-z]', '', text)
        # ç§»é™¤å¤šé¤˜ç©ºç™½
        text = re.sub(r'\s+', ' ', text).strip()
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ä½†ä¿ç•™ä¸­æ–‡æ¨™é»
        text = re.sub(r'["""''""''ã€Œã€ã€ã€]', '', text)
        
        return text
    
    def create_segmentation_prompt(self, text: str) -> str:
        """å‰µå»ºæ¨™æº–åŒ–æ–·è©æç¤ºè©"""
        return f"""è«‹å°ä»¥ä¸‹ä¸­æ–‡æ–‡æœ¬é€²è¡Œç²¾ç¢ºæ–·è©ä¸¦æ¨™è¨»è©æ€§ã€‚

è¦æ±‚ï¼š
1. åªè™•ç†ä¸­æ–‡éƒ¨åˆ†ï¼Œå¿½ç•¥è‹±æ–‡
2. ç²¾ç¢ºæ–·è©ï¼Œä¿æŒèªç¾©å®Œæ•´æ€§
3. å°ˆæœ‰åè©ä¿æŒå®Œæ•´
4. è©æ€§æ¨™è¨»ä½¿ç”¨æ¨™æº–æ¨™è¨˜ï¼šn(åè©) v(å‹•è©) a(å½¢å®¹è©) ad(å‰¯è©) p(ä»‹è©) c(é€£è©) u(åŠ©è©) m(æ•¸è©) r(ä»£è©) w(æ¨™é») ns(åœ°å) nr(äººå) nt(æ©Ÿæ§‹å)
5. æ ¼å¼ï¼šè©/è©æ€§
6. ç”¨ç©ºæ ¼åˆ†éš”
7. åªè¿”å›æ–·è©çµæœï¼Œä¸è¦è§£é‡‹

æ–‡æœ¬ï¼š{text}

æ–·è©çµæœï¼š"""
    
    def segment_text(self, text: str, retry_count: int = 2) -> Dict:
        """ä½¿ç”¨ChatGPTé€²è¡Œæ¨™æº–åŒ–æ–‡æœ¬æ–·è©"""
        
        if not self.client:
            return self._error_result('APIå®¢æˆ¶ç«¯æœªåˆå§‹åŒ–')
        
        if not text or not text.strip():
            return self._empty_result()
        
        # æ¸…ç†æ–‡æœ¬
        cleaned_text = self.clean_text(text)
        if not cleaned_text:
            return self._empty_result()
        
        for attempt in range(retry_count + 1):
            try:
                prompt = self.create_segmentation_prompt(cleaned_text)
                
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {
                            "role": "system",
                            "content": "ä½ æ˜¯å°ˆæ¥­çš„ä¸­æ–‡è‡ªç„¶èªè¨€è™•ç†å°ˆå®¶ï¼Œæ“…é•·ç²¾ç¢ºçš„ä¸­æ–‡æ–·è©å’Œè©æ€§æ¨™è¨»ã€‚è«‹åš´æ ¼æŒ‰ç…§è¦æ±‚æ ¼å¼è¼¸å‡ºã€‚"
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    max_tokens=500,
                    temperature=0.1,
                    top_p=0.9
                )
                
                self.request_count += 1
                self.total_tokens += response.usage.total_tokens
                
                result_text = response.choices[0].message.content.strip()
                return self._parse_segmentation_result(result_text, cleaned_text, response.usage.total_tokens)
                
            except Exception as e:
                if attempt < retry_count:
                    print(f"âš ï¸  APIèª¿ç”¨å¤±æ•—ï¼Œé‡è©¦ä¸­... (ç¬¬{attempt + 1}æ¬¡)")
                    time.sleep(2 ** attempt)
                    continue
                else:
                    self.error_count += 1
                    return self._error_result(f"APIèª¿ç”¨å¤±æ•—: {str(e)}")
    
    def _parse_segmentation_result(self, result_text: str, cleaned_text: str, tokens_used: int) -> Dict:
        """è§£ææ–·è©çµæœ - æ¨™æº–åŒ–æ ¼å¼"""
        words = []
        pos_tags = []
        
        # åˆ†å‰²ä¸¦è§£ææ¯å€‹è©æ€§æ¨™è¨»å°
        items = result_text.split()
        for item in items:
            if '/' in item:
                parts = item.split('/', 1)
                if len(parts) == 2:
                    word = parts[0].strip()
                    pos = parts[1].strip()
                    if word and word not in ['', ' ']:
                        words.append(word)
                        pos_tags.append(f"{word}/{pos}")
        
        # å‰µå»ºæ¨™æº–åŒ–æ ¼å¼
        segmented_text = ' / '.join(words)
        pos_tags_str = ' | '.join(pos_tags)
        words_list = ', '.join(words)
        
        return {
            'error': None,
            'cleaned_text': cleaned_text,
            'segmented_text': segmented_text,
            'pos_tags': pos_tags_str,
            'word_count': len(words),
            'unique_word_count': len(set(words)),
            'words_list': words_list,
            'tokens_used': tokens_used,
            'raw_response': result_text
        }
    
    def _empty_result(self) -> Dict:
        """ç©ºçµæœ - æ¨™æº–åŒ–æ ¼å¼"""
        return {
            'error': None,
            'cleaned_text': '',
            'segmented_text': '',
            'pos_tags': '',
            'word_count': 0,
            'unique_word_count': 0,
            'words_list': '',
            'tokens_used': 0,
            'raw_response': ''
        }
    
    def _error_result(self, error_msg: str) -> Dict:
        """éŒ¯èª¤çµæœ - æ¨™æº–åŒ–æ ¼å¼"""
        return {
            'error': error_msg,
            'cleaned_text': '',
            'segmented_text': '',
            'pos_tags': '',
            'word_count': 0,
            'unique_word_count': 0,
            'words_list': '',
            'tokens_used': 0,
            'raw_response': ''
        }

def analyze_field_with_standardized_chatgpt(df: pd.DataFrame, field_name: str, 
                                           segmenter: StandardizedChatGPTSegmenter,
                                           max_records: Optional[int] = None) -> List[Dict]:
    """ä½¿ç”¨æ¨™æº–åŒ–ChatGPTåˆ†ææ¬„ä½ - èˆ‡jiebaæ ¼å¼ä¸€è‡´"""
    
    print(f"\nğŸ¤– ä½¿ç”¨æ¨™æº–åŒ–ChatGPTåˆ†æ {field_name} æ¬„ä½")
    print("=" * 50)
    
    if not segmenter.client:
        print("âŒ ChatGPT APIæœªè¨­å®š")
        return []
    
    # é™åˆ¶è™•ç†æ•¸é‡
    field_data = df[field_name].dropna()
    if max_records:
        field_data = field_data.head(max_records)
        print(f"ğŸ“Š è™•ç†å‰ {max_records} ç­†è¨˜éŒ„")
    else:
        print(f"ğŸ“Š è™•ç†å…¨éƒ¨ {len(field_data)} ç­†è¨˜éŒ„")
    
    results = []
    
    for idx, (original_idx, text) in enumerate(field_data.items()):
        print(f"è™•ç†é€²åº¦: {idx + 1}/{len(field_data)} - {str(text)[:30]}...")
        
        seg_result = segmenter.segment_text(str(text))
        
        # å‰µå»ºèˆ‡jiebaå®Œå…¨ä¸€è‡´çš„æ ¼å¼
        results.append({
            'record_id': original_idx,
            'field': field_name,
            'original_text': str(text),
            'cleaned_text': seg_result['cleaned_text'],
            'segmented_text': seg_result['segmented_text'],
            'pos_tags': seg_result['pos_tags'],
            'word_count': seg_result['word_count'],
            'unique_word_count': seg_result['unique_word_count'],
            'words_list': seg_result['words_list'],
            'tokens_used': seg_result['tokens_used'],
            'api_error': seg_result['error'],
            'raw_response': seg_result['raw_response']
        })
        
        # é¿å…APIé™åˆ¶
        time.sleep(1.2)
    
    # é¡¯ç¤ºçµ±è¨ˆ
    successful = len([r for r in results if not r['api_error']])
    total_tokens = sum(r['tokens_used'] for r in results if r['tokens_used'] > 0)
    
    print(f"\nğŸ“ˆ {field_name} è™•ç†å®Œæˆ:")
    print(f"  æˆåŠŸè™•ç†: {successful}/{len(results)}")
    print(f"  ç¸½tokens: {total_tokens}")
    print(f"  å¹³å‡è©æ•¸: {sum(r['word_count'] for r in results if r['word_count'] > 0) / max(successful, 1):.1f}")
    
    return results

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ æ¨™æº–åŒ–ChatGPTä¸­æ–‡æ–·è©åˆ†æ")
    print("=" * 50)
    
    # æª¢æŸ¥APIå¯†é‘°
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("âŒ è«‹è¨­å®šOPENAI_API_KEYç’°å¢ƒè®Šæ•¸")
        return
    
    print(f"âœ… APIå¯†é‘°å·²è¨­å®š: {api_key[:10]}...")
    
    try:
        # è®€å–æ•¸æ“š
        print("\nğŸ“Š è®€å–æ•¸æ“šé›†...")
        df = pd.read_excel('../datasets/dataseet.xlsx')
        print(f"æ•¸æ“šé›†å½¢ç‹€: {df.shape}")
        
        # ç›®æ¨™æ¬„ä½
        target_fields = ['æ–°èæ¨™é¡Œ', 'å½±ç‰‡å°è©±', 'å½±ç‰‡æè¿°']
        available_fields = [field for field in target_fields if field in df.columns]
        
        if not available_fields:
            print("âŒ æœªæ‰¾åˆ°ç›®æ¨™æ¬„ä½")
            return
        
        print(f"âœ… æ‰¾åˆ°æ¬„ä½: {', '.join(available_fields)}")
        
        # è©¢å•è™•ç†ç¯„åœ
        max_records_input = input(f"æ¯å€‹æ¬„ä½è™•ç†å¤šå°‘ç­†è¨˜éŒ„ï¼Ÿ(Enter=å…¨éƒ¨, æ•¸å­—=é™åˆ¶ç­†æ•¸): ").strip()
        
        max_records = None
        if max_records_input.isdigit():
            max_records = int(max_records_input)
            print(f"ğŸ“ å°‡è™•ç†æ¯å€‹æ¬„ä½çš„å‰ {max_records} ç­†è¨˜éŒ„")
        else:
            print("ğŸ“ å°‡è™•ç†å…¨éƒ¨è¨˜éŒ„")
        
        # åˆå§‹åŒ–æ¨™æº–åŒ–ChatGPTæ–·è©å™¨
        segmenter = StandardizedChatGPTSegmenter(api_key=api_key)
        
        # è™•ç†å„æ¬„ä½
        all_results = []
        
        for field in available_fields:
            field_results = analyze_field_with_standardized_chatgpt(
                df, field, segmenter, max_records
            )
            all_results.extend(field_results)
            
            # ä¿å­˜ä¸­é–“çµæœ
            if field_results:
                field_df = pd.DataFrame(field_results)
                filename = f"../segmentation_results/chatgpt_standardized_{field}_segmentation.csv"
                field_df.to_csv(filename, index=False, encoding='utf-8-sig')
                print(f"ğŸ’¾ {field} çµæœå·²ä¿å­˜: {filename}")
        
        # ä¿å­˜å®Œæ•´çµæœ - èˆ‡jiebaæ ¼å¼å®Œå…¨ä¸€è‡´
        if all_results:
            results_df = pd.DataFrame(all_results)
            
            # ç¢ºä¿æ¬„ä½é †åºèˆ‡jiebaä¸€è‡´
            column_order = [
                'record_id', 'field', 'original_text', 'cleaned_text', 
                'segmented_text', 'pos_tags', 'word_count', 
                'unique_word_count', 'words_list'
            ]
            
            # åªä¿ç•™æ¨™æº–æ¬„ä½
            standard_df = results_df[column_order].copy()
            standard_df.to_csv('../segmentation_results/chatgpt_standardized_segmentation_results.csv', 
                              index=False, encoding='utf-8-sig')
            
            print(f"\nğŸ’¾ æ¨™æº–åŒ–çµæœå·²ä¿å­˜: ../segmentation_results/chatgpt_standardized_segmentation_results.csv")
            
            # ç”Ÿæˆçµ±è¨ˆæ‘˜è¦
            successful_results = [r for r in all_results if not r['api_error']]
            
            print(f"\nğŸ‰ æ¨™æº–åŒ–è™•ç†å®Œæˆæ‘˜è¦:")
            print(f"  ç¸½è¨˜éŒ„æ•¸: {len(all_results)}")
            print(f"  æˆåŠŸè™•ç†: {len(successful_results)}")
            print(f"  å¹³å‡è©æ•¸: {np.mean([r['word_count'] for r in successful_results]) if successful_results else 0:.1f}")
            print(f"  æ ¼å¼: èˆ‡jiebaå®Œå…¨ä¸€è‡´")
            
        else:
            print("âŒ æ²’æœ‰æˆåŠŸè™•ç†ä»»ä½•è¨˜éŒ„")
    
    except Exception as e:
        print(f"âŒ è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
