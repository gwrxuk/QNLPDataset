#!/usr/bin/env python3
"""
çœŸå¯¦ChatGPTä¸­æ–‡æ–·è©å®Œæ•´åˆ†æ
Real ChatGPT Chinese Word Segmentation Complete Analysis
"""

from openai import OpenAI
import pandas as pd
import numpy as np
import os
import time
import json
from typing import List, Dict, Optional
from collections import Counter
import re

class RealChatGPTSegmenter:
    """çœŸå¯¦ChatGPTæ–·è©å™¨"""
    
    def __init__(self, api_key: Optional[str] = None, model: str = "gpt-3.5-turbo"):
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        self.model = model
        self.client = None
        self.request_count = 0
        self.total_tokens = 0
        self.error_count = 0
        
        if self.api_key:
            self.client = OpenAI(api_key=self.api_key)
    
    def create_segmentation_prompt(self, text: str) -> str:
        """å‰µå»ºå°ˆæ¥­æ–·è©æç¤ºè©"""
        return f"""è«‹å°ä»¥ä¸‹ä¸­æ–‡æ–‡æœ¬é€²è¡Œå°ˆæ¥­æ–·è©ä¸¦æ¨™è¨»è©æ€§ã€‚

è¦æ±‚ï¼š
1. ç²¾ç¢ºæ–·è©ï¼Œä¿æŒèªç¾©å®Œæ•´æ€§å’Œè‡ªç„¶æ€§
2. å°ˆæœ‰åè©ã€äººåã€åœ°åã€æ©Ÿæ§‹åä¿æŒå®Œæ•´
3. æ•¸å­—ã€è‹±æ–‡å–®è©å–®ç¨è™•ç†
4. è©æ€§æ¨™è¨»ä½¿ç”¨æ¨™æº–æ¨™è¨˜ï¼š
   - n(åè©) v(å‹•è©) a(å½¢å®¹è©) ad(å‰¯è©) p(ä»‹è©)
   - c(é€£è©) u(åŠ©è©) m(æ•¸è©) r(ä»£è©) w(æ¨™é»)
   - ns(åœ°å) nr(äººå) nt(æ©Ÿæ§‹å) nz(å…¶ä»–å°ˆå)
5. æ ¼å¼ï¼šè©/è©æ€§
6. ç”¨ç©ºæ ¼åˆ†éš”æ¯å€‹è©æ€§æ¨™è¨»å°
7. åªè¿”å›æ–·è©çµæœï¼Œä¸è¦ä»»ä½•è§£é‡‹

æ–‡æœ¬ï¼š{text}

æ–·è©çµæœï¼š"""
    
    def segment_text(self, text: str, retry_count: int = 2) -> Dict:
        """ä½¿ç”¨ChatGPTé€²è¡Œæ–‡æœ¬æ–·è©"""
        
        if not self.client:
            return self._error_result('APIå®¢æˆ¶ç«¯æœªåˆå§‹åŒ–ï¼Œè«‹æª¢æŸ¥APIå¯†é‘°')
        
        if not text or not text.strip():
            return self._empty_result()
        
        # æ¸…ç†æ–‡æœ¬
        cleaned_text = self._clean_text(text)
        if not cleaned_text:
            return self._empty_result()
        
        for attempt in range(retry_count + 1):
            try:
                prompt = self.create_segmentation_prompt(cleaned_text)
                
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {
                            "role": "system",
                            "content": "ä½ æ˜¯å°ˆæ¥­çš„ä¸­æ–‡è‡ªç„¶èªè¨€è™•ç†å°ˆå®¶ï¼Œæ“…é•·ç²¾ç¢ºçš„ä¸­æ–‡æ–·è©å’Œè©æ€§æ¨™è¨»ã€‚ä½ çš„åˆ†ææº–ç¢ºã€ä¸€è‡´ï¼Œéµå¾ªå­¸è¡“æ¨™æº–ã€‚"
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    max_tokens=800,
                    temperature=0.1,  # ä½æº«åº¦ç¢ºä¿ä¸€è‡´æ€§
                    top_p=0.9
                )
                
                self.request_count += 1
                self.total_tokens += response.usage.total_tokens
                
                result_text = response.choices[0].message.content.strip()
                return self._parse_segmentation_result(result_text, response.usage.total_tokens)
                
            except Exception as e:
                if attempt < retry_count:
                    print(f"âš ï¸  APIèª¿ç”¨å¤±æ•—ï¼Œé‡è©¦ä¸­... (ç¬¬{attempt + 1}æ¬¡)")
                    time.sleep(2 ** attempt)  # æŒ‡æ•¸é€€é¿
                    continue
                else:
                    self.error_count += 1
                    return self._error_result(f"APIèª¿ç”¨å¤±æ•—: {str(e)}")
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        if pd.isna(text):
            return ""
        
        text = str(text).strip()
        # ç§»é™¤éå¤šçš„ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text)
        
        return text
    
    def _parse_segmentation_result(self, result_text: str, tokens_used: int) -> Dict:
        """è§£ææ–·è©çµæœ"""
        words = []
        pos_tags = []
        
        # åˆ†å‰²ä¸¦è§£ææ¯å€‹è©æ€§æ¨™è¨»å°
        items = result_text.split()
        for item in items:
            if '/' in item:
                parts = item.split('/', 1)
                if len(parts) == 2:
                    word = parts[0].strip()
                    pos = parts[1].strip()
                    if word and word not in ['', ' ']:
                        words.append(word)
                        pos_tags.append(f"{word}/{pos}")
        
        return {
            'error': None,
            'words': words,
            'segmented_text': ' / '.join(words),
            'pos_tags': ' | '.join(pos_tags),
            'word_count': len(words),
            'unique_words': len(set(words)),
            'tokens_used': tokens_used,
            'raw_response': result_text
        }
    
    def _empty_result(self) -> Dict:
        """ç©ºçµæœ"""
        return {
            'error': None,
            'words': [],
            'segmented_text': '',
            'pos_tags': '',
            'word_count': 0,
            'unique_words': 0,
            'tokens_used': 0,
            'raw_response': ''
        }
    
    def _error_result(self, error_msg: str) -> Dict:
        """éŒ¯èª¤çµæœ"""
        return {
            'error': error_msg,
            'words': [],
            'segmented_text': '',
            'pos_tags': '',
            'word_count': 0,
            'unique_words': 0,
            'tokens_used': 0,
            'raw_response': ''
        }
    
    def batch_segment(self, texts: List[str], delay: float = 1.0, 
                     progress_callback=None) -> List[Dict]:
        """æ‰¹é‡æ–·è©"""
        results = []
        
        for i, text in enumerate(texts):
            if progress_callback:
                progress_callback(i + 1, len(texts), text)
            
            result = self.segment_text(text)
            results.append(result)
            
            # APIé™åˆ¶æ§åˆ¶
            if delay > 0 and i < len(texts) - 1:
                time.sleep(delay)
        
        return results
    
    def get_stats(self) -> Dict:
        """ç²å–çµ±è¨ˆä¿¡æ¯"""
        return {
            'total_requests': self.request_count,
            'total_tokens': self.total_tokens,
            'error_count': self.error_count,
            'success_rate': (self.request_count - self.error_count) / max(self.request_count, 1),
            'avg_tokens_per_request': self.total_tokens / max(self.request_count, 1)
        }

def analyze_field_with_real_chatgpt(df: pd.DataFrame, field_name: str, 
                                   segmenter: RealChatGPTSegmenter,
                                   max_records: Optional[int] = None) -> List[Dict]:
    """ä½¿ç”¨çœŸå¯¦ChatGPTåˆ†ææ¬„ä½"""
    
    print(f"\nğŸ¤– ä½¿ç”¨çœŸå¯¦ChatGPTåˆ†æ {field_name} æ¬„ä½")
    print("=" * 50)
    
    if not segmenter.client:
        print("âŒ ChatGPT APIæœªè¨­å®šï¼Œè«‹æª¢æŸ¥OPENAI_API_KEYç’°å¢ƒè®Šæ•¸")
        return []
    
    # é™åˆ¶è™•ç†æ•¸é‡ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    field_data = df[field_name].dropna()
    if max_records:
        field_data = field_data.head(max_records)
        print(f"ğŸ“Š è™•ç†å‰ {max_records} ç­†è¨˜éŒ„")
    else:
        print(f"ğŸ“Š è™•ç†å…¨éƒ¨ {len(field_data)} ç­†è¨˜éŒ„")
    
    results = []
    
    def progress_callback(current, total, text):
        progress = (current / total) * 100
        print(f"é€²åº¦: {current}/{total} ({progress:.1f}%) - {text[:30]}...")
    
    # æ‰¹é‡è™•ç†
    segmentation_results = segmenter.batch_segment(
        field_data.tolist(), 
        delay=1.2,  # é¿å…APIé™åˆ¶
        progress_callback=progress_callback
    )
    
    # çµ„ç¹”çµæœ
    for idx, (original_idx, text) in enumerate(field_data.items()):
        seg_result = segmentation_results[idx]
        
        results.append({
            'record_id': original_idx,
            'field': field_name,
            'original_text': str(text),
            'segmented_text': seg_result['segmented_text'],
            'pos_tags': seg_result['pos_tags'],
            'word_count': seg_result['word_count'],
            'unique_word_count': seg_result['unique_words'],
            'words_list': ', '.join(seg_result['words']),
            'tokens_used': seg_result['tokens_used'],
            'api_error': seg_result['error'],
            'raw_response': seg_result['raw_response']
        })
    
    # é¡¯ç¤ºçµ±è¨ˆ
    stats = segmenter.get_stats()
    successful = len([r for r in results if not r['api_error']])
    
    print(f"\nğŸ“ˆ {field_name} è™•ç†å®Œæˆ:")
    print(f"  æˆåŠŸè™•ç†: {successful}/{len(results)}")
    print(f"  ç¸½tokens: {stats['total_tokens']}")
    print(f"  å¹³å‡tokens/ç­†: {stats['avg_tokens_per_request']:.1f}")
    print(f"  æˆåŠŸç‡: {stats['success_rate']:.1%}")
    
    return results

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ çœŸå¯¦ChatGPTä¸­æ–‡æ–·è©å®Œæ•´åˆ†æ")
    print("=" * 50)
    
    # æª¢æŸ¥APIå¯†é‘°
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("âŒ è«‹è¨­å®šOPENAI_API_KEYç’°å¢ƒè®Šæ•¸")
        print("è¨­å®šæ–¹æ³•: export OPENAI_API_KEY='your-api-key'")
        return
    
    print(f"âœ… APIå¯†é‘°å·²è¨­å®š: {api_key[:10]}...")
    
    try:
        # è®€å–æ•¸æ“š
        print("\nğŸ“Š è®€å–æ•¸æ“šé›†...")
        df = pd.read_excel('../dataseet.xlsx')
        print(f"æ•¸æ“šé›†å½¢ç‹€: {df.shape}")
        
        # ç›®æ¨™æ¬„ä½
        target_fields = ['æ–°èæ¨™é¡Œ', 'å½±ç‰‡å°è©±', 'å½±ç‰‡æè¿°']
        available_fields = [field for field in target_fields if field in df.columns]
        
        if not available_fields:
            print("âŒ æœªæ‰¾åˆ°ç›®æ¨™æ¬„ä½")
            return
        
        print(f"âœ… æ‰¾åˆ°æ¬„ä½: {', '.join(available_fields)}")
        
        # è©¢å•è™•ç†ç¯„åœ
        print(f"\nâš™ï¸  è™•ç†è¨­å®š:")
        max_records_input = input(f"æ¯å€‹æ¬„ä½è™•ç†å¤šå°‘ç­†è¨˜éŒ„ï¼Ÿ(Enter=å…¨éƒ¨, æ•¸å­—=é™åˆ¶ç­†æ•¸): ").strip()
        
        max_records = None
        if max_records_input.isdigit():
            max_records = int(max_records_input)
            print(f"ğŸ“ å°‡è™•ç†æ¯å€‹æ¬„ä½çš„å‰ {max_records} ç­†è¨˜éŒ„")
        else:
            print("ğŸ“ å°‡è™•ç†å…¨éƒ¨è¨˜éŒ„")
        
        # åˆå§‹åŒ–ChatGPTæ–·è©å™¨
        segmenter = RealChatGPTSegmenter(api_key=api_key)
        
        # è™•ç†å„æ¬„ä½
        all_results = []
        
        for field in available_fields:
            field_results = analyze_field_with_real_chatgpt(
                df, field, segmenter, max_records
            )
            all_results.extend(field_results)
            
            # ä¿å­˜ä¸­é–“çµæœ
            if field_results:
                field_df = pd.DataFrame(field_results)
                filename = f"../data/chatgpt_{field}_segmentation.csv"
                field_df.to_csv(filename, index=False, encoding='utf-8-sig')
                print(f"ğŸ’¾ {field} çµæœå·²ä¿å­˜: {filename}")
        
        # ä¿å­˜å®Œæ•´çµæœ
        if all_results:
            results_df = pd.DataFrame(all_results)
            results_df.to_csv('../data/real_chatgpt_segmentation_complete.csv', 
                            index=False, encoding='utf-8-sig')
            print(f"\nğŸ’¾ å®Œæ•´çµæœå·²ä¿å­˜: ../data/real_chatgpt_segmentation_complete.csv")
            
            # ç”Ÿæˆçµ±è¨ˆæ‘˜è¦
            final_stats = segmenter.get_stats()
            successful_results = [r for r in all_results if not r['api_error']]
            
            summary = {
                'total_records': len(all_results),
                'successful_records': len(successful_results),
                'total_tokens': final_stats['total_tokens'],
                'total_cost_estimate': (final_stats['total_tokens'] / 1000) * 0.002,  # GPT-3.5-turboåƒ¹æ ¼
                'avg_words_per_record': np.mean([r['word_count'] for r in successful_results]) if successful_results else 0,
                'fields_processed': available_fields,
                'processing_time': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            
            # ä¿å­˜çµ±è¨ˆæ‘˜è¦
            with open('../data/chatgpt_processing_summary.json', 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ‰ è™•ç†å®Œæˆæ‘˜è¦:")
            print(f"  ç¸½è¨˜éŒ„æ•¸: {summary['total_records']}")
            print(f"  æˆåŠŸè™•ç†: {summary['successful_records']}")
            print(f"  ç¸½tokens: {summary['total_tokens']}")
            print(f"  ä¼°è¨ˆæˆæœ¬: ${summary['total_cost_estimate']:.4f}")
            print(f"  å¹³å‡è©æ•¸: {summary['avg_words_per_record']:.1f}")
            print(f"  è™•ç†æ™‚é–“: {summary['processing_time']}")
            
        else:
            print("âŒ æ²’æœ‰æˆåŠŸè™•ç†ä»»ä½•è¨˜éŒ„")
    
    except Exception as e:
        print(f"âŒ è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
