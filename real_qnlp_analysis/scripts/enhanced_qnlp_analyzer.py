#!/usr/bin/env python3
"""
å¢å¼·ç‰ˆé‡å­è‡ªç„¶èªè¨€è™•ç†åˆ†æå™¨
Enhanced Quantum Natural Language Processing Analyzer
æ”¯æŒjiebaå’ŒChatGPTå…©ç¨®æ–·è©æ–¹æ³•çš„æ¯”è¼ƒåˆ†æ
"""

import pandas as pd
import numpy as np
from qiskit import QuantumCircuit, execute, Aer
from qiskit.quantum_info import entropy, Statevector
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re
from collections import Counter
from typing import List, Dict, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

class EnhancedQNLPAnalyzer:
    """å¢å¼·ç‰ˆQNLPåˆ†æå™¨ï¼Œæ”¯æŒå¤šç¨®æ–·è©æ–¹æ³•"""
    
    def __init__(self):
        self.backend = Aer.get_backend('qasm_simulator')
        self.statevector_backend = Aer.get_backend('statevector_simulator')
        self.results = {}
        
    def create_quantum_circuit_from_segmentation(self, words: List[str], max_qubits: int = 6) -> Tuple[QuantumCircuit, int]:
        """æ ¹æ“šæ–·è©çµæœå‰µå»ºé‡å­é›»è·¯"""
        if not words:
            return None, 0
        
        # ä½¿ç”¨TF-IDFè¨ˆç®—è©å½™æ¬Šé‡
        text = ' '.join(words)
        vectorizer = TfidfVectorizer(max_features=max_qubits)
        
        try:
            tfidf_matrix = vectorizer.fit_transform([text])
            feature_names = vectorizer.get_feature_names_out()
            weights = tfidf_matrix.toarray()[0]
        except:
            # å¦‚æœTF-IDFå¤±æ•—ï¼Œä½¿ç”¨å‡å‹»æ¬Šé‡
            n_qubits = min(len(words), max_qubits)
            weights = np.ones(n_qubits) / n_qubits
            feature_names = words[:n_qubits]
        
        n_qubits = len(weights)
        if n_qubits == 0:
            return None, 0
        
        # å‰µå»ºé‡å­é›»è·¯
        qc = QuantumCircuit(n_qubits)
        
        # åˆå§‹åŒ–ç–ŠåŠ æ…‹
        for i in range(n_qubits):
            qc.h(i)
        
        # åŸºæ–¼TF-IDFæ¬Šé‡çš„æ—‹è½‰
        for i, weight in enumerate(weights):
            if weight > 0:
                angle = weight * np.pi + np.pi/4
                qc.ry(angle, i)
        
        # å‰µå»ºç³¾çº
        for i in range(n_qubits - 1):
            qc.cx(i, i + 1)
        
        # æ·»åŠ ç›¸ä½é–€
        for i in range(n_qubits - 1):
            qc.cp(np.pi/4, i, (i + 1) % n_qubits)
        
        return qc, n_qubits
    
    def measure_quantum_properties(self, quantum_circuit: QuantumCircuit, n_qubits: int) -> Dict:
        """æ¸¬é‡é‡å­ç‰¹æ€§"""
        if quantum_circuit is None or n_qubits == 0:
            return {
                'entropy': 0,
                'coherence': 0,
                'interference': 0,
                'superposition_strength': 0
            }
        
        try:
            # ç²å–ç‹€æ…‹å‘é‡
            job = execute(quantum_circuit, self.statevector_backend)
            result = job.result()
            statevector = result.get_statevector()
            
            # è¨ˆç®—von Neumannç†µï¼ˆæ•˜äº‹è¤‡é›œåº¦ï¼‰
            entropy_val = entropy(statevector)
            
            # è¨ˆç®—é‡å­é€£è²«æ€§
            amplitudes = np.abs(statevector.data)
            coherence = 1 - np.sum(amplitudes**4)  # åƒèˆ‡æ¯”
            
            # è¨ˆç®—é‡å­å¹²æ¶‰
            phases = np.angle(statevector.data)
            phase_variance = np.var(phases)
            interference = 1 - (phase_variance / (np.pi**2)) if phase_variance > 0 else 1
            
            # è¨ˆç®—ç–ŠåŠ å¼·åº¦
            prob_dist = amplitudes**2
            superposition_strength = 1 - np.max(prob_dist)  # 1 - æœ€å¤§æ©Ÿç‡
            
            return {
                'entropy': float(entropy_val),
                'coherence': float(coherence),
                'interference': float(interference),
                'superposition_strength': float(superposition_strength)
            }
            
        except Exception as e:
            print(f"é‡å­æ¸¬é‡éŒ¯èª¤: {e}")
            return {
                'entropy': 0,
                'coherence': 0,
                'interference': 0,
                'superposition_strength': 0
            }
    
    def analyze_semantic_complexity(self, words: List[str]) -> float:
        """åˆ†æèªç¾©è¤‡é›œåº¦"""
        if not words:
            return 0
        
        # åŸºæ–¼è©å½™å¤šæ¨£æ€§å’Œé•·åº¦çš„è¤‡é›œåº¦
        unique_words = len(set(words))
        total_words = len(words)
        
        if total_words == 0:
            return 0
        
        # è©å½™è±å¯Œåº¦
        richness = unique_words / total_words
        
        # å¹³å‡è©é•·
        avg_word_length = np.mean([len(word) for word in words])
        
        # ç¶œåˆè¤‡é›œåº¦
        complexity = (richness * 0.7 + min(avg_word_length / 5, 1) * 0.3)
        
        return float(complexity)
    
    def detect_narrative_superposition(self, segmentation_results: List[Dict]) -> float:
        """æª¢æ¸¬æ•˜äº‹ç–ŠåŠ ç¾è±¡"""
        if not segmentation_results:
            return 0
        
        # æ”¶é›†æ‰€æœ‰è©å½™
        all_words = []
        for result in segmentation_results:
            if 'words' in result and result['words']:
                all_words.extend(result['words'])
        
        if not all_words:
            return 0
        
        # è¨ˆç®—è©é »åˆ†å¸ƒçš„ç†µ
        word_counts = Counter(all_words)
        total_words = sum(word_counts.values())
        
        if total_words == 0:
            return 0
        
        # è¨ˆç®—æ©Ÿç‡åˆ†å¸ƒçš„ç†µ
        probs = [count / total_words for count in word_counts.values()]
        narrative_entropy = -sum(p * np.log2(p) for p in probs if p > 0)
        
        # æ­£è¦åŒ–åˆ°0-1ç¯„åœ
        max_entropy = np.log2(len(word_counts))
        superposition = narrative_entropy / max_entropy if max_entropy > 0 else 0
        
        return float(superposition)
    
    def measure_semantic_entanglement(self, text1_words: List[str], text2_words: List[str]) -> float:
        """æ¸¬é‡èªç¾©ç³¾çº"""
        if not text1_words or not text2_words:
            return 0
        
        # å‰µå»ºè©å½™å‘é‡
        all_words = list(set(text1_words + text2_words))
        
        if len(all_words) < 2:
            return 0
        
        # è¨ˆç®—è©é »å‘é‡
        vec1 = [text1_words.count(word) for word in all_words]
        vec2 = [text2_words.count(word) for word in all_words]
        
        # æ­£è¦åŒ–
        vec1 = np.array(vec1) / (np.sum(vec1) + 1e-10)
        vec2 = np.array(vec2) / (np.sum(vec2) + 1e-10)
        
        # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦ä½œç‚ºç³¾çºåº¦é‡
        similarity = cosine_similarity([vec1], [vec2])[0][0]
        
        # è½‰æ›ç‚ºç³¾çºå¼·åº¦ï¼ˆ0-2ç¯„åœï¼‰
        entanglement = (1 + similarity) * 1.0
        
        return float(entanglement)
    
    def analyze_segmentation_method(self, segmentation_data: pd.DataFrame, method_name: str) -> Dict:
        """åˆ†æç‰¹å®šæ–·è©æ–¹æ³•çš„çµæœ"""
        print(f"\nğŸ”¬ åˆ†æ {method_name} æ–·è©æ–¹æ³•")
        print("=" * 40)
        
        results = {
            'method': method_name,
            'field_results': {},
            'overall_stats': {}
        }
        
        # æŒ‰æ¬„ä½åˆ†æ
        fields = segmentation_data['field'].unique()
        
        for field in fields:
            field_data = segmentation_data[segmentation_data['field'] == field]
            print(f"\nğŸ“Š åˆ†æ {field} æ¬„ä½ ({len(field_data)} ç­†è¨˜éŒ„)")
            
            field_results = {
                'quantum_properties': [],
                'semantic_complexity': [],
                'word_counts': [],
                'unique_word_counts': [],
                'text_lengths': []
            }
            
            # é€ç­†åˆ†æ
            for _, row in field_data.iterrows():
                if pd.isna(row['words_list']) or not row['words_list'].strip():
                    continue
                
                # è§£æè©å½™
                words = [w.strip() for w in str(row['words_list']).split(',') if w.strip()]
                
                if not words:
                    continue
                
                # å‰µå»ºé‡å­é›»è·¯ä¸¦æ¸¬é‡
                qc, n_qubits = self.create_quantum_circuit_from_segmentation(words)
                quantum_props = self.measure_quantum_properties(qc, n_qubits)
                
                # è¨ˆç®—èªç¾©è¤‡é›œåº¦
                complexity = self.analyze_semantic_complexity(words)
                
                # è¨˜éŒ„çµæœ
                field_results['quantum_properties'].append(quantum_props)
                field_results['semantic_complexity'].append(complexity)
                field_results['word_counts'].append(len(words))
                field_results['unique_word_counts'].append(len(set(words)))
                field_results['text_lengths'].append(len(' '.join(words)))
            
            # è¨ˆç®—æ¬„ä½çµ±è¨ˆ
            if field_results['quantum_properties']:
                field_stats = self._calculate_field_statistics(field_results)
                results['field_results'][field] = field_stats
                
                print(f"  é‡å­é€£è²«æ€§: {field_stats['avg_coherence']:.4f} Â± {field_stats['std_coherence']:.4f}")
                print(f"  é‡å­å¹²æ¶‰: {field_stats['avg_interference']:.4f} Â± {field_stats['std_interference']:.4f}")
                print(f"  æ•˜äº‹è¤‡é›œåº¦: {field_stats['avg_entropy']:.4f} Â± {field_stats['std_entropy']:.4f}")
                print(f"  èªç¾©è¤‡é›œåº¦: {field_stats['avg_semantic_complexity']:.4f}")
                print(f"  å¹³å‡è©æ•¸: {field_stats['avg_word_count']:.1f}")
        
        # è¨ˆç®—æ•´é«”çµ±è¨ˆ
        results['overall_stats'] = self._calculate_overall_statistics(results['field_results'])
        
        return results
    
    def _calculate_field_statistics(self, field_results: Dict) -> Dict:
        """è¨ˆç®—æ¬„ä½çµ±è¨ˆ"""
        quantum_props = field_results['quantum_properties']
        
        coherences = [qp['coherence'] for qp in quantum_props]
        interferences = [qp['interference'] for qp in quantum_props]
        entropies = [qp['entropy'] for qp in quantum_props]
        superpositions = [qp['superposition_strength'] for qp in quantum_props]
        
        return {
            'avg_coherence': np.mean(coherences),
            'std_coherence': np.std(coherences),
            'avg_interference': np.mean(interferences),
            'std_interference': np.std(interferences),
            'avg_entropy': np.mean(entropies),
            'std_entropy': np.std(entropies),
            'avg_superposition': np.mean(superpositions),
            'std_superposition': np.std(superpositions),
            'avg_semantic_complexity': np.mean(field_results['semantic_complexity']),
            'avg_word_count': np.mean(field_results['word_counts']),
            'avg_unique_word_count': np.mean(field_results['unique_word_counts']),
            'avg_text_length': np.mean(field_results['text_lengths'])
        }
    
    def _calculate_overall_statistics(self, field_results: Dict) -> Dict:
        """è¨ˆç®—æ•´é«”çµ±è¨ˆ"""
        if not field_results:
            return {}
        
        all_coherences = []
        all_interferences = []
        all_entropies = []
        all_superpositions = []
        
        for field_stats in field_results.values():
            all_coherences.append(field_stats['avg_coherence'])
            all_interferences.append(field_stats['avg_interference'])
            all_entropies.append(field_stats['avg_entropy'])
            all_superpositions.append(field_stats['avg_superposition'])
        
        return {
            'overall_avg_coherence': np.mean(all_coherences),
            'overall_avg_interference': np.mean(all_interferences),
            'overall_avg_entropy': np.mean(all_entropies),
            'overall_avg_superposition': np.mean(all_superpositions)
        }
    
    def compare_segmentation_methods(self, jieba_results: Dict, chatgpt_results: Dict) -> Dict:
        """æ¯”è¼ƒå…©ç¨®æ–·è©æ–¹æ³•çš„QNLPçµæœ"""
        print("\nğŸ” æ¯”è¼ƒjiebaèˆ‡ChatGPTçš„QNLPåˆ†æçµæœ")
        print("=" * 50)
        
        comparison = {
            'method_comparison': {},
            'field_comparison': {},
            'insights': []
        }
        
        # æ•´é«”æ–¹æ³•æ¯”è¼ƒ
        jieba_overall = jieba_results.get('overall_stats', {})
        chatgpt_overall = chatgpt_results.get('overall_stats', {})
        
        if jieba_overall and chatgpt_overall:
            comparison['method_comparison'] = {
                'coherence_diff': chatgpt_overall['overall_avg_coherence'] - jieba_overall['overall_avg_coherence'],
                'interference_diff': chatgpt_overall['overall_avg_interference'] - jieba_overall['overall_avg_interference'],
                'entropy_diff': chatgpt_overall['overall_avg_entropy'] - jieba_overall['overall_avg_entropy'],
                'superposition_diff': chatgpt_overall['overall_avg_superposition'] - jieba_overall['overall_avg_superposition']
            }
        
        # æ¬„ä½æ¯”è¼ƒ
        common_fields = set(jieba_results.get('field_results', {}).keys()) & set(chatgpt_results.get('field_results', {}).keys())
        
        for field in common_fields:
            jieba_field = jieba_results['field_results'][field]
            chatgpt_field = chatgpt_results['field_results'][field]
            
            comparison['field_comparison'][field] = {
                'coherence_diff': chatgpt_field['avg_coherence'] - jieba_field['avg_coherence'],
                'interference_diff': chatgpt_field['avg_interference'] - jieba_field['avg_interference'],
                'entropy_diff': chatgpt_field['avg_entropy'] - jieba_field['avg_entropy'],
                'word_count_diff': chatgpt_field['avg_word_count'] - jieba_field['avg_word_count'],
                'semantic_complexity_diff': chatgpt_field['avg_semantic_complexity'] - jieba_field['avg_semantic_complexity']
            }
        
        # ç”Ÿæˆæ´å¯Ÿ
        comparison['insights'] = self._generate_comparison_insights(comparison)
        
        return comparison
    
    def _generate_comparison_insights(self, comparison: Dict) -> List[str]:
        """ç”Ÿæˆæ¯”è¼ƒæ´å¯Ÿ"""
        insights = []
        
        method_comp = comparison.get('method_comparison', {})
        
        if method_comp:
            if method_comp['coherence_diff'] > 0.1:
                insights.append("ChatGPTæ–·è©ç”¢ç”Ÿæ›´é«˜çš„é‡å­é€£è²«æ€§ï¼Œè¡¨ç¤ºèªç¾©ä¸€è‡´æ€§æ›´å¼·")
            elif method_comp['coherence_diff'] < -0.1:
                insights.append("jiebaæ–·è©ç”¢ç”Ÿæ›´é«˜çš„é‡å­é€£è²«æ€§ï¼Œè¡¨ç¤ºèªç¾©ä¸€è‡´æ€§æ›´å¼·")
            
            if method_comp['entropy_diff'] > 0.1:
                insights.append("ChatGPTæ–·è©é¡¯ç¤ºæ›´é«˜çš„æ•˜äº‹è¤‡é›œåº¦ï¼Œå¯èƒ½åæ˜ æ›´ç´°ç·»çš„èªç¾©çµæ§‹")
            elif method_comp['entropy_diff'] < -0.1:
                insights.append("jiebaæ–·è©é¡¯ç¤ºæ›´é«˜çš„æ•˜äº‹è¤‡é›œåº¦")
            
            if method_comp['superposition_diff'] > 0.1:
                insights.append("ChatGPTæ–·è©å±•ç¾æ›´å¼·çš„æ•˜äº‹ç–ŠåŠ ç¾è±¡ï¼Œæ”¯æŒå¤šé‡ç¾å¯¦ç†è«–")
            elif method_comp['superposition_diff'] < -0.1:
                insights.append("jiebaæ–·è©å±•ç¾æ›´å¼·çš„æ•˜äº‹ç–ŠåŠ ç¾è±¡")
        
        return insights

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ”¬ å¢å¼·ç‰ˆé‡å­è‡ªç„¶èªè¨€è™•ç†åˆ†æ")
    print("=" * 50)
    
    analyzer = EnhancedQNLPAnalyzer()
    
    try:
        # è®€å–jiebaçµæœ
        print("ğŸ“Š è®€å–jiebaæ–·è©çµæœ...")
        jieba_df = pd.read_csv('../jieba_segmentation_results.csv')
        print(f"jiebaçµæœ: {len(jieba_df)} ç­†è¨˜éŒ„")
        
        # è®€å–ChatGPTçµæœ
        chatgpt_files = [
            '../data/real_chatgpt_segmentation_complete.csv',
            '../real_chatgpt_segmentation_sample.csv'
        ]
        
        chatgpt_df = None
        for file_path in chatgpt_files:
            try:
                chatgpt_df = pd.read_csv(file_path)
                print(f"âœ… è®€å–ChatGPTçµæœ: {file_path} ({len(chatgpt_df)} ç­†è¨˜éŒ„)")
                break
            except FileNotFoundError:
                continue
        
        if chatgpt_df is None:
            print("âŒ æœªæ‰¾åˆ°ChatGPTæ–·è©çµæœï¼Œè«‹å…ˆé‹è¡ŒChatGPTæ–·è©åˆ†æ")
            return
        
        # åˆ†æjiebaçµæœ
        jieba_analysis = analyzer.analyze_segmentation_method(jieba_df, "jieba")
        
        # åˆ†æChatGPTçµæœ
        chatgpt_analysis = analyzer.analyze_segmentation_method(chatgpt_df, "ChatGPT")
        
        # æ¯”è¼ƒåˆ†æ
        comparison = analyzer.compare_segmentation_methods(jieba_analysis, chatgpt_analysis)
        
        # ä¿å­˜çµæœ
        import json
        
        # ä¿å­˜è©³ç´°åˆ†æçµæœ
        analysis_results = {
            'jieba_analysis': jieba_analysis,
            'chatgpt_analysis': chatgpt_analysis,
            'comparison': comparison,
            'analysis_timestamp': pd.Timestamp.now().isoformat()
        }
        
        with open('../results/qnlp_comparative_analysis.json', 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nğŸ’¾ åˆ†æçµæœå·²ä¿å­˜: ../results/qnlp_comparative_analysis.json")
        
        # é¡¯ç¤ºä¸»è¦ç™¼ç¾
        print(f"\nğŸ” ä¸»è¦ç™¼ç¾:")
        for insight in comparison['insights']:
            print(f"  â€¢ {insight}")
        
        # é¡¯ç¤ºæ•¸å€¼æ¯”è¼ƒ
        if comparison['method_comparison']:
            mc = comparison['method_comparison']
            print(f"\nğŸ“Š é‡å­æŒ‡æ¨™æ¯”è¼ƒ (ChatGPT - jieba):")
            print(f"  é‡å­é€£è²«æ€§å·®ç•°: {mc['coherence_diff']:+.4f}")
            print(f"  é‡å­å¹²æ¶‰å·®ç•°: {mc['interference_diff']:+.4f}")
            print(f"  æ•˜äº‹è¤‡é›œåº¦å·®ç•°: {mc['entropy_diff']:+.4f}")
            print(f"  ç–ŠåŠ å¼·åº¦å·®ç•°: {mc['superposition_diff']:+.4f}")
        
        print(f"\nğŸ‰ QNLPæ¯”è¼ƒåˆ†æå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
